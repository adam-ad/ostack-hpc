\documentclass[12pt]{article}
\usepackage{graphicx}

\begin{document}
	\newenvironment{bash}
	{\begin{quote}
		}
		{
		\end{quote}
	}
\section[]{CloudInitPrep}

OpenStack uses cloud-init for boot time initialization of cloud instances. This recipe relies on cloud-init to initialize HPC instances in an OpenStack cloud. This recipe prepares cloud-init initialization template script, which is than updated with sms-ip and other environment variables just before the provisioning. This is than fed as user data to Nova during instance creation. Script generated here will be executed by root during boot.

Preparing template for compute node cloud-init

Create an empty chpc\_init file and open for editing. You can also use existing template and modify.
Start editing by adding some environment variable, first one is to set path to shared folder for cloud-init

\begin{bash}\texttt{\small{ chpcInitPath=/opt/ohpc/admin/cloud\_hpc\_init}}\end{bash}
\begin{bash}\texttt{\small{ logger "chpcInit: Updating Compute Node with HPC configuration"}}\end{bash}

Update rsyslog configuration file to send all the syslog to sms. Sms\_ip is the tag used here is updated with IP of SMS node just before provisioning.

\begin{bash}\texttt{\small{ \# Update rsyslog}}\end{bash}
\begin{bash}\texttt{\small{ cat /etc/rsyslog.conf | grep "<sms\_ip>:514"}}\end{bash}
\begin{bash}\texttt{\small{ rsyslog\_set=\$?}}\end{bash}
\begin{bash}\texttt{\small{ if [ "\${rsyslog\_set}" -ne "0" ]; then}}\end{bash}
\begin{bash}\texttt{\small{     echo "*.* @<sms\_ip>:514" >> /etc/rsyslog.conf}}\end{bash}
\begin{bash}\texttt{\small{ fi}}\end{bash}
\begin{bash}\texttt{\small{ systemctl restart rsyslog}}\end{bash}
\begin{bash}\texttt{\small{ logger "chpcInit: rsyslog configuration complete, updating remaining HPC configuration"}}\end{bash}

Assuming sms node nfs share /home, /opt/ohpc/pub, =/opt/ohpc/admin/cloud\_hpc\_init lets mount them during boot

\begin{bash}\texttt{\small{ \# nfs mount directory from SMS head node to Compute Node}}\end{bash}
\begin{bash}\texttt{\small{ cat /etc/fstab | grep "<sms\_ip>:/home"}}\end{bash}
\begin{bash}\texttt{\small{ home\_exists=\$?}}\end{bash}
\begin{bash}\texttt{\small{ if [ "\${home\_exists}" -ne "0" ]; then}}\end{bash}
\begin{bash}\texttt{\small{     echo "<sms\_ip>:/home /home nfs nfsvers=3,rsize=1024,wsize=1024,cto 0 0" >> /etc/fstab}}\end{bash}
\begin{bash}\texttt{\small{ fi}}\end{bash}
\begin{bash}\texttt{\small{ cat /etc/fstab | grep "<sms\_ip>:/opt/ohpc/pub"}}\end{bash}
\begin{bash}\texttt{\small{ ohpc\_pub\_exists=\$?}}\end{bash}
\begin{bash}\texttt{\small{ }}\end{bash}
\begin{bash}\texttt{\small{ if [ "\${ohpc\_pub\_exists}" -ne "0" ]; then}}\end{bash}
\begin{bash}\texttt{\small{     echo "<sms\_ip>:/opt/ohpc/pub /opt/ohpc/pub nfs nfsvers=3 0 0" >> /etc/fstab}}\end{bash}
\begin{bash}\texttt{\small{     \# Make sure we have directory to mount}}\end{bash}
\begin{bash}\texttt{\small{     \# Clean up if required}}\end{bash}
\begin{bash}\texttt{\small{     if [ -e /opt/ohpc/pub ]; then}}\end{bash}
\begin{bash}\texttt{\small{         echo "chpcInit: [WARNING] /opt/ohpc/pub already exists!!"}}\end{bash}
\begin{bash}\texttt{\small{     fi}}\end{bash}
\begin{bash}\texttt{\small{ fi}}\end{bash}
\begin{bash}\texttt{\small{ mkdir -p /opt/ohpc/pub}}\end{bash}
\begin{bash}\texttt{\small{ mount /home}}\end{bash}
\begin{bash}\texttt{\small{ mount /opt/ohpc/pub}}\end{bash}
\begin{bash}\texttt{\small{ }}\end{bash}
\begin{bash}\texttt{\small{ \# mount cloud\_hpc\_init}}\end{bash}
\begin{bash}\texttt{\small{ cat /etc/fstab | grep "sms\_ip:\$chpcInitPath"}}\end{bash}
\begin{bash}\texttt{\small{ CloudHPCInit\_exist=\$?}}\end{bash}
\begin{bash}\texttt{\small{ if [ "\${CloudHPCInit\_exist}" -ne "0" ]; then}}\end{bash}
\begin{bash}\texttt{\small{     echo "<sms\_ip>:\$chpcInitPath \$chpcInitPath nfs nfsvers=3 0 0" >> /etc/fstab}}\end{bash}
\begin{bash}\texttt{\small{ fi}}\end{bash}
\begin{bash}\texttt{\small{ mkdir -p \$chpcInitPath}}\end{bash}
\begin{bash}\texttt{\small{ mount \$chpcInitPath}}\end{bash}
\begin{bash}\texttt{\small{ \# Restart nfs}}\end{bash}
\begin{bash}\texttt{\small{ systemctl restart nfs}}\end{bash}

Next, have ntp sync with sms node. 

\begin{bash}\texttt{\small{ \# Restart ntp at CN}}\end{bash}
\begin{bash}\texttt{\small{ systemctl enable ntpd}}\end{bash}
\begin{bash}\texttt{\small{ \# Update ntp server}}\end{bash}
\begin{bash}\texttt{\small{ cat /etc/ntp.conf | grep "server <sms\_ip>"}}\end{bash}
\begin{bash}\texttt{\small{ ntp\_server\_exists=\$?}}\end{bash}
\begin{bash}\texttt{\small{ if [ "\${ntp\_server\_exists}" -ne "0" ]; then}}\end{bash}
\begin{bash}\texttt{\small{     echo "server <sms\_ip>" >> /etc/ntp.conf}}\end{bash}
\begin{bash}\texttt{\small{ fi}}\end{bash}
\begin{bash}\texttt{\small{ systemctl restart ntpd}}\end{bash}
\begin{bash}\texttt{\small{ \# time sync}}\end{bash}
\begin{bash}\texttt{\small{ ntpstat}}\end{bash}

Sync sms node with compute nodes. sync users, slurm and enable munge by copying munge key

\begin{bash}\texttt{\small{ \# Sync following files to compute node}}\end{bash}
\begin{bash}\texttt{\small{ \# Assuming nfs is setup properly}}\end{bash}
\begin{bash}\texttt{\small{ if [ -d \$chpcInitPath ]; then}}\end{bash}
\begin{bash}\texttt{\small{     \# Update the slurm file}}\end{bash}
\begin{bash}\texttt{\small{     cp -f -L \$chpcInitPath/slurm.conf /etc/slurm/slurm.conf}}\end{bash}
\begin{bash}\texttt{\small{     \# Sync head node configuration with Compute Node}}\end{bash}
\begin{bash}\texttt{\small{     cp -f -L \$chpcInitPath/passwd /etc/passwd}}\end{bash}
\begin{bash}\texttt{\small{     cp -f -L \$chpcInitPath/group /etc/group}}\end{bash}
\begin{bash}\texttt{\small{     cp -f -L \$chpcInitPath/shadow /etc/shadow}}\end{bash}
\begin{bash}\texttt{\small{     cp -f -L \$chpcInitPath/slurm.conf /etc/slurm/slurm.conf}}\end{bash}
\begin{bash}\texttt{\small{     cp -f -L \$chpcInitPath/slurm /etc/pam.d/slurm}}\end{bash}
\begin{bash}\texttt{\small{     cp -f -L \$chpcInitPath/munge.key /etc/munge/munge.key}}\end{bash}
\begin{bash}\texttt{\small{     \# For hostname resolution}}\end{bash}
\begin{bash}\texttt{\small{     cp -f -L \$chpcInitPath/hosts /etc/hosts}}\end{bash}
\begin{bash}\texttt{\small{     \# make sure that hostname mentioned into /etc/hosts matches machine hostname. TBD}}\end{bash}
\begin{bash}\texttt{\small{     \# Start slurm and munge }}\end{bash}
\begin{bash}\texttt{\small{     systemctl enable munge}}\end{bash}
\begin{bash}\texttt{\small{     systemctl restart munge}}\end{bash}
\begin{bash}\texttt{\small{     systemctl enable slurmd}}\end{bash}
\begin{bash}\texttt{\small{     systemctl restart slurmd}}\end{bash}
\begin{bash}\texttt{\small{ else}}\end{bash}
\begin{bash}\texttt{\small{     logger "chpcInit:ERROR: cannot stat nfs shared /opt directory, cannot copy HPC system files"}}\end{bash}
\begin{bash}\texttt{\small{ fi}}\end{bash}

Update the hostname as per sms node.

\begin{bash}\texttt{\small{ \# Setup hostname as per the head node}}\end{bash}
\begin{bash}\texttt{\small{ \#Find the hostname of this machine from the copied over /etc/hosts file}}\end{bash}
\begin{bash}\texttt{\small{ cc\_ipaddrs=(`hostname -I`)}}\end{bash}
\begin{bash}\texttt{\small{ for cc\_ipaddr in \${cc\_ipaddrs[@]}; do}}\end{bash}
\begin{bash}\texttt{\small{     cat /etc/hosts | grep \${cc\_ipaddr} > /dev/null}}\end{bash}
\begin{bash}\texttt{\small{     result=\$?}}\end{bash}
\begin{bash}\texttt{\small{     if [ "\$result" -eq "0" ]; then}}\end{bash}
\begin{bash}\texttt{\small{         cc\_hostname=`cat /etc/hosts | grep \${cc\_ipaddr} | cut -d\$'\t' -f2`}}\end{bash}
\begin{bash}\texttt{\small{         break}}\end{bash}
\begin{bash}\texttt{\small{     fi}}\end{bash}
\begin{bash}\texttt{\small{ done}}\end{bash}
\begin{bash}\texttt{\small{ }}\end{bash}
\begin{bash}\texttt{\small{ if [ -z "\${cc\_hostname}" ]; then}}\end{bash}
\begin{bash}\texttt{\small{     logger "chpcInit:ERROR: No resolved hostname found for any IP address in /etc/hosts"}}\end{bash}
\begin{bash}\texttt{\small{     exit 1}}\end{bash}
\begin{bash}\texttt{\small{ fi}}\end{bash}
\begin{bash}\texttt{\small{ }}\end{bash}
\begin{bash}\texttt{\small{ \#set the hostname}}\end{bash}
\begin{bash}\texttt{\small{ if [ \$(hostname) != \${cc\_hostname} ]; then}}\end{bash}
\begin{bash}\texttt{\small{     hostnamectl set-hostname \${cc\_hostname}}}\end{bash}
\begin{bash}\texttt{\small{ fi}}\end{bash}

By now all pre-requisite for slurm is taken care, lets start slurm daemon.

\begin{bash}\texttt{\small{ \# Start slurm and munge }}\end{bash}
\begin{bash}\texttt{\small{ systemctl enable munge}}\end{bash}
\begin{bash}\texttt{\small{ systemctl restart munge}}\end{bash}
\begin{bash}\texttt{\small{ systemctl enable slurmd}}\end{bash}
\begin{bash}\texttt{\small{ systemctl restart slurmd}}\end{bash}

One last step to make sure ssh is working and enabled on compute nodes. Update the permissions of ssh.

\begin{bash}\texttt{\small{ \#Change file permissions in /etc/ssh to fix ssh into compute node}}\end{bash}
\begin{bash}\texttt{\small{ chmod 0600 /etc/ssh/ssh\_host\_*\_key}}\end{bash}

Save the file with name chp\_cinit, we will use this file during baremetal node instance creation.

\subsection[]{Preparing template for sms node cloud-init (chpc\_sms\_init)}

Cloud init script for sms node is little different than compute node. Sms node, when instantiated within openstack, serve as a head node for HPC as a service and hosts all the services as a sms node in an independent hpc clusters. This will host server side of applications, resource manager, and share users. For more detail on sms node functionality please refer to OpenHPC documentation.

In this recipe, we will prepare cloud-init template script for sms node, which than is updated with compute node IP, ntp server and other environmental variables, just before provisioning. 

Create an empty chpc\_init file and open for editing. You can also use existing template and modify. Start editing by adding some environment variable, which will be updated later, just before provisioning.

\begin{bash}\texttt{\small{ \# Get the Compute node prefix and number of compute nodesa}}\end{bash}
\begin{bash}\texttt{\small{ cnodename\_prefix=<update\_cnodename\_prefixa}}\end{bash}>
\begin{bash}\texttt{\small{ num\_ccomputes=<update\_num\_ccomputesa}}\end{bash}>
\begin{bash}\texttt{\small{ ntp\_server=<update\_ntp\_servera}}\end{bash}>
\begin{bash}\texttt{\small{ sms\_name=<update\_sms\_namea}}\end{bash}>

Now setup nfs share for cloud-init and files which want to send to compute nodes.

\begin{bash}\texttt{\small{ \# setup cloudinit directory}}\end{bash}
\begin{bash}\texttt{\small{ chpcInitPath=/opt/ohpc/admin/cloud\_hpc\_init}}\end{bash}
\begin{bash}\texttt{\small{ \# create directory of not exists}}\end{bash}
\begin{bash}\texttt{\small{ mkdir -p \$chpcInitPath}}\end{bash}
\begin{bash}\texttt{\small{ chmod 700 \$chpcInitPath}}\end{bash}

To create same user environment, copy user files 

\begin{bash}\texttt{\small{ \# Copy other files needed for Cloud Init}}\end{bash}
\begin{bash}\texttt{\small{ sudo cp -fpr /etc/passwd \$chpcInitPath}}\end{bash}
\begin{bash}\texttt{\small{ sudo cp -fpr /etc/shadow \$chpcInitPath}}\end{bash}
\begin{bash}\texttt{\small{ sudo cp -fpr /etc/group \$chpcInitPath}}\end{bash}

Share /home, /opt/ohpc/pub and /opt/ohpc/admin/cloud\_hpc\_init over nfs

\begin{bash}\texttt{\small{ \# export CloudInit Path to nfs share}}\end{bash}
\begin{bash}\texttt{\small{ cat /etc/exports | grep "\$chpcInitPath"}}\end{bash}
\begin{bash}\texttt{\small{ chpcInitPath\_exported=\$?}}\end{bash}
\begin{bash}\texttt{\small{ }}\end{bash}
\begin{bash}\texttt{\small{ if [ "\${chpcInitPath\_exported}" -ne "0" ]; then}}\end{bash}
\begin{bash}\texttt{\small{     echo "\$chpcInitPath *(rw,no\_subtree\_check,no\_root\_squash)" >> /etc/exports}}\end{bash}
\begin{bash}\texttt{\small{ fi}}\end{bash}
\begin{bash}\texttt{\small{ \# share /home from HN}}\end{bash}
\begin{bash}\texttt{\small{ if ! grep "\^/home" /etc/exports; then}}\end{bash}
\begin{bash}\texttt{\small{     echo "/home *(rw,no\_subtree\_check,fsid=10,no\_root\_squash)" >> /etc/exports}}\end{bash}
\begin{bash}\texttt{\small{ fi}}\end{bash}
\begin{bash}\texttt{\small{ \# share /opt/ from HN}}\end{bash}
\begin{bash}\texttt{\small{ if ! grep "\^/opt/ohpc/pub" /etc/exports; then}}\end{bash}
\begin{bash}\texttt{\small{     echo "/opt/ohpc/pub *(ro,no\_subtree\_check,fsid=11)" >> /etc/exports}}\end{bash}
\begin{bash}\texttt{\small{ fi}}\end{bash}
\begin{bash}\texttt{\small{ exportfs -a}}\end{bash}
\begin{bash}\texttt{\small{ \# Restart nfs}}\end{bash}
\begin{bash}\texttt{\small{ systemctl restart nfs}}\end{bash}
\begin{bash}\texttt{\small{ systemctl enable nfs-server}}\end{bash}
\begin{bash}\texttt{\small{ logger "chpcInit: nfs configuration complete, updating remaining HPC configuration" }}\end{bash}

Configure ntp sever on sms node, as per the site setting.

\begin{bash}\texttt{\small{ \# configure NTP}}\end{bash}
\begin{bash}\texttt{\small{ systemctl enable ntpd}}\end{bash}
\begin{bash}\texttt{\small{ if [[ ! -z "\$ntp\_server" ]]; then}}\end{bash}
\begin{bash}\texttt{\small{    echo "server \${ntp\_server}" >> /etc/ntp.conf}}\end{bash}
\begin{bash}\texttt{\small{ fi}}\end{bash}
\begin{bash}\texttt{\small{ systemctl restart ntpd}}\end{bash}
\begin{bash}\texttt{\small{ systemctl enable ntpd.service}}\end{bash}
\begin{bash}\texttt{\small{ \# time sync}}\end{bash}
\begin{bash}\texttt{\small{ ntpstat}}\end{bash}
\begin{bash}\texttt{\small{ logger "chpcInit:ntp configuration done"}}\end{bash}

Distribute munge keys with compute nodes and then Update SLURM resource manager with hpc compute nodes.

\begin{bash}\texttt{\small{ \#\#\# Update Resource manager configuration \#\#\#}}\end{bash}
\begin{bash}\texttt{\small{ \# Update basic slurm configuration at sms node}}\end{bash}
\begin{bash}\texttt{\small{ perl -pi -e "s/ControlMachine=\S+/ControlMachine=\${sms\_name}/" /etc/slurm/slurm.conf}}\end{bash}
\begin{bash}\texttt{\small{ perl -pi -e "s/\^NodeName=(\S+)/NodeName=\${cnodename\_prefix}[1-\${num\_ccomputes}]/" /etc/slurm/slurm.conf}}\end{bash}
\begin{bash}\texttt{\small{ perl -pi -e "s/\^PartitionName=normal Nodes=(\S+)/PartitionName=normal Nodes=\${cnodename\_prefix}[1-\${num\_ccomputes}]/" /etc/slurm/slurm.conf}}\end{bash}
\begin{bash}\texttt{\small{ \# copy slurm file from sms node to Cloud Comute Nodes}}\end{bash}
\begin{bash}\texttt{\small{ cp -fpr -L /etc/slurm/slurm.conf \$chpcInitPath}}\end{bash}
\begin{bash}\texttt{\small{ cp -fpr -L /etc/pam.d/slurm \$chpcInitPath}}\end{bash}
\begin{bash}\texttt{\small{ cp -fpr -L /etc/munge/munge.key \$chpcInitPath}}\end{bash}
\begin{bash}\texttt{\small{ \# Start slurm and munge }}\end{bash}
\begin{bash}\texttt{\small{ systemctl enable munge}}\end{bash}
\begin{bash}\texttt{\small{ systemctl restart munge}}\end{bash}
\begin{bash}\texttt{\small{ systemctl enable slurmctld}}\end{bash}
\begin{bash}\texttt{\small{ systemctl restart slurmctld}}\end{bash}
\begin{bash}\texttt{\small{ \#systemctl enable slurmd}}\end{bash}
\begin{bash}\texttt{\small{ \#systemctl restart slurmd}}\end{bash}
\begin{bash}\texttt{\small{ logger "chpcInit:slurm configuration done"}}\end{bash}

One last step to make sure ssh is working and enabled on compute nodes. Update the permissions of ssh.

\begin{bash}\texttt{\small{ \#Change file permissions in /etc/ssh to fix ssh into compute node}}\end{bash}
\begin{bash}\texttt{\small{ chmod 0600 /etc/ssh/ssh\_host\_*\_key}}\end{bash}

Save the file with name chp\_sms\_cinit, we will use this file during sms node instance creation.

Prepare optional part of cloud-init

Update mrsh during cloud-init 

Create a new file sms/update\_mrsh and add mrsh configuration to enable mrsh on sms node. And save it. 


\begin{bash}\texttt{\small{ \# Update mrsh}}\end{bash}
\begin{bash}\texttt{\small{ \# check if it is already configured grep mshell /etc/services will return non-zero, else configure"}}\end{bash}
\begin{bash}\texttt{\small{ cat /etc/services | grep mshell}}\end{bash}
\begin{bash}\texttt{\small{ mshell\_exists=\$?}}\end{bash}
\begin{bash}\texttt{\small{ if [ "\${mshell\_exists}" -ne "0" ]; then}}\end{bash}
\begin{bash}\texttt{\small{     echo "mshell          21212/tcp                  \# mrshd" >> /etc/services}}\end{bash}
\begin{bash}\texttt{\small{ fi}}\end{bash}
\begin{bash}\texttt{\small{ cat /etc/services | grep mlogin}}\end{bash}
\begin{bash}\texttt{\small{ mlogin\_exists=\$?}}\end{bash}
\begin{bash}\texttt{\small{ if [ "\${mlogin\_exists}" -ne "0" ]; then}}\end{bash}
\begin{bash}\texttt{\small{     echo "mlogin            541/tcp                  \# mrlogind" >> /etc/services}}\end{bash}
\begin{bash}\texttt{\small{ fi}}\end{bash}

Updating cluster shell during cloud -init

Create a new file sms/update\_clustershell and add configuration to enable clustershell on sms node. And save it. 

\begin{bash}\texttt{\small{ sed -i -- 's/all: @adm,@compute/compute: cc[1-\${num\_ccomputes}]\\n\&/' /etc/clustershell/groups.d/local.cfg}}\end{bash}


Configuring overall cloud-init

In previous section we created template for cloud-init for hpc head node and hpc compute nodes. We need to update these template with user defined inputs like IP Address, node names. With these updates, cloud-init script is ready to deploy with OpenStack Nova.

Copy cloud-init template to working folder

\begin{bash}\texttt{\small{ chpcInitPath=/opt/ohpc/admin/cloud\_hpc\_init}}\end{bash}
\begin{bash}\texttt{\small{ \# if directory exists then mv to Old directory. TBD}}\end{bash}
\begin{bash}\texttt{\small{ mkdir -p \$chpcInitPath}}\end{bash}
\begin{bash}\texttt{\small{ \#copy Cloud HPC files to temp working directory}}\end{bash}
\begin{bash}\texttt{\small{ sudo cp -fr -L < \${SCRIPTDIR} >/ cloud\_hpc\_init/\${chpc\_base}/* \$chpcInitPath/}}\end{bash}
\begin{bash}\texttt{\small{ export chpcInit=\$chpcInitPath/chpc\_init}}\end{bash}
\begin{bash}\texttt{\small{ export chpcSMSInit=\$chpcInitPath/chpc\_sms\_init}}\end{bash}

Update sms\_ip in compute node cloud-init template with HPC head node. 

\begin{bash}\texttt{\small{ sudo sed -i -e "s/<sms\_ip>/\${sms\_ip}/g" \$chpcInit}}\end{bash}

Update HPC head node cloud-init template with compute name prefix as defined by user

\begin{bash}\texttt{\small{ sudo sed -i -e "s/<update\_cnodename\_prefix>/\${cnodename\_prefix}/g" \$chpcSMSInit}}\end{bash}
\begin{bash}\texttt{\small{ sudo sed -i -e "s/<update\_num\_ccomputes>/\${num\_ccomputes}/g" \$chpcSMSInit}}\end{bash}

Update hostname of HPC head node \& NTP server information

\begin{bash}\texttt{\small{ sudo sed -i -e "s/<update\_ntp\_server>/\${controller\_ip}/g" \$chpcSMSInit}}\end{bash}
\begin{bash}\texttt{\small{ sudo sed -i -e "s/<update\_sms\_name>/\${sms\_name}/g" \$chpcSMSInit}}\end{bash}

Optionally if user enabled mrsh or clusteshell, then update cloud-init accordingly

\begin{bash}\texttt{\small{ if [[ \${enable\_mrsh} -eq 1 ]];then}}\end{bash}
\begin{bash}\texttt{\small{    \# update mrsh for sms node}}\end{bash}
\begin{bash}\texttt{\small{    cat \$CHPC\_SCRIPTDIR/sms/update\_mrsh >> \$chpcSMSInit}}\end{bash}
\begin{bash}\texttt{\small{ fi}}\end{bash}
\begin{bash}\texttt{\small{ if [[ \${enable\_clustershell} -eq 1 ]];then}}\end{bash}
\begin{bash}\texttt{\small{    \# update clustershell for sms node}}\end{bash}
\begin{bash}\texttt{\small{    cat \$CHPC\_SCRIPTDIR/sms/update\_cluster}}\end{bash}shell >> \$chpcSMSInit
\begin{bash}\texttt{\small{ fi}}\end{bash}


\end{document}
