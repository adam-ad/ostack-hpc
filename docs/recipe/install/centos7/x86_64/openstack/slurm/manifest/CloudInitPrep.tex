OpenStack uses cloud-init for boot time initialization of cloud instances. This recipe relies on cloud-init to initialize HPC instances in an OpenStack cloud. This recipe prepares cloud-init initialization template script, which is than updated with sms-ip and other environment variables just before the provisioning. This is than fed as user data to Nova during instance creation. Script generated here will be executed by root during boot.
Preparing template for compute node cloud-init
Create an empty chpc_init file and open for editing. You can also use existing template and modify.
Start editing by adding some environment variable, first one is to set path to shared folder for cloud-init
chpcInitPath=/opt/ohpc/admin/cloud_hpc_init
logger "chpcInit: Updating Compute Node with HPC configuration"

Update rsyslog configuration file to send all the syslog to sms. Sms_ip is the tag used here is updated with IP of SMS node just before provisioning.
# Update rsyslog
cat /etc/rsyslog.conf | grep "<sms_ip>:514"
rsyslog_set=$?
if [ "${rsyslog_set}" -ne "0" ]; then
    echo "*.* @<sms_ip>:514" >> /etc/rsyslog.conf
fi
systemctl restart rsyslog
logger "chpcInit: rsyslog configuration complete, updating remaining HPC configuration"

Assuming sms node nfs share /home, /opt/ohpc/pub, =/opt/ohpc/admin/cloud_hpc_init lets mount them during boot
# nfs mount directory from SMS head node to Compute Node
cat /etc/fstab | grep "<sms_ip>:/home"
home_exists=$?
if [ "${home_exists}" -ne "0" ]; then
    echo "<sms_ip>:/home /home nfs nfsvers=3,rsize=1024,wsize=1024,cto 0 0" >> /etc/fstab
fi
cat /etc/fstab | grep "<sms_ip>:/opt/ohpc/pub"
ohpc_pub_exists=$?

if [ "${ohpc_pub_exists}" -ne "0" ]; then
    echo "<sms_ip>:/opt/ohpc/pub /opt/ohpc/pub nfs nfsvers=3 0 0" >> /etc/fstab
    # Make sure we have directory to mount
    # Clean up if required
    if [ -e /opt/ohpc/pub ]; then
        echo "chpcInit: [WARNING] /opt/ohpc/pub already exists!!"
    fi
fi
mkdir -p /opt/ohpc/pub
mount /home
mount /opt/ohpc/pub

# mount cloud_hpc_init
cat /etc/fstab | grep "sms_ip:$chpcInitPath"
CloudHPCInit_exist=$?
if [ "${CloudHPCInit_exist}" -ne "0" ]; then
    echo "<sms_ip>:$chpcInitPath $chpcInitPath nfs nfsvers=3 0 0" >> /etc/fstab
fi
mkdir -p $chpcInitPath
mount $chpcInitPath
# Restart nfs
systemctl restart nfs

have ntp sync with sms node. 
# Restart ntp at CN
systemctl enable ntpd
# Update ntp server
cat /etc/ntp.conf | grep "server <sms_ip>"
ntp_server_exists=$?
if [ "${ntp_server_exists}" -ne "0" ]; then
    echo "server <sms_ip>" >> /etc/ntp.conf
fi
systemctl restart ntpd
# time sync
Ntpstat
Sync sms node with compute nodes. sync users, slurm and enable munge by copying munge keys
# Sync following files to compute node
# Assuming nfs is setup properly
if [ -d $chpcInitPath ]; then
    # Update the slurm file
    cp -f -L $chpcInitPath/slurm.conf /etc/slurm/slurm.conf
    # Sync head node configuration with Compute Node
    cp -f -L $chpcInitPath/passwd /etc/passwd
    cp -f -L $chpcInitPath/group /etc/group
    cp -f -L $chpcInitPath/shadow /etc/shadow
    cp -f -L $chpcInitPath/slurm.conf /etc/slurm/slurm.conf
    cp -f -L $chpcInitPath/slurm /etc/pam.d/slurm
    cp -f -L $chpcInitPath/munge.key /etc/munge/munge.key
    # For hostname resolution
    cp -f -L $chpcInitPath/hosts /etc/hosts
    # make sure that hostname mentioned into /etc/hosts matches machine hostname. TBD
    # Start slurm and munge 
    systemctl enable munge
    systemctl restart munge
    systemctl enable slurmd
    systemctl restart slurmd
else
    logger "chpcInit:ERROR: cannot stat nfs shared /opt directory, cannot copy HPC system files"
fi
Update the hostname as per sms node.
# Setup hostname as per the head node
#Find the hostname of this machine from the copied over /etc/hosts file
cc_ipaddrs=(`hostname -I`)
for cc_ipaddr in ${cc_ipaddrs[@]}; do
    cat /etc/hosts | grep ${cc_ipaddr} > /dev/null
    result=$?
    if [ "$result" -eq "0" ]; then
        cc_hostname=`cat /etc/hosts | grep ${cc_ipaddr} | cut -d$'\t' -f2`
        break
    fi
done

if [ -z "${cc_hostname}" ]; then
    logger "chpcInit:ERROR: No resolved hostname found for any IP address in /etc/hosts"
    exit 1
fi

#set the hostname
if [ $(hostname) != ${cc_hostname} ]; then
    hostnamectl set-hostname ${cc_hostname}
fi

By now all pre-requisite for slurm is taken care, lets start slurm daemon.
# Start slurm and munge 
systemctl enable munge
systemctl restart munge
systemctl enable slurmd
systemctl restart slurmd
One last step to make sure ssh is working and enabled on compute nodes. Update the permissions of ssh.
#Change file permissions in /etc/ssh to fix ssh into compute node
chmod 0600 /etc/ssh/ssh_host_*_key

Save the file with name chp_cinit, we will use this file during baremetal node instance creation.

Preparing template for sms node cloud-init (chpc_sms_init)
Cloud init script for sms node is little different than compute node. Sms node, when instantiated within openstack, serve as a head node for HPC as a service and hosts all the services as a sms node in an independent hpc clusters. This will host server side of applications, resource manager, and share users. For more detail on sms node functionality please refer to OpenHPC documentation.
In this recipe will prepare cloud-init template script for sms node, which than is updated with compute node IP, ntp server and other environmental variables, just before provisioning. 
Create an empty chpc_init file and open for editing. You can also use existing template and modify. Start editing by adding some environment variable, which will be updated later, just before provisioning.
# Get the Compute node prefix and number of compute nodes
cnodename_prefix=<update_cnodename_prefix>
num_ccomputes=<update_num_ccomputes>
ntp_server=<update_ntp_server>
sms_name=<update_sms_name>

Now setup nfs share for cloud-init and files which want to send to compute nodes.
# setup cloudinit directory
chpcInitPath=/opt/ohpc/admin/cloud_hpc_init
# create directory of not exists
mkdir -p $chpcInitPath
chmod 700 $chpcInitPath
To create same user environment, copy user files 
# Copy other files needed for Cloud Init
sudo cp -fpr /etc/passwd $chpcInitPath
sudo cp -fpr /etc/shadow $chpcInitPath
sudo cp -fpr /etc/group $chpcInitPath

share /home, /opt/ohpc/pub and /opt/ohpc/admin/cloud_hpc_init over nfs
# export CloudInit Path to nfs share
cat /etc/exports | grep "$chpcInitPath"
chpcInitPath_exported=$?

if [ "${chpcInitPath_exported}" -ne "0" ]; then
    echo "$chpcInitPath *(rw,no_subtree_check,no_root_squash)" >> /etc/exports
fi
# share /home from HN
if ! grep "^/home" /etc/exports; then
    echo "/home *(rw,no_subtree_check,fsid=10,no_root_squash)" >> /etc/exports
fi
# share /opt/ from HN
if ! grep "^/opt/ohpc/pub" /etc/exports; then
    echo "/opt/ohpc/pub *(ro,no_subtree_check,fsid=11)" >> /etc/exports
fi
exportfs -a
# Restart nfs
systemctl restart nfs
systemctl enable nfs-server
logger "chpcInit: nfs configuration complete, updating remaining HPC configuration" 

Configure ntp sever on sms node, as per the site setting.
# configure NTP
systemctl enable ntpd
if [[ ! -z "$ntp_server" ]]; then
   echo "server ${ntp_server}" >> /etc/ntp.conf
fi
systemctl restart ntpd
systemctl enable ntpd.service
# time sync
ntpstat
logger "chpcInit:ntp configuration done"

Distribute munge keys with compute nodes and then Update SLURM resource manager with hpc compute nodes.

### Update Resource manager configuration ###
# Update basic slurm configuration at sms node
perl -pi -e "s/ControlMachine=\S+/ControlMachine=${sms_name}/" /etc/slurm/slurm.conf
perl -pi -e "s/^NodeName=(\S+)/NodeName=${cnodename_prefix}[1-${num_ccomputes}]/" /etc/slurm/slurm.conf
perl -pi -e "s/^PartitionName=normal Nodes=(\S+)/PartitionName=normal Nodes=${cnodename_prefix}[1-${num_ccomputes}]/" /etc/slurm/slurm.conf
# copy slurm file from sms node to Cloud Comute Nodes
cp -fpr -L /etc/slurm/slurm.conf $chpcInitPath
cp -fpr -L /etc/pam.d/slurm $chpcInitPath
cp -fpr -L /etc/munge/munge.key $chpcInitPath
# Start slurm and munge 
systemctl enable munge
systemctl restart munge
systemctl enable slurmctld
systemctl restart slurmctld
#systemctl enable slurmd
#systemctl restart slurmd
logger "chpcInit:slurm configuration done"

One last step to make sure ssh is working and enabled on compute nodes. Update the permissions of ssh.
#Change file permissions in /etc/ssh to fix ssh into compute node
chmod 0600 /etc/ssh/ssh_host_*_key

Save the file with name chp_sms_cinit, we will use this file during sms node instance creation.

Prepare optional part of cloud-init
Update mrsh during cloud-init 
Create a new file sms/update_mrsh and add mrsh configuration to enable mrsh on sms node. And save it. 
# Update mrsh
# check if it is already configured grep mshell /etc/services will return non-zero, else configure"
cat /etc/services | grep mshell
mshell_exists=$?
if [ "${mshell_exists}" -ne "0" ]; then
    echo "mshell          21212/tcp                  # mrshd" >> /etc/services
fi
cat /etc/services | grep mlogin
mlogin_exists=$?
if [ "${mlogin_exists}" -ne "0" ]; then
    echo "mlogin            541/tcp                  # mrlogind" >> /etc/services
fi

Updating cluster shell during cloud -inin
Create a new file sms/update_clustershell and add configuration to enable clustershell on sms node. And save it. 
sed -i -- 's/all: @adm,@compute/compute: cc[1-${num_ccomputes}]\n&/' /etc/clustershell/groups.d/local.cfg

Configuring overall cloud-init

