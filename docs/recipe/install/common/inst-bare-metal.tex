\subsubsection{Setup generic bare metal instance}

This section configure open stack for bare metal instance according to HPC images and user inputs. Before starting this it is assumed that system administrator has installed openstack and its services and has done appropriate configuration for bare metal provisioning, which includes installing ironinc, keystone, nova, neutron, glance. It is assumed that keystone is configured with 

Before instantiating bare metal nodes with HPC, we need to do little bit more configuration. 
Setup generic bare metal instance

This section configures the network for "HPC as a services", upload compute OS images to glance, create a flavor for bare metal and upload public keys for ssh session.

First create a generic network for "HPC as a service" with a name "sharednet1"

\begin{lstlisting}[language=bash,keywords={}]
(*\#*)Get the tenant ID for the services tenant
    SERVICES_TENANT_ID=`keystone tenant-list | grep "|\s*services\s*|" | awk '{print $2}'`

    (*\#*)Create the flat network on which you are going to launch instances
    neutron net-list | grep "|\s*sharednet1\s*|"
    net_exists=$?
    if [ "${net_exists}" -ne "0" ]; then
        neutron net-create --tenant-id ${SERVICES_TENANT_ID} sharednet1 --shared \
         --provider:network_type flat --provider:physical_network physnet1
    fi
    NEUTRON_NETWORK_UUID=`neutron net-list | grep "|\s*sharednet1\s*|" | awk '{print $2}'`
\end{lstlisting}


Create a subnet for our cluster with user defined start and end IP addresses. Make the controller as a gateway for our instances.

\begin{lstlisting}[language=bash,keywords={}]
(*\#*)Create the subnet on the newly created network
    neutron subnet-list | grep "|\s*subnet01\s*|"
    subnet_exists=$?
    if [ "${subnet_exists}" -ne "0" ]; then
        neutron subnet-create sharednet1 --name subnet01 --ip-version=4 \
         --gateway=${controller_ip} --allocation-pool \
          start=${cc_subnet_dhcp_start},end=${cc_subnet_dhcp_end} --enable-dhcp \
           ${cc_subnet_cidr}
    fi
    NEUTRON_SUBNET_UUID=`neutron subnet-list | grep "|\s*subnet01\s*|" | awk '{print $2}'`
\end{lstlisting}

Upload kernel and initrd images to glance service so that they are available to ironic while deploying node.

\begin{lstlisting}[language=bash,keywords={}]
(*\#*)Create the deploy-kernel and deploy-initrd images
    glance image-list | grep "|\s*deploy-vmlinuz\s*|"
    img_exists=$?
    if [ "${img_exists}" -ne "0" ]; then
        glance image-create --name deploy-vmlinuz --visibility public --disk-format \
        aki --container-format aki < ${chpc_image_deploy_kernel}
    fi
    DEPLOY_VMLINUZ_UUID=`glance image-list | grep "|\s*deploy-vmlinuz\s*|" | awk '{print $2}'`

    glance image-list | grep "|\s*deploy-initrd\s*|"
    img_exists=$?
    if [ "${img_exists}" -ne "0" ]; then
        glance image-create --name deploy-initrd --visibility public --disk-format \
        ari --container-format ari < ${chpc_image_deploy_ramdisk}
    fi
    DEPLOY_INITRD_UUID=`glance image-list | grep "|\s*deploy-initrd\s*|" | awk '{print $2}
\end{lstlisting}

Create a bare metal flavor with nova.

\begin{lstlisting}[language=bash,keywords={}]
(*\#*)Create the baremetal flavor and set the architecture to x86_64
    (*\#*) This will create common baremetal flavor, if SMS node & compute has different
    (*\#*) characteristic than user shall create multiple flavor one each characterisitc
    nova flavor-list | grep "|\s*baremetal-flavor\s*|"
    flavor_exists=$?
    if [ "$flavor_exists" -ne "0" ]; then
        nova flavor-create baremetal-flavor baremetal-flavor ${RAM_MB} ${DISK_GB} ${CPU}
        nova flavor-key baremetal-flavor set cpu_arch=$ARCH
    fi
    FLAVOR_UUID=`nova flavor-list | grep "|\s*baremetal-flavor\s*|" | awk '{print $2}'`
(*\#*)Increase the Quota limit for admin to allow nova boot
    openstack quota set --ram 512000 --cores 1000 --instances 100 admin
\end{lstlisting}

Finally register public ssh keys with nova, so that admin can ssh to the node.

\begin{lstlisting}[language=bash,keywords={}]
(*\#*)Register SSH keys with Nova
 nova keypair-list | grep "|\s*ostack_key\s*|"
 keypair_exists=$?
 if [ "${keypair_exists}" -ne "0" ]; then
    nova keypair-add --pub-key ${HOME}/.ssh/id_rsa.pub ostack_key
 fi
\end{lstlisting}

Export keypay name for use it later in other sections

\begin{lstlisting}[language=bash,keywords={}]
    KEYPAIR_NAME=ostack_key
\end{lstlisting}

\subsubsection{Setup HPC head node}


Previous section we created generic bare metal setup. In this section we will create configuration for HPC head node in an OpenStack cloud.
We created HPC head node OS images in previous sections, let's upload this image to glance, and store IMAGE id in environment variable SMS\_DISK\_IMAGE\_UUID, to be used during boot. 

\begin{lstlisting}[language=bash,keywords={}]
(*\#*) Create sms node image
   glance image-list | grep "|\s*sms-image\s*|"
   img_exists=$?
   if [ "${img_exists}" -ne "0" ]; then
       glance image-create --name sms-image --visibility public --disk-format \
       qcow2 --container-format bare < ${chpc_image_sms}
   fi
   SMS_DISK_IMAGE_UUID=`glance image-list | grep "|\s*sms-image\s*|" | awk '{print $2}'`
\end{lstlisting}

For provisioning sms node with ironic, we need to register node with ironic. This is done by registering node's BMC, node characteristic (aka flavor) like memory, cpu, disk space and node architecture. And registering kernel boot images. We will use pxe\_ipmitool as a provisioning driver in ironic with a boot mode as bios.

\begin{lstlisting}[language=bash,keywords={}]
(*\#*)Create a sms node in the bare metal service ironic.
    ironic node-list | grep "|\s*${sms_name}$\s*|"
    node_exists=$?
    if [ "${node_exists}" -ne "0" ]; then 
        ironic node-create -d pxe_ipmitool -i deploy_kernel=${DEPLOY_VMLINUZ_UUID} -i \
         deploy_ramdisk=${DEPLOY_INITRD_UUID} -i ipmi_terminal_port=8023 -i \
          ipmi_address=${sms_bmc} -i ipmi_username=${sms_bmc_username} -i \
           ipmi_password=${sms_bmc_password} -p cpus=${CPU} -p memory_mb=${RAM_MB} -p \
         local_gb=${DISK_GB} -p cpu_arch=${ARCH} -p capabilities="boot_mode:bios" \
         -n ${sms_name}
    fi
    SMS_UUID=`ironic node-list | grep "|\s*${sms_name}\s*|" | awk '{print $2}'`
\end{lstlisting}

Now we need tell ironic about the network port on which node will perform pxe boot by configuring MAC Address. 

	\begin{lstlisting}[language=bash,keywords={}]
    (*\#*)Add the associated port(s) MAC address to the created node(s)
    ironic port-create -n ${SMS_UUID} -a ${sms_mac}
	\end{lstlisting}

Add the instance info and disk space for root 
Add the instance\_info/image\_source and instance\_info/root\_gb
    
    \begin{lstlisting}[language=bash,keywords={}]
    ironic node-update $SMS_UUID add instance_info/image_source=${SMS_DISK_IMAGE_UUID} \
     instance_info/root_gb=50
	\end{lstlisting}


We will assign a fixed IP address to sms node. This is done by associating sms nodeâ€™s MAC address with neutron port. We will store this information in the neutron with sms\_name. we will also set environment SMS\_PORT\_ID variable with this port id, to be used during boot.
    
    \begin{lstlisting}[language=bash,keywords={}]
    (*\#*)Setup neutron port for static IP addressing of sms node, this is an optional part
    neutron port-create sharednet1 --dns_name $sms_name --fixed-ip ip_address=$sms_ip \
     --name $sms_name --mac-address $sms_mac
    SMS_PORT_ID=`neutron port-list | grep "|\s*$sms_name\s*|" | awk '{print $2}'`
	\end{lstlisting}


\subsubsection{Setup HPC compute nodes}

In previous section we configured Openstack to instantiate sms node. In this section we will be configuring openstack to instantiate HPC compute nodes.

For HPC compute nodes, we created compute node images, upload hpc compute node image to glance as a user image, and store IMAGE id in environment variable USER\_DISK\_IMAGE\_UUID, to be used during boot.

\begin{lstlisting}[language=bash,keywords={}]
(*\#*)Create the whole-disk-image from the user's qcow2 file
    glance image-list | grep "|\s*user-image\s*|"
    img_exists=$?
    if [ "${img_exists}" -ne "0" ]; then
        glance image-create --name user-image --visibility public --disk-format qcow2 \
         --container-format bare < ${chpc_image_user}
    fi
    USER_DISK_IMAGE_UUID=`glance image-list | grep "|\s*user-image\s*|" | awk '{print $2}'`
\end{lstlisting}


Similar to sms node, create setup for all compute nodes including creating ironic node, associating node MAC address, adding instance information and assigning fix IP address. In our example we used 4 hpc compute nodes. to store the information in each OpenStack component we will assign compute node host name as a name, which is host name prefix (as chosen by user in inputs), followed by a node counter. 

\begin{lstlisting}[language=bash,keywords={}]
(*\#*) Setup Compute nodes
(*\#*) Note: if installed from the rpm, the following script is installed as setup_compute_nodes.sh 

    for ((i=0; i < ${num_ccomputes}; i++)); do
        (*\#*)(*\#*)Create compute nodes in the bare metal service
        ironic node-list | grep "|\s*${cnodename_prefix}$((i+1))\s*|"
        node_exists=$?
        if [ "${node_exists}" -ne "0" ]; then
            ironic node-create -d pxe_ipmitool -i deploy_kernel=${DEPLOY_VMLINUZ_UUID} -i \
             deploy_ramdisk=${DEPLOY_INITRD_UUID} -i ipmi_terminal_port=8023 -i \
              ipmi_address=${cc_bmc[$i]} -i ipmi_username=${cc_bmc_username} -i \
               ipmi_password=${cc_bmc_password} -p cpus=${CPU} -p memory_mb=${RAM_MB} -p \
                local_gb=${DISK_GB} -p cpu_arch=${ARCH} -p capabilities="boot_mode:bios" -n \
                 ${cnodename_prefix}$((i+1))
        fi
        NODE_UUID_CC[$i]=`ironic node-list | grep "|\s*${cnodename_prefix}$((i+1))\s*|" | \
        awk '{print $2}'`

        (*\#*) update for compute nodes node MAC
        ironic port-create -n ${NODE_UUID_CC[$i]} -a ${cc_mac[$i]}

        (*\#*)Add the instance_info/image_source and instance_info/root_gb
        ironic node-update ${NODE_UUID_CC[$i]} add \
         instance_info/image_source=${USER_DISK_IMAGE_UUID} instance_info/root_gb=50

        (*\#*)Setup neutron port for static IP addressing of compute nodes
        cn_name=${cnodename_prefix}$((i+1))
        neutron port-create sharednet1 --dns_name $cn_name --fixed-ip ip_address=${cc_ip[$i]} \
         --name $cn_name --mac-address ${cc_mac[$i]}
        NEUTRON_PORT_ID_CC[$i]=`neutron port-list | grep "|\s*${cnodename_prefix}$((i+1))\s*|" \
         | awk '{print $2}'`
    Done
\end{lstlisting}


 Ironic periodically sync with Nova with available nodes. Nova then updates its record for all available hosts. So before booting the node with Nova allow some time to sync ironic with it. 


\begin{lstlisting}[language=bash,keywords={}]
(*\#*) Wait for the Nova hypervisor-stats to sync with available Ironic resources
sleep 121
\end{lstlisting}

\subsubsection{Boot SMS node}

In previous section we completed the bare metal configuration. User can request any available baremetal nodes by specifying the flavor they want and image they want to boot node with. For bare metal we created a flavor with name baremetal-flavor, we will provide this to nova with a CLI option --flavor. In our situation we will request 1 bare metal node with a baremetal flavor (--flavor) and SMS node image to boot (--image).  We also would like to reserve the IP address of this node. 

In previous section (setup sms) we associated one of the nodes MAC address with IP address, we will request this from nova by indicating port-id we created earlier (port-id=\${SMS\_PORT\_ID}). 

In a previous section we also created cloud-init script for sms nodes. We will provide cloud-init script (chpcSMSInit) to nova CLI option --user-data. For cloud init we will use metadata server, which will be provided by "--meta role= option". We will provide sms public key with "--key-name" option. At the end we will give our node a name. This name will be a host name of booted bare metal node.

Before booting, save boot command to a script, which will useful later on if user wants to re-instantiate same node.

\begin{lstlisting}[language=bash,keywords={}]
(*\#*)Boot the sms node with nova. chpcInit is set from prepare_cloudInit
echo "nova boot --config-drive true --flavor ${FLAVOR_UUID} --image ${SMS_DISK_IMAGE_UUID} \
 --key-name ${KEYPAIR_NAME} --meta role=webservers --user-data=$chpcSMSInit --nic \
 port-id=${SMS_PORT_ID} ${sms_name}" > boot_sms
\end{lstlisting}

Issue a boot command to nova to boot a SMS node:

\begin{lstlisting}[language=bash,keywords={}]
nova boot --config-drive true --flavor ${FLAVOR_UUID} --image ${SMS_DISK_IMAGE_UUID} --key-name
 ${KEYPAIR_NAME} --meta role=webservers --user-data=$chpcSMSInit --nic port-id=${SMS_PORT_ID} \
 ${sms_name}
\end{lstlisting}


Wait around 15 seconds before we boot compute nodes. This will allow enough time to boot SMS node before compute nodes starts. 

\begin{lstlisting}[language=bash,keywords={}]
sleep 15
\end{lstlisting}

\subsubsection{Boot compute nodes}

Booting compute nodes are very similar to SMS nodes. In our case we will boot 4 compute nodes (as specified in user inputs. Host name of compute node will use prefix defined by cnodename\_prefix variable, followed by node counter. For compute node we will use compute node image (USER\_DISK\_IMAGE\_UUID) and compute node cloud-init script (chpcInit). 


\begin{lstlisting}[language=bash,keywords={}]
for ((i=0; i < ${num_ccomputes}; i++)); do
filename="cn$((i+1))"
echo "nova boot --config-drive true --flavor ${FLAVOR_UUID} --image ${USER_DISK_IMAGE_UUID} \
 --key-name ${KEYPAIR_NAME} --meta role=webservers --user-data=$chpcInit --nic \
  port-id=${NEUTRON_PORT_ID_CC[$i]} ${cnodename_prefix}$((i+1))" > boot_$filename
nova boot --config-drive true --flavor ${FLAVOR_UUID} --image ${USER_DISK_IMAGE_UUID} \
 --key-name ${KEYPAIR_NAME} --meta role=webservers --user-data=$chpcInit --nic \
  port-id=${NEUTRON_PORT_ID_CC[$i]} ${cnodename_prefix}$((i+1))
(*\#*)wait for 5 sec before booting other compute node
sleep 5
done
\end{lstlisting}
