\subsubsection{Setup generic bare metal instance}

This section configure open stack for bare metal instance according to HPC images and user inputs. Before starting this it is assumed that system administrator has installed openstack and its services and has done appropriate configuration for bare metal provisioning, which includes installing ironic, keystone, nova, neutron, glance. It is assumed that keystone is configured with 

Before instantiating bare metal nodes with HPC, we need to do little bit more configuration. 
Setup generic bare metal instance

This section configures the network for "HPC as a services", upload compute OS images to glance, create a flavor for bare metal and upload public keys for ssh session.

First create a generic network for "HPC as a service" with a name "sharednet1"

% begin_ohpc_run
% ohpc_validation_newline
% ohpc_validation_comment # XFILEX
% ohpc_validation_comment # FILE: deploy_chpc_openstack part 2
% ohpc_command function setup_baremetal() {
\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) #Get the tenant ID for the services tenant
    SERVICES_TENANT_ID=`keystone tenant-list | grep "|\s*services\s*|" | awk '{print $2}'`
[ctrlr](*\#*) 
[ctrlr](*\#*) #Create the flat network on which you are going to launch instances
[ctrlr](*\#*)     neutron net-list | grep "|\s*sharednet1\s*|"
[ctrlr](*\#*)     net_exists=$?
[ctrlr](*\#*)     if [ "${net_exists}" -ne "0" ]; then
[ctrlr](*\#*)         neutron net-create --tenant-id ${SERVICES_TENANT_ID} sharednet1 --shared --provider:network_type flat --provider:physical_network physnet1
[ctrlr](*\#*)     fi
[ctrlr](*\#*)     NEUTRON_NETWORK_UUID=`neutron net-list | grep "|\s*sharednet1\s*|" | awk '{print $2}'`
\end{lstlisting}
% end_ohpc_run


Create a subnet for our cluster with user defined start and end IP addresses. Make the controller as a gateway for our instances.

% begin_ohpc_run
% ohpc_validation_newline

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) #Create the subnet on the newly created network
[ctrlr](*\#*)     neutron subnet-list | grep "|\s*subnet01\s*|"
[ctrlr](*\#*)     subnet_exists=$?
[ctrlr](*\#*)     if [ "${subnet_exists}" -ne "0" ]; then
[ctrlr](*\#*)         neutron subnet-create sharednet1 --name subnet01 --ip-version=4 --gateway=${controller_ip} --allocation-pool start=${cc_subnet_dhcp_start},end=${cc_subnet_dhcp_end} --enable-dhcp ${cc_subnet_cidr}
[ctrlr](*\#*)     fi
[ctrlr](*\#*)     NEUTRON_SUBNET_UUID=`neutron subnet-list | grep "|\s*subnet01\s*|" | [ctrlr](*\#*) awk '{print $2}'`
\end{lstlisting} 
% end_ohpc_run

Upload kernel and initrd images to glance service so that they are available to ironic while deploying node.

% begin_ohpc_run
% ohpc_validation_newline

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) #Create the deploy-kernel and deploy-initrd images
[ctrlr](*\#*)     glance image-list | grep "|\s*deploy-vmlinuz\s*|"
[ctrlr](*\#*)     img_exists=$?
[ctrlr](*\#*)     if [ "${img_exists}" -ne "0" ]; then
[ctrlr](*\#*)         glance image-create --name deploy-vmlinuz --visibility public --disk-format aki --container-format aki < ${chpc_image_deploy_kernel}
[ctrlr](*\#*)     fi
[ctrlr](*\#*)     DEPLOY_VMLINUZ_UUID=`glance image-list | grep "|\s*deploy-vmlinuz\s*|" | awk '{print $2}'`
[ctrlr](*\#*) 
[ctrlr](*\#*)     glance image-list | grep "|\s*deploy-initrd\s*|"
[ctrlr](*\#*)     img_exists=$?
[ctrlr](*\#*)     if [ "${img_exists}" -ne "0" ]; then
[ctrlr](*\#*)         glance image-create --name deploy-initrd --visibility public --disk-format ari --container-format ari < ${chpc_image_deploy_ramdisk}
[ctrlr](*\#*)     fi
[ctrlr](*\#*)     DEPLOY_INITRD_UUID=`glance image-list | grep "|\s*deploy-initrd\s*|" | awk '{print $2}
\end{lstlisting} 
% end_ohpc_run

Create a bare metal flavor with nova.

% begin_ohpc_run
% ohpc_validation_newline

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) #Create the baremetal flavor and set the architecture to x86_64
[ctrlr](*\#*) # This will create common baremetal flavor, if SMS node & compute has different
[ctrlr](*\#*) # characteristic than user shall create multiple flavor one each characterisitc
[ctrlr](*\#*)     nova flavor-list | grep "|\s*baremetal-flavor\s*|"
[ctrlr](*\#*)     flavor_exists=$?
[ctrlr](*\#*)     if [ "$flavor_exists" -ne "0" ]; then
[ctrlr](*\#*)         nova flavor-create baremetal-flavor baremetal-flavor ${RAM_MB} ${DISK_GB} ${CPU}
[ctrlr](*\#*)         nova flavor-key baremetal-flavor set cpu_arch=$ARCH
[ctrlr](*\#*)     fi
[ctrlr](*\#*)     FLAVOR_UUID=`nova flavor-list | grep "|\s*baremetal-flavor\s*|" | awk '{print $2}'`
[ctrlr](*\#*) #Increase the Quota limit for admin to allow nova boot
[ctrlr](*\#*)     openstack quota set --ram 512000 --cores 1000 --instances 100 admin
\end{lstlisting} 
% end_ohpc_run

Finally register public ssh keys with nova, so that admin can ssh to the node.

% begin_ohpc_run
% ohpc_validation_newline

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) #Register SSH keys with Nova
[ctrlr](*\#*)  nova keypair-list | grep "|\s*ostack_key\s*|"
[ctrlr](*\#*)  keypair_exists=$?
[ctrlr](*\#*)  if [ "${keypair_exists}" -ne "0" ]; then
[ctrlr](*\#*)     nova keypair-add --pub-key ${HOME}/.ssh/id_rsa.pub ostack_key
[ctrlr](*\#*)  fi
\end{lstlisting} 
% end_ohpc_run

Export keypair name for use it later in other sections

% begin_ohpc_run
% ohpc_validation_newline

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*)     KEYPAIR_NAME=ostack_key
\end{lstlisting} 
% ohpc_command }
% end_ohpc_run

\subsubsection{Setup HPC head node}


Previous section we created generic bare metal setup. In this section we will create configuration for HPC head node in an OpenStack cloud.
We created HPC head node OS images in previous sections, let's upload this image to glance, and store IMAGE id in environment variable SMS\_DISK\_IMAGE\_UUID, to be used during boot. 

% begin_ohpc_run
% ohpc_validation_newline
% ohpc_command function setup_sms() {
\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) # Create sms node image
[ctrlr](*\#*)    glance image-list | grep "|\s*sms-image\s*|"
[ctrlr](*\#*)    img_exists=$?
[ctrlr](*\#*)    if [ "${img_exists}" -ne "0" ]; then
[ctrlr](*\#*)        glance image-create --name sms-image --visibility public --disk-format qcow2 --container-format bare < ${chpc_image_sms}
[ctrlr](*\#*)    fi
[ctrlr](*\#*)    SMS_DISK_IMAGE_UUID=`glance image-list | grep "|\s*sms-image\s*|" | awk '{print $2}'`
\end{lstlisting} 
% end_ohpc_run

For provisioning sms node with ironic, we need to register node with ironic. This is done by registering node's BMC, node characteristic (aka flavor) like memory, cpu, disk space and node architecture. And registering kernel boot images. We will use pxe\_ipmitool as a provisioning driver in ironic with a boot mode as bios.

% begin_ohpc_run
% ohpc_validation_newline

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) #Create a sms node in the bare metal service ironic.
[ctrlr](*\#*)     ironic node-list | grep "|\s*${sms_name}$\s*|"
[ctrlr](*\#*)     node_exists=$?
[ctrlr](*\#*)     if [ "${node_exists}" -ne "0" ]; then 
[ctrlr](*\#*)         ironic node-create -d pxe_ipmitool -i deploy_kernel=${DEPLOY_VMLINUZ_UUID} -i deploy_ramdisk=${DEPLOY_INITRD_UUID} -i ipmi_terminal_port=8023 -i ipmi_address=${sms_bmc} -i ipmi_username=${sms_bmc_username} -i ipmi_password=${sms_bmc_password} -p cpus=${CPU} -p memory_mb=${RAM_MB} -p local_gb=${DISK_GB} -p cpu_arch=${ARCH} -p capabilities="boot_mode:bios" -n ${sms_name}
[ctrlr](*\#*)     fi
    SMS_UUID=`ironic node-list | grep "|\s*${sms_name}\s*|" | awk '{print $2}'`
\end{lstlisting} 
% end_ohpc_run

Now we need tell ironic about the network port on which node will perform pxe boot by configuring MAC Address. 


% begin_ohpc_run
% ohpc_validation_newline

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) #Add the associated port(s) MAC address to the created node(s)
[ctrlr](*\#*)     ironic port-create -n ${SMS_UUID} -a ${sms_mac}
\end{lstlisting} 
% end_ohpc_run

Add the instance info and disk space for root 
Add the instance\_info/image\_source and instance\_info/root\_gb
    
% begin_ohpc_run
% ohpc_validation_newline

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*)     ironic node-update $SMS_UUID add instance_info/image_source=${SMS_DISK_IMAGE_UUID} instance_info/root_gb=50
\end{lstlisting} 
% end_ohpc_run


We will assign a fixed IP address to sms node. This is done by associating sms nodeâ€™s MAC address with neutron port. We will store this information in the neutron with sms\_name. we will also set environment SMS\_PORT\_ID variable with this port id, to be used during boot.

% begin_ohpc_run
% ohpc_validation_newline
    
    \begin{lstlisting}[language=bash,keywords={}]
    [ctrlr](*\#*) #Setup neutron port for static IP addressing of sms node, this is an optional part
    [ctrlr](*\#*) neutron port-create sharednet1 --dns_name $sms_name --fixed-ip ip_address=$sms_ip --name $sms_name --mac-address $sms_mac
[ctrlr](*\#*)     SMS_PORT_ID=`neutron port-list | grep "|\s*$sms_name\s*|" | awk '{print $2}'`
	\end{lstlisting} 
% ohpc_command }
% end_ohpc_run


\subsubsection{Setup HPC compute nodes}

In previous section we configured Openstack to instantiate sms node. In this section we will be configuring openstack to instantiate HPC compute nodes.

For HPC compute nodes, we created compute node images, upload hpc compute node image to glance as a user image, and store IMAGE id in environment variable USER\_DISK\_IMAGE\_UUID, to be used during boot.

% begin_ohpc_run
% ohpc_validation_newline
% ohpc_command function setup_cn() {

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) #Create the whole-disk-image from the user's qcow2 file
[ctrlr](*\#*)     glance image-list | grep "|\s*user-image\s*|"
[ctrlr](*\#*)     img_exists=$?
[ctrlr](*\#*)     if [ "${img_exists}" -ne "0" ]; then
[ctrlr](*\#*)         glance image-create --name user-image --visibility public --disk-format qcow2 --container-format bare < ${chpc_image_user}
[ctrlr](*\#*)     fi
[ctrlr](*\#*)     USER_DISK_IMAGE_UUID=`glance image-list | grep "|\s*user-image\s*|" | awk '{print $2}'`
\end{lstlisting} 
% end_ohpc_run


Similar to sms node, create setup for all compute nodes including creating ironic node, associating node MAC address, adding instance information and assigning fix IP address. In our example we used 4 hpc compute nodes. to store the information in each OpenStack component we will assign compute node host name as a name, which is host name prefix (as chosen by user in inputs), followed by a node counter. 

% begin_ohpc_run
% ohpc_validation_newline

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) # Setup Compute nodes
[ctrlr](*\#*) # Note: if installed from the rpm, the following script is installed as setup_compute_nodes.sh 
[ctrlr](*\#*) 
[ctrlr](*\#*)     for ((i=0; i < ${num_ccomputes}; i++)); do
[ctrlr](*\#*)         (*\#*)(*\#*)Create compute nodes in the bare metal service
[ctrlr](*\#*)         ironic node-list | grep "|\s*${cnodename_prefix}$((i+1))\s*|"
[ctrlr](*\#*)         node_exists=$?
[ctrlr](*\#*)         if [ "${node_exists}" -ne "0" ]; then
[ctrlr](*\#*)             ironic node-create -d pxe_ipmitool -i deploy_kernel=${DEPLOY_VMLINUZ_UUID} -i deploy_ramdisk=${DEPLOY_INITRD_UUID} -i ipmi_terminal_port=8023 -i ipmi_address=${cc_bmc[$i]} -i ipmi_username=${cc_bmc_username} -i ipmi_password=${cc_bmc_password} -p cpus=${CPU} -p memory_mb=${RAM_MB} -p local_gb=${DISK_GB} -p cpu_arch=${ARCH} -p capabilities="boot_mode:bios" -n ${cnodename_prefix}$((i+1))
[ctrlr](*\#*)        fi
[ctrlr](*\#*)        NODE_UUID_CC[$i]=`ironic node-list | grep "|\s*${cnodename_prefix}$((i+1))\s*|" | awk '{print $2}'`

[ctrlr](*\#*)         (*\#*) update for compute nodes node MAC
[ctrlr](*\#*)         ironic port-create -n ${NODE_UUID_CC[$i]} -a ${cc_mac[$i]}

[ctrlr](*\#*)         (*\#*)Add the instance_info/image_source and instance_info/root_gb
[ctrlr](*\#*)         ironic node-update ${NODE_UUID_CC[$i]} add instance_info/image_source=${USER_DISK_IMAGE_UUID} instance_info/root_gb=50
[ctrlr](*\#*) 
[ctrlr](*\#*)         (*\#*)Setup neutron port for static IP addressing of compute nodes
[ctrlr](*\#*)         cn_name=${cnodename_prefix}$((i+1))
[ctrlr](*\#*)         neutron port-create sharednet1 --dns_name $cn_name --fixed-ip ip_address=${cc_ip[$i]} --name $cn_name --mac-address ${cc_mac[$i]}
[ctrlr](*\#*)         NEUTRON_PORT_ID_CC[$i]=`neutron port-list | grep "|\s*${cnodename_prefix}$((i+1))\s*|" | awk '{print $2}'`
[ctrlr](*\#*)     Done
\end{lstlisting} 
% end_ohpc_run


 Ironic periodically sync with Nova with available nodes. Nova then updates its record for all available hosts. So before booting the node with Nova allow some time to sync ironic with it. 

% begin_ohpc_run
% ohpc_validation_newline

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) # Wait for the Nova hypervisor-stats to sync with available Ironic resources
[ctrlr](*\#*) sleep 121
\end{lstlisting} 
% ohpc_command }
% end_ohpc_run

\subsubsection{Boot SMS node}

In previous section we completed the bare metal configuration. User can request any available baremetal nodes by specifying the flavor they want and image they want to boot node with. For bare metal we created a flavor with name baremetal-flavor, we will provide this to nova with a CLI option --flavor. In our situation we will request 1 bare metal node with a baremetal flavor (--flavor) and SMS node image to boot (--image).  We also would like to reserve the IP address of this node. 

In previous section (setup sms) we associated one of the nodes MAC address with IP address, we will request this from nova by indicating port-id we created earlier (port-id=\${SMS\_PORT\_ID}). 

In a previous section we also created cloud-init script for sms nodes. We will provide cloud-init script (chpcSMSInit) to nova CLI option --user-data. For cloud init we will use metadata server, which will be provided by "--meta role= option". We will provide sms public key with "--key-name" option. At the end we will give our node a name. This name will be a host name of booted bare metal node.

Before booting, save boot command to a script, which will useful later on if user wants to re-instantiate same node.

% begin_ohpc_run
% ohpc_validation_newline
% ohpc_command function boot_sms() {

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) #Boot the sms node with nova. chpcInit is set from prepare_cloudInit
[ctrlr](*\#*) echo "nova boot --config-drive true --flavor ${FLAVOR_UUID} --image ${SMS_DISK_IMAGE_UUID} --key-name ${KEYPAIR_NAME} --meta role=webservers --user-data=$chpcSMSInit --nic port-id=${SMS_PORT_ID} ${sms_name}" > boot_sms
\end{lstlisting} 
% end_ohpc_run

Issue a boot command to nova to boot a SMS node:

% begin_ohpc_run
% ohpc_validation_newline

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) nova boot --config-drive true --flavor ${FLAVOR_UUID} --image ${SMS_DISK_IMAGE_UUID} --key-name ${KEYPAIR_NAME} --meta role=webservers --user-data=$chpcSMSInit --nic port-id=${SMS_PORT_ID} ${sms_name}
\end{lstlisting} 
% end_ohpc_run

Wait around 15 seconds before we boot compute nodes. This will allow enough time to boot SMS node before compute nodes starts. 

% begin_ohpc_run
% ohpc_validation_newline

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) sleep 15
\end{lstlisting} 
% ohpc_command }
% end_ohpc_run


\subsubsection{Boot compute nodes}

Booting compute nodes are very similar to SMS nodes. In our case we will boot 4 compute nodes (as specified in user inputs. Host name of compute node will use prefix defined by cnodename\_prefix variable, followed by node counter. For compute node we will use compute node image (USER\_DISK\_IMAGE\_UUID) and compute node cloud-init script (chpcInit). 

% begin_ohpc_run
% ohpc_validation_newline
% ohpc_command function boot_cn() {

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) for ((i=0; i < ${num_ccomputes}; i++)); do
[ctrlr](*\#*) filename="cn$((i+1))"
[ctrlr](*\#*) echo "nova boot --config-drive true --flavor ${FLAVOR_UUID} --image ${USER_DISK_IMAGE_UUID} --key-name ${KEYPAIR_NAME} --meta role=webservers --user-data=$chpcInit --nic port-id=${NEUTRON_PORT_ID_CC[$i]} ${cnodename_prefix}$((i+1))" > boot_$filename
[ctrlr](*\#*) nova boot --config-drive true --flavor ${FLAVOR_UUID} --image ${USER_DISK_IMAGE_UUID} --key-name ${KEYPAIR_NAME} --meta role=webservers --user-data=$chpcInit --nic port-id=${NEUTRON_PORT_ID_CC[$i]} ${cnodename_prefix}$((i+1))
[ctrlr](*\#*) #wait for 5 sec before booting other compute node
[ctrlr](*\#*) sleep 5
[ctrlr](*\#*) done
\end{lstlisting}
% ohpc_command }
% ohpc_validation_comment ## Initial setup baremetal environment 
% ohpc_command setup_baremetal
% ohpc_validation_comment ## Setup SMS node first
% ohpc_command setup_sms
% ohpc_validation_comment ## Setup Compute nodes second
% ohpc_command setup_cn
% ohpc_validation_comment ## Wait for the Nova hypervisor-stats to sync with available Ironic resources
% ohpc_command sleep 121
% ohpc_validation_comment ## Boot sms node
% ohpc_command boot_sms
% ohpc_validation_comment ## # wait for 15 sec before starting to boot compute nodes. TBD need to tune this time
% ohpc_validation_comment ## # SMS node should be booted before compute nodes starts booting. At minimum
% ohpc_validation_comment ## # sms node shall have cloud init executed before CN's cloud init
% ohpc_command sleep 15
% ohpc_validation_comment ## # Now boot compute nodes
% ohpc_command boot_cn

% end_ohpc_run