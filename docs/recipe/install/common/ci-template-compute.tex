	Create an empty chpc\_init file and open it for editing. You can also modify the existing template. 

	Start editing the new file by adding environment variables. First, set the path to the shared folder for cloud-init

% begin_ohpc_run
% ohpc_validation_newline
% ohpc_validation_comment #   XFILEX
% ohpc_validation_comment #   PFILEP
% ohpc_command #!/bin/bash
% ohpc_validation_comment #   FILE: chpc_init

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) #Ensure the executing shell is in the same directory as the script.
[ctrlr](*\#*) SCRIPTDIR="$( cd "$( dirname "$( readlink -f "${BASH_SOURCE[0]}" )" )" && pwd -P && echo x)"
[ctrlr](*\#*) SCRIPTDIR="${SCRIPTDIR%x}"
[ctrlr](*\#*) cd $SCRIPTDIR
[ctrlr](*\#*) chpcInitPath=/opt/ohpc/admin/cloud_hpc_init

[ctrlr](*\#*) logger "chpcInit: Updating Compute Node with HPC configuration"
\end{lstlisting}
% end_ohpc_run

	Update the rsyslog configuration file to send the syslog to sms. "sms\_ip" is the tag used, which is updated with the IP address of the SMS node just before provisioning.

% begin_ohpc_run
% ohpc_validation_newline
\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) # Update rsyslog
[ctrlr](*\#*) cat /etc/rsyslog.conf | grep "<sms_ip>:514"
[ctrlr](*\#*) rsyslog_set=$?
[ctrlr](*\#*) if [ "${rsyslog_set}" -ne "0" ]; then
[ctrlr](*\#*)    echo "*.* @<sms_ip>:514" >> /etc/rsyslog.conf
[ctrlr](*\#*) fi
[ctrlr](*\#*) systemctl restart rsyslog
[ctrlr](*\#*) logger "chpcInit: rsyslog configuration complete, updating remaining HPC configuration"
\end{lstlisting}
% end_ohpc_run

	Assuming sms node share the directories "/home", "/opt/ohpc/pub", and "/opt/ohpc/admin/cloud\_hpc\_init" via nfs, we will mount them on compute nodes during boot.

% begin_ohpc_run
% ohpc_validation_newline
\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) # nfs mount directory from SMS head node to Compute Node
[ctrlr](*\#*) cat /etc/fstab | grep "<sms_ip>:/home"
[ctrlr](*\#*) home_exists=$?
[ctrlr](*\#*) if [ "${home_exists}" -ne "0" ]; then
[ctrlr](*\#*)     echo "<sms_ip>:/home /home nfs nfsvers=3,rsize=1024,wsize=1024,cto 0 0" >> /etc/fstab
[ctrlr](*\#*) fi
[ctrlr](*\#*) cat /etc/fstab | grep "<sms_ip>:/opt/ohpc/pub"
[ctrlr](*\#*) ohpc_pub_exists=$?
[ctrlr](*\#*) 
[ctrlr](*\#*) if [ "${ohpc_pub_exists}" -ne "0" ]; then
[ctrlr](*\#*)     echo "<sms_ip>:/opt/ohpc/pub /opt/ohpc/pub nfs nfsvers=3 0 0" >> /etc/fstab
[ctrlr](*\#*)     # Make sure we have directory to mount
[ctrlr](*\#*)     # Clean up if required
[ctrlr](*\#*)     if [ -e /opt/ohpc/pub ]; then
[ctrlr](*\#*)         echo "chpcInit: [WARNING] /opt/ohpc/pub already exists!!"
[ctrlr](*\#*)     fi
[ctrlr](*\#*) fi
[ctrlr](*\#*) mkdir -p /opt/ohpc/pub
[ctrlr](*\#*) mount /home
[ctrlr](*\#*) mount /opt/ohpc/pub
[ctrlr](*\#*) 
[ctrlr](*\#*) # Mount cloud_hpc_init
[ctrlr](*\#*) cat /etc/fstab | grep "sms_ip:$chpcInitPath"
[ctrlr](*\#*) CloudHPCInit_exist=$?
[ctrlr](*\#*) if [ "${CloudHPCInit_exist}" -ne "0" ]; then
[ctrlr](*\#*)     echo "<sms_ip>:$chpcInitPath $chpcInitPath nfs nfsvers=3 0 0" >> /etc/fstab
[ctrlr](*\#*) fi
[ctrlr](*\#*) mkdir -p $chpcInitPath
[ctrlr](*\#*) mount $chpcInitPath
[ctrlr](*\#*) # Restart nfs
[ctrlr](*\#*) systemctl restart nfs
[ctrlr](*\#*) 
\end{lstlisting}
% end_ohpc_run

	Have ntp sync with the sms node and then copy shared keys.

% begin_ohpc_run
% ohpc_validation_newline

\begin{lstlisting}
[ctrlr](*\#*) # Restart ntp at the compute node
[ctrlr](*\#*) systemctl enable ntpd
[ctrlr](*\#*) # Update ntp server
[ctrlr](*\#*) cat /etc/ntp.conf | grep "server <sms_ip>"
[ctrlr](*\#*) ntp_server_exists=$?
[ctrlr](*\#*) if [ "${ntp_server_exists}" -ne "0" ]; then
[ctrlr](*\#*)     echo "server <sms_ip>" >> /etc/ntp.conf
[ctrlr](*\#*) fi
[ctrlr](*\#*) systemctl restart ntpd
[ctrlr](*\#*) # Sync time
[ctrlr](*\#*) ntpstat
[ctrlr](*\#*) #Sync sms node with compute nodes. sync users, HPC resource manager and enable munge by copying munge keys
[ctrlr](*\#*) # Sync following files to compute node
[ctrlr](*\#*) # Assuming nfs is setup properly
[ctrlr](*\#*) if [ -d $chpcInitPath ]; then
[ctrlr](*\#*)     # Update the slurm file
[ctrlr](*\#*)     cp -f -L $chpcInitPath/slurm.conf /etc/slurm/slurm.conf
[ctrlr](*\#*)     # Copy public keys
[ctrlr](*\#*)     cp -f -L $chpcInitPath/authorized_keys /root/.ssh/
[ctrlr](*\#*)     cp -f -L $chpcInitPath/slurm.conf /etc/slurm/slurm.conf
[ctrlr](*\#*)     cp -f -L $chpcInitPath/slurm /etc/pam.d/slurm
[ctrlr](*\#*)     cp -f -L $chpcInitPath/munge.key /etc/munge/munge.key
[ctrlr](*\#*)     # make sure that hostname mentioned into /etc/hosts matches machine hostname. 
[ctrlr](*\#*)     # Start hpc resource manager 
[ctrlr](*\#*)     systemctl enable munge
[ctrlr](*\#*)     systemctl restart munge
[ctrlr](*\#*)     systemctl enable slurmd
[ctrlr](*\#*)     systemctl restart slurmd
[ctrlr](*\#*) else
[ctrlr](*\#*)     logger "chpcInit:ERROR: cannot stat nfs shared /opt directory, cannot copy HPC system files"
[ctrlr](*\#*) fi
\end{lstlisting}
% end_ohpc_run

	At this point, all pre-requisites for HPC resource manager should be met. Let's start the daemon.

% begin_ohpc_run
% ohpc_validation_newline

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) # Start slurm and munge 
[ctrlr](*\#*) systemctl enable munge
[ctrlr](*\#*) systemctl restart munge
[ctrlr](*\#*) systemctl enable slurmd
[ctrlr](*\#*) systemctl restart slurmd
\end{lstlisting}
%end_ohpc_run

	Complete the one last step to ensure sure ssh is working and enabled on the compute node. Update/verify the permissions of ssh.

% begin_ohpc_run
% ohpc_validation_newline

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) #Change file permissions in /etc/ssh to fix ssh into compute node
[ctrlr](*\#*) chmod 0600 /etc/ssh/ssh_host_*_key
[ctrlr](*\#*) logger "chpcInit: Complete"
\end{lstlisting}
% end_ohpc_run

	Save the file with name 'chpc\_init', we will use  file during the creation of baremetal node instances.

