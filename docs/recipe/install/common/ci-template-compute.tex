	Create an empty chpc\_init file and open for editing. You can also use  existing template and modify.

	Start editing by adding some environment variable, first one is to set path to shared folder for cloud-init

% begin_ohpc_run
% ohpc_validation_newline
% ohpc_validation_comment #   XFILEX
% ohpc_validation_comment #   PFILEP
% ohpc_command #!/bin/bash
% ohpc_validation_comment #   FILE: chpc_init

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) #Ensure the executing shell is in the same directory as the script.
[ctrlr](*\#*) SCRIPTDIR="$( cd "$( dirname "$( readlink -f "${BASH_SOURCE[0]}" )" )" && pwd -P && echo x)"
[ctrlr](*\#*) SCRIPTDIR="${SCRIPTDIR%x}"
[ctrlr](*\#*) cd $SCRIPTDIR
[ctrlr](*\#*) chpcInitPath=/opt/ohpc/admin/cloud_hpc_init

[ctrlr](*\#*) logger "chpcInit: Updating Compute Node with HPC configuration"
\end{lstlisting}
% end_ohpc_run

	Update rsyslog configuration file to send all the syslog to sms. sms\_ip is the tag used here is updated with IP of SMS node just before provisioning.

% begin_ohpc_run
% ohpc_validation_newline
\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) # Update rsyslog
[ctrlr](*\#*) cat /etc/rsyslog.conf | grep "<sms_ip>:514"
[ctrlr](*\#*) rsyslog_set=$?
[ctrlr](*\#*) if [ "${rsyslog_set}" -ne "0" ]; then
[ctrlr](*\#*)    echo "*.* @<sms_ip>:514" >> /etc/rsyslog.conf
[ctrlr](*\#*) fi
[ctrlr](*\#*) systemctl restart rsyslog
[ctrlr](*\#*) logger "chpcInit: rsyslog configuration complete, updating remaining HPC configuration"
\end{lstlisting}
% end_ohpc_run

	Assuming sms node nfs share /home, /opt/ohpc/pub, =/opt/ohpc/admin/cloud\_hpc\_init lets mount them during boot

% begin_ohpc_run
% ohpc_validation_newline
\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) # nfs mount directory from SMS head node to Compute Node
[ctrlr](*\#*) cat /etc/fstab | grep "<sms_ip>:/home"
[ctrlr](*\#*) home_exists=$?
[ctrlr](*\#*) if [ "${home_exists}" -ne "0" ]; then
[ctrlr](*\#*)     echo "<sms_ip>:/home /home nfs nfsvers=3,rsize=1024,wsize=1024,cto 0 0" >> /etc/fstab
[ctrlr](*\#*) fi
[ctrlr](*\#*) cat /etc/fstab | grep "<sms_ip>:/opt/ohpc/pub"
[ctrlr](*\#*) ohpc_pub_exists=$?
[ctrlr](*\#*) 
[ctrlr](*\#*) if [ "${ohpc_pub_exists}" -ne "0" ]; then
[ctrlr](*\#*)     echo "<sms_ip>:/opt/ohpc/pub /opt/ohpc/pub nfs nfsvers=3 0 0" >> /etc/fstab
[ctrlr](*\#*)     # Make sure we have directory to mount
[ctrlr](*\#*)     # Clean up if required
[ctrlr](*\#*)     if [ -e /opt/ohpc/pub ]; then
[ctrlr](*\#*)         echo "chpcInit: [WARNING] /opt/ohpc/pub already exists!!"
[ctrlr](*\#*)     fi
[ctrlr](*\#*) fi
[ctrlr](*\#*) mkdir -p /opt/ohpc/pub
[ctrlr](*\#*) mount /home
[ctrlr](*\#*) mount /opt/ohpc/pub
[ctrlr](*\#*) 
[ctrlr](*\#*) # Mount cloud_hpc_init
[ctrlr](*\#*) cat /etc/fstab | grep "sms_ip:$chpcInitPath"
[ctrlr](*\#*) CloudHPCInit_exist=$?
[ctrlr](*\#*) if [ "${CloudHPCInit_exist}" -ne "0" ]; then
[ctrlr](*\#*)     echo "<sms_ip>:$chpcInitPath $chpcInitPath nfs nfsvers=3 0 0" >> /etc/fstab
[ctrlr](*\#*) fi
[ctrlr](*\#*) mkdir -p $chpcInitPath
[ctrlr](*\#*) mount $chpcInitPath
[ctrlr](*\#*) # Restart nfs
[ctrlr](*\#*) systemctl restart nfs
[ctrlr](*\#*) 
\end{lstlisting}
% end_ohpc_run

	Have ntp sync with the sms node. 

% begin_ohpc_run
% ohpc_validation_newline

\begin{lstlisting}
[ctrlr](*\#*) # Restart ntp at CN
[ctrlr](*\#*) systemctl enable ntpd
[ctrlr](*\#*) # Update ntp server
[ctrlr](*\#*) cat /etc/ntp.conf | grep "server <sms_ip>"
[ctrlr](*\#*) ntp_server_exists=$?
[ctrlr](*\#*) if [ "${ntp_server_exists}" -ne "0" ]; then
[ctrlr](*\#*)     echo "server <sms_ip>" >> /etc/ntp.conf
[ctrlr](*\#*) fi
[ctrlr](*\#*) systemctl restart ntpd
[ctrlr](*\#*) # Sync time
[ctrlr](*\#*) ntpstat
[ctrlr](*\#*) #Sync sms node with compute nodes. sync users, slurm and enable munge by copying munge keys
[ctrlr](*\#*) # Sync following files to compute node
[ctrlr](*\#*) # Assuming nfs is setup properly
[ctrlr](*\#*) if [ -d $chpcInitPath ]; then
[ctrlr](*\#*)     # Update the slurm file
[ctrlr](*\#*)     cp -f -L $chpcInitPath/slurm.conf /etc/slurm/slurm.conf
[ctrlr](*\#*)     # Copy public keys
[ctrlr](*\#*)     cp -f -L $chpcInitPath/authorized_keys /root/.ssh/
[ctrlr](*\#*)     cp -f -L $chpcInitPath/slurm.conf /etc/slurm/slurm.conf
[ctrlr](*\#*)     cp -f -L $chpcInitPath/slurm /etc/pam.d/slurm
[ctrlr](*\#*)     cp -f -L $chpcInitPath/munge.key /etc/munge/munge.key
[ctrlr](*\#*)     # For hostname resolution
[ctrlr](*\#*)     # cp -f -L $chpcInitPath/hosts /etc/hosts
[ctrlr](*\#*)     # make sure that hostname mentioned into /etc/hosts matches machine hostname. TBD
[ctrlr](*\#*)     # Start slurm and munge 
[ctrlr](*\#*)     systemctl enable munge
[ctrlr](*\#*)     systemctl restart munge
[ctrlr](*\#*)     systemctl enable slurmd
[ctrlr](*\#*)     systemctl restart slurmd
[ctrlr](*\#*) else
[ctrlr](*\#*)     logger "chpcInit:ERROR: cannot stat nfs shared /opt directory, cannot copy HPC system files"
[ctrlr](*\#*) fi
\end{lstlisting}
% end_ohpc_run

	At this point, all pre-requisites for slurm should be met. Let's start the slurm daemon.

% begin_ohpc_run
% ohpc_validation_newline

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) # Start slurm and munge 
[ctrlr](*\#*) systemctl enable munge
[ctrlr](*\#*) systemctl restart munge
[ctrlr](*\#*) systemctl enable slurmd
[ctrlr](*\#*) systemctl restart slurmd
\end{lstlisting}
%end_ohpc_run

	One last step to make sure ssh is working and enabled on compute nodes. Update/verify the permissions of ssh.

% begin_ohpc_run
% ohpc_validation_newline

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) #Change file permissions in /etc/ssh to fix ssh into compute node
[ctrlr](*\#*) chmod 0600 /etc/ssh/ssh_host_*_key
[ctrlr](*\#*) logger "chpcInit: Complete"
\end{lstlisting}
% end_ohpc_run

	Save the file with name chpc\_init, we will use this file during baremetal node instance creation.

