	The cloud-init script for sms node is little different than compute node. The SMS node, when instantiated within Openstack, serves as a head node for HPC-as-a-Service, and hosts all the services as a sms node in an independent HPC cluster. This will host the server side of applications, resource manager, and share users. For more detail on sms node functionality please refer to OpenHPC documentation.

	In this recipe, we will prepare cloud-init template script for sms node, which then is updated with compute node IP, NTP server, and other environmental variables, just before provisioning. 

	Create an empty chpc\_init file and open for editing. You can also use existing template and modify. Start editing by adding some environment variable, which will be updated later, just before provisioning.

% begin_ohpc_run
% ohpc_validation_newline
% ohpc_validation_comment #   PFILEP

% ohpc_validation_comment # FILE: chpc_sms_init

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) # Get the Compute node prefix and number of compute nodes
[ctrlr](*\#*) cnodename_prefix=<update_cnodename_prefix>
[ctrlr](*\#*) num_ccomputes=<update_num_ccomputes>
[ctrlr](*\#*) ntp_server=<update_ntp_server>
[ctrlr](*\#*) sms_name=<update_sms_name>
[ctrlr](*\#*) logger "chpcInit: Entered chpcInit"
[ctrlr](*\#*) #Ensure the executing shell is in the same directory as the script.
[ctrlr](*\#*) SCRIPTDIR="$( cd "$( dirname "$( readlink -f "${BASH_SOURCE[0]}" )" )" && pwd -P && echo x)"
[ctrlr](*\#*) SCRIPTDIR="${SCRIPTDIR%x}"
[ctrlr](*\#*) cd $SCRIPTDIR

\end{lstlisting} 
 % end_ohpc_run

	Now, setup nfs share for cloud-init and files which want to send to compute nodes.

% begin_ohpc_run
% ohpc_validation_newline

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) # setup cloudinit directory
[ctrlr](*\#*) chpcInitPath=/opt/ohpc/admin/cloud_hpc_init
[ctrlr](*\#*) # create directory of not exists
[ctrlr](*\#*) mkdir -p $chpcInitPath
[ctrlr](*\#*) chmod 700 $chpcInitPath
[ctrlr](*\#*) # To create same user environment, copy user files 
[ctrlr](*\#*) # Copy other files needed for Cloud Init
[ctrlr](*\#*) sudo cp -fpr /etc/passwd $chpcInitPath
[ctrlr](*\#*) sudo cp -fpr /etc/shadow $chpcInitPath
[ctrlr](*\#*) sudo cp -fpr /etc/group $chpcInitPath
[ctrlr](*\#*) # Copy public ssh key to shared drive
[ctrlr](*\#*) _ssh_path=/root/.ssh
[ctrlr](*\#*) if [ ! -e "$_ssh_path/hpcasservice" ]; then
[ctrlr](*\#*) 
[ctrlr](*\#*) 	if [ ! -d "$_ssh_path" ]; then
[ctrlr](*\#*) 		install -d -m 700 $_ssh_path
[ctrlr](*\#*) 	fi
[ctrlr](*\#*) 	ssh-keygen -t dsa -f $_ssh_path/hpcasservice -N '' -C "HPC Cluster key" > /dev/null 2>&1
[ctrlr](*\#*) 	cat $_ssh_path/hpcasservice.pub >> $_ssh_path/authorized_keys
[ctrlr](*\#*) 	chmod 0600 $_ssh_path/authorized_keys
[ctrlr](*\#*) fi
[ctrlr](*\#*) 	#update config
[ctrlr](*\#*) if [ ! -e "$_ssh_path/config" ]; then
[ctrlr](*\#*) 	echo "Host *" > $_ssh_path/config
[ctrlr](*\#*) 	echo "    IdentityFile ~/.ssh/hpcasservice" >> $_ssh_path/config
[ctrlr](*\#*) 	echo "    StrictHostKeyChecking=no" >> $_ssh_path/config
[ctrlr](*\#*) fi
[ctrlr](*\#*) cp -fpr $_ssh_path/authorized_keys $chpcInitPath

\end{lstlisting} 
 % end_ohpc_run

	Share /home, /opt/ohpc/pub and /opt/ohpc/admin/cloud\_hpc\_init over nfs

% begin_ohpc_run
% ohpc_validation_newline

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) # export CloudInit Path to nfs share
[ctrlr](*\#*) cat /etc/exports | grep "$chpcInitPath"
[ctrlr](*\#*) chpcInitPath_exported=$?
[ctrlr](*\#*) 
[ctrlr](*\#*) if [ "${chpcInitPath_exported}" -ne "0" ]; then
[ctrlr](*\#*)    echo "$chpcInitPath *(rw,no_subtree_check,no_root_squash)" >> /etc/exports
[ctrlr](*\#*) fi
[ctrlr](*\#*) # share /home from HN
[ctrlr](*\#*) if ! grep "^/home" /etc/exports; then
[ctrlr](*\#*)     echo "/home *(rw,no_subtree_check,fsid=10,no_root_squash)" >> /etc/exports
[ctrlr](*\#*) fi
[ctrlr](*\#*) # share /opt/ from HN
[ctrlr](*\#*) if ! grep "^/opt/ohpc/pub" /etc/exports; then
[ctrlr](*\#*)     echo "/opt/ohpc/pub *(ro,no_subtree_check,fsid=11)" >> /etc/exports
[ctrlr](*\#*) fi
[ctrlr](*\#*) exportfs -a
[ctrlr](*\#*) # Restart nfs
[ctrlr](*\#*) systemctl restart nfs
[ctrlr](*\#*) systemctl enable nfs-server
[ctrlr](*\#*) logger "chpcInit: nfs configuration complete, updating remaining HPC configuration" 
\end{lstlisting} 
 % end_ohpc_run

	Configure ntp sever on sms node, as per the site setting.

% begin_ohpc_run
% ohpc_validation_newline

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) # configure NTP
[ctrlr](*\#*) systemctl enable ntpd
[ctrlr](*\#*) if [[ ! -z "$ntp_server" ]]; then
[ctrlr](*\#*)    echo "server ${ntp_server}" >> /etc/ntp.conf
[ctrlr](*\#*) fi
[ctrlr](*\#*) systemctl restart ntpd
[ctrlr](*\#*) systemctl enable ntpd.service
[ctrlr](*\#*) # time sync
[ctrlr](*\#*) ntpstat
[ctrlr](*\#*) logger "chpcInit:ntp configuration done"
\end{lstlisting} 
 % end_ohpc_run

	Distribute munge keys with compute nodes, and then update the SLURM resource manager with HPC compute nodes.


% begin_ohpc_run
% ohpc_validation_newline

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) #(*\#*)(*\#*) Update Resource manager configuration (*\#*)(*\#*)(*\#*)
[ctrlr](*\#*) # Update basic slurm configuration at sms node
[ctrlr](*\#*) perl -pi -e "s/ControlMachine=\S+/ControlMachine=${sms_name}/" /etc/slurm/slurm.conf
[ctrlr](*\#*) perl -pi -e "s/^NodeName=(\S+)/NodeName=${cnodename_prefix}[1-${num_ccomputes}]/" /etc/slurm/slurm.conf
[ctrlr](*\#*) perl -pi -e "s/^PartitionName=normal Nodes=(\S+)/PartitionName=normal Nodes=${cnodename_prefix}[1-${num_ccomputes}]/" /etc/slurm/slurm.conf
[ctrlr](*\#*) # copy slurm file from sms node to Cloud Comute Nodes
[ctrlr](*\#*) cp -fpr -L /etc/slurm/slurm.conf $chpcInitPath
[ctrlr](*\#*) cp -fpr -L /etc/pam.d/slurm $chpcInitPath
[ctrlr](*\#*) cp -fpr -L /etc/munge/munge.key $chpcInitPath
[ctrlr](*\#*) # Start slurm and munge 
[ctrlr](*\#*) systemctl enable munge
[ctrlr](*\#*) systemctl restart munge
[ctrlr](*\#*) systemctl enable slurmctld
[ctrlr](*\#*) systemctl restart slurmctld
[ctrlr](*\#*) #systemctl enable slurmd
[ctrlr](*\#*) #systemctl restart slurmd
[ctrlr](*\#*) logger "chpcInit:slurm configuration done"
\end{lstlisting} 
 % end_ohpc_run


	One last step is to make sure ssh is working and enabled on compute nodes. Update/verify the permissions of ssh.

% begin_ohpc_run
% ohpc_validation_newline

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) #Change file permissions in /etc/ssh to fix ssh into compute node
[ctrlr](*\#*) chmod 0600 /etc/ssh/ssh_host_*_key
\end{lstlisting} 
 % end_ohpc_run

	Save the file with name chp\_sms\_cinit, we will use this file during sms node instance creation.

