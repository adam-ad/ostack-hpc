
# #!/bin/bash
# # FILE:  prepare_chpc_openstack ---------------------------------------------------------------------------------
# This script installs and configures ironic for baremetal provisioning on CentOS 7
# Using the OpenStack-Mitaka release.
# This relies on the packstack installation to happen first and the keystonerc_admin
# file being in the user's home directory. It is assumed this script is run with
# sudo permissions.
# ---------------------------------------------------------------------------------
# Set SELinux to permissive
#
setenforce 0
#
#  
# Source the keystonerc_admin file
#
source ${HOME}/keystonerc_admin
#
#  
# #Create roles for the baremetal service. These can be used later to give special permissions to baremetal. This script will be using the defaults.
openstack role list | grep -i baremetal_admin
#
role_exists=$?
if [ "${role_exists}" -ne "0" ]; then 
    openstack role create baremetal_admin
fi
openstack role list | grep -i baremetal_observer
role_exists=$?
if [ "${role_exists}" -ne "0" ]; then 
    openstack role create baremetal_observer
fi
systemctl restart openstack-ironic-api
#
# #Ensure the utilities for baremetal are installed
#
yum install -y tftp-server syslinux-tftpboot xinetd
#
# #Make the directory for tftp and give it the ironic owner
#
mkdir -p /tftpboot
chown -R ironic /tftpboot
#
# #Configure /etc/xinet.d/tftp
#
echo "service tftp" > /etc/xinetd.d/tftp
echo "{" >> /etc/xinetd.d/tftp
echo "  protocol        = udp" >> /etc/xinetd.d/tftp
echo "  port            = 69" >> /etc/xinetd.d/tftp
echo "  socket_type     = dgram" >> /etc/xinetd.d/tftp
echo "  wait            = yes" >> /etc/xinetd.d/tftp
echo "  user            = root" >> /etc/xinetd.d/tftp
echo "  server          = /usr/sbin/in.tftpd" >> /etc/xinetd.d/tftp
echo "  server_args     = -v -v -v -v -v --map-file /tftpboot/map-file /tftpboot" >> /etc/xinetd.d/tftp
echo "  disable         = no" >> /etc/xinetd.d/tftp
echo "  # This is a workaround for Fedora, where TFTP will listen only on" >> /etc/xinetd.d/tftp
echo "  # IPv6 endpoint, if IPv4 flag is not used." >> /etc/xinetd.d/tftp
echo "  flags           = IPv4" >> /etc/xinetd.d/tftp
echo "}" >> /etc/xinetd.d/tftp
#
# #Restart the xinetd service
#
systemctl restart xinetd
#
# #Copy the PXE linux files to the tftpboot directory we created
#
cp /var/lib/tftpboot/pxelinux.0 /tftpboot
cp /var/lib/tftpboot/chain.c32 /tftpboot
#
# #Generate a map file for the PXE files
#
echo 're ^(/tftpboot/) /tftpboot/\2' > /tftpboot/map-file
echo 're ^/tftpboot/ /tftpboot/' >> /tftpboot/map-file
echo 're ^(^/) /tftpboot/\1' >> /tftpboot/map-file
echo 're ^([^/]) /tftpboot/\1' >> /tftpboot/map-file
#
# #Edit /etc/ironic/ironic.conf file with the <controller_ip> variable's value
#
sed --in-place "s|#tftp_server=\$my_ip|tftp_server=${controller_ip}|" /etc/ironic/ironic.conf
sed --in-place "s|#tftp_root=/tftpboot|tftp_root=/tftpboot|" /etc/ironic/ironic.conf
sed --in-place "s|#ip_version=4|ip_version=4|" /etc/ironic/ironic.conf
sed --in-place "s|#automated_clean=true|automated_clean=false|" /etc/ironic/ironic.conf
#
# #Edit /etc/nova/nova.conf file
#
sed --in-place "s|reserved_host_memory_mb=512|reserved_host_memory_mb=0|" 
sed --in-place "s|#scheduler_host_subset_size=1|scheduler_host_subset_size=9999999|" /etc/nova/nova.conf
sed --in-place "s|#scheduler_use_baremetal_filters=false|scheduler_use_baremetal_filters=true|" /etc/nova/nova.conf
#
# # Enable meta data
# # Edit /etc/neutron/dhcp_agent.ini
#
sed --in-place "s|enable_isolated_metadata\ =\ False|enable_isolated_metadata\ =\ True|" /etc/neutron/dhcp_agent.ini
sed --in-place "s|#force_metadata\ =\ false|force_metadata\ =\ True|" /etc/neutron/dhcp_agent.ini
#
#  
# #####
# # Enable internal dns for hostname resolution, if it already not set
# # manipulating configuration file via shell, alternate is to use openstack-config (TODO)
# ####
# # setup dns domain first
#
if grep -q "^dns_domain.*openstacklocal$" /etc/neutron/neutron.conf; then
   sed -in-place  "s|^dns_domain.*|dns_domain = oslocal|" /etc/neutron/neutron.conf
#
else
#
   if ! grep -q "^dns_domain" neutron.conf; then
       sed -in-place  "s|^#dns_domain = openstacklocal$|dns_domain = oslocal|" /etc/neutron/neutron.conf
#
   fi
fi
#
# # configure ml2 dns driver for neutron
#
ml2file=/etc/neutron/plugins/ml2/ml2_conf.ini
#
if ! grep -q "^extension_drivers" $ml2file; then
    # Assuming there is a place holder in comments, replace that string
    sed -in-place  "s|^#extension_drivers.*|extension_drivers = port_security,dns|" $ml2file
#
else
#
    # Entry is present, check if dns is already present, if not then enable
#
    if ! grep "^extension_drivers" $ml2file|grep -q dns; then
        current_dns=`grep "^extension_drivers" $ml2file`
        new_dns="$current_dns,dns"
        sed -in-place  "s|^extension_drivers.*|$new_dns|" $ml2file
#
    fi 
#
fi
#
# #------
# # Now restart the services
# #Restart ironic, nova, neutron, and ovs to load in the new configuration
#
systemctl restart neutron-dhcp-agent
systemctl restart neutron-openvswitch-agent
systemctl restart neutron-metadata-agent
systemctl restart neutron-server
systemctl restart openstack-nova-scheduler
systemctl restart openstack-nova-compute
systemctl restart openstack-ironic-conductor
#
#  PFILEP
## FILE: hera_preq/15-default.conf
#
# This is the main Apache server configuration file.  It contains the
# configuration directives that give the server its instructions.
# See <URL:http://httpd.apache.org/docs/2.2/> for detailed information.
# In particular, see
# <URL:http://httpd.apache.org/docs/2.2/mod/directives.html>
# for a discussion of each configuration directive.
#
#
# Do NOT simply read the instructions in here without understanding
# what they do.  They're here only as hints or reminders.  If you are unsure
# consult the online docs. You have been warned.  
#
# The configuration directives are grouped into three basic sections:
#  1. Directives that control the operation of the Apache server process as a
#     whole (the 'global environment').
#  2. Directives that define the parameters of the 'main' or 'default' server,
#     which responds to requests that aren't handled by a virtual host.
#     These directives also provide default values for the settings
#     of all virtual hosts.
#  3. Settings for virtual hosts, which allow Web requests to be sent to
#     different IP addresses or hostnames and have them handled by the
#     same Apache server process.
#
# Configuration and logfile names: If the filenames you specify for many
# of the server's control files begin with "/" (or "drive:/" for Win32), the
# server will use that explicit path.  If the filenames do *not* begin
# with "/", the value of ServerRoot is prepended -- so "logs/foo.log"
# with ServerRoot set to "/etc/httpd" will be interpreted by the
# server as "/etc/httpd/logs/foo.log".
#
### Section 1: Global Environment
#
# The directives in this section affect the overall operation of Apache,
# such as the number of concurrent requests it can handle or where it
# can find its configuration files.
#
#
# Don't give away too much information about all the subcomponents
# we are running.  Comment out this line if you don't mind remote sites
# finding out what major optional modules you are running
ServerTokens OS
#
# ServerRoot: The top of the directory tree under which the server's
# configuration, error, and log files are kept.
#
# NOTE!  If you intend to place this on an NFS (or otherwise network)
# mounted filesystem then please read the LockFile documentation
# (available at <URL:http://httpd.apache.org/docs/2.2/mod/mpm_common.html#lockfile>);
# you will save yourself a lot of trouble.
#
# Do NOT add a slash at the end of the directory path.
#
ServerRoot "/etc/httpd"
#
# PidFile: The file in which the server should record its process
# identification number when it starts.  Note the PIDFILE variable in
# /etc/sysconfig/httpd must be set appropriately if this location is
# changed.
#
PidFile run/httpd.pid
#
# Timeout: The number of seconds before receives and sends time out.
#
Timeout 60
#
# KeepAlive: Whether or not to allow persistent connections (more than
# one request per connection). Set to "Off" to deactivate.
#
KeepAlive Off
#
# MaxKeepAliveRequests: The maximum number of requests to allow
# during a persistent connection. Set to 0 to allow an unlimited amount.
# We recommend you leave this number high, for maximum performance.
#
MaxKeepAliveRequests 100
#
# KeepAliveTimeout: Number of seconds to wait for the next request from the
# same client on the same connection.
#
KeepAliveTimeout 15
##
## Server-Pool Size Regulation (MPM specific)
## 
# prefork MPM
# StartServers: number of server processes to start
# MinSpareServers: minimum number of server processes which are kept spare
# MaxSpareServers: maximum number of server processes which are kept spare
# ServerLimit: maximum value for MaxClients for the lifetime of the server
# MaxClients: maximum number of server processes allowed to start
# MaxRequestsPerChild: maximum number of requests a server process serves
<IfModule prefork.c>
StartServers       8
MinSpareServers    5
MaxSpareServers   20
ServerLimit      256
MaxClients       256
MaxRequestsPerChild  4000
</IfModule>
# worker MPM
# StartServers: initial number of server processes to start
# MaxClients: maximum number of simultaneous client connections
# MinSpareThreads: minimum number of worker threads which are kept spare
# MaxSpareThreads: maximum number of worker threads which are kept spare
# ThreadsPerChild: constant number of worker threads in each server process
# MaxRequestsPerChild: maximum number of requests a server process serves
<IfModule worker.c>
StartServers         4
MaxClients         300
MinSpareThreads     25
MaxSpareThreads     75 
ThreadsPerChild     25
MaxRequestsPerChild  0
</IfModule>
#
# Listen: Allows you to bind Apache to specific IP addresses and/or
# ports, in addition to the default. See also the <VirtualHost>
# directive.
#
# Change this to Listen on specific IP addresses as shown below to 
# prevent Apache from glomming onto all bound IP addresses (0.0.0.0)
#
#Listen 12.34.56.78:80
Listen 80
#
# Dynamic Shared Object (DSO) Support
#
# To be able to use the functionality of a module which was built as a DSO you
# have to place corresponding `LoadModule' lines at this location so the
# directives contained in it are actually available _before_ they are used.
# Statically compiled modules (those listed by `httpd -l') do not need
# to be loaded here.
#
# Example:
# LoadModule foo_module modules/mod_foo.so
#
LoadModule auth_basic_module modules/mod_auth_basic.so
LoadModule auth_digest_module modules/mod_auth_digest.so
LoadModule authn_file_module modules/mod_authn_file.so
LoadModule authn_alias_module modules/mod_authn_alias.so
LoadModule authn_anon_module modules/mod_authn_anon.so
LoadModule authn_dbm_module modules/mod_authn_dbm.so
LoadModule authn_default_module modules/mod_authn_default.so
LoadModule authz_host_module modules/mod_authz_host.so
LoadModule authz_user_module modules/mod_authz_user.so
LoadModule authz_owner_module modules/mod_authz_owner.so
LoadModule authz_groupfile_module modules/mod_authz_groupfile.so
LoadModule authz_dbm_module modules/mod_authz_dbm.so
LoadModule authz_default_module modules/mod_authz_default.so
LoadModule ldap_module modules/mod_ldap.so
LoadModule authnz_ldap_module modules/mod_authnz_ldap.so
LoadModule include_module modules/mod_include.so
LoadModule log_config_module modules/mod_log_config.so
LoadModule logio_module modules/mod_logio.so
LoadModule env_module modules/mod_env.so
LoadModule ext_filter_module modules/mod_ext_filter.so
LoadModule mime_magic_module modules/mod_mime_magic.so
LoadModule expires_module modules/mod_expires.so
LoadModule deflate_module modules/mod_deflate.so
LoadModule headers_module modules/mod_headers.so
LoadModule usertrack_module modules/mod_usertrack.so
LoadModule setenvif_module modules/mod_setenvif.so
LoadModule mime_module modules/mod_mime.so
LoadModule dav_module modules/mod_dav.so
LoadModule status_module modules/mod_status.so
LoadModule autoindex_module modules/mod_autoindex.so
LoadModule info_module modules/mod_info.so
LoadModule dav_fs_module modules/mod_dav_fs.so
LoadModule vhost_alias_module modules/mod_vhost_alias.so
LoadModule negotiation_module modules/mod_negotiation.so
LoadModule dir_module modules/mod_dir.so
LoadModule actions_module modules/mod_actions.so
LoadModule speling_module modules/mod_speling.so
LoadModule userdir_module modules/mod_userdir.so
LoadModule alias_module modules/mod_alias.so
LoadModule substitute_module modules/mod_substitute.so
LoadModule rewrite_module modules/mod_rewrite.so
LoadModule proxy_module modules/mod_proxy.so
LoadModule proxy_balancer_module modules/mod_proxy_balancer.so
LoadModule proxy_ftp_module modules/mod_proxy_ftp.so
LoadModule proxy_http_module modules/mod_proxy_http.so
LoadModule proxy_ajp_module modules/mod_proxy_ajp.so
LoadModule proxy_connect_module modules/mod_proxy_connect.so
LoadModule cache_module modules/mod_cache.so
LoadModule suexec_module modules/mod_suexec.so
LoadModule disk_cache_module modules/mod_disk_cache.so
LoadModule cgi_module modules/mod_cgi.so
LoadModule version_module modules/mod_version.so
#
# The following modules are not loaded by default:
#
#LoadModule asis_module modules/mod_asis.so
#LoadModule authn_dbd_module modules/mod_authn_dbd.so
#LoadModule cern_meta_module modules/mod_cern_meta.so
#LoadModule cgid_module modules/mod_cgid.so
#LoadModule dbd_module modules/mod_dbd.so
#LoadModule dumpio_module modules/mod_dumpio.so
#LoadModule filter_module modules/mod_filter.so
#LoadModule ident_module modules/mod_ident.so
#LoadModule log_forensic_module modules/mod_log_forensic.so
#LoadModule unique_id_module modules/mod_unique_id.so
#
#
# Load config files from the config directory "/etc/httpd/conf.d".
#
Include conf.d/*.conf
#
# ExtendedStatus controls whether Apache will generate "full" status
# information (ExtendedStatus On) or just basic information (ExtendedStatus
# Off) when the "server-status" handler is called. The default is Off.
#
#ExtendedStatus On
#
# If you wish httpd to run as a different user or group, you must run
# httpd as root initially and it will switch.  
#
# User/Group: The name (or #number) of the user/group to run httpd as.
#  . On SCO (ODT 3) use "User nouser" and "Group nogroup".
#  . On HPUX you may not be able to use shared memory as nobody, and the
#    suggested workaround is to create a user www and use that user.
#  NOTE that some kernels refuse to setgid(Group) or semctl(IPC_SET)
#  when the value of (unsigned)Group is above 60000; 
#  don't use Group #-1 on these systems!
#
User apache
Group apache
### Section 2: 'Main' server configuration
#
# The directives in this section set up the values used by the 'main'
# server, which responds to any requests that aren't handled by a
# <VirtualHost> definition.  These values also provide defaults for
# any <VirtualHost> containers you may define later in the file.
#
# All of these directives may appear inside <VirtualHost> containers,
# in which case these default settings will be overridden for the
# virtual host being defined.
#
#
# ServerAdmin: Your address, where problems with the server should be
# e-mailed.  This address appears on some server-generated pages, such
# as error documents.  e.g. admin@your-domain.com
#
ServerAdmin root@localhost
#
# ServerName gives the name and port that the server uses to identify itself.
# This can often be determined automatically, but we recommend you specify
# it explicitly to prevent problems during startup.
#
# If this is not set to valid DNS name for your host, server-generated
# redirections will not work.  See also the UseCanonicalName directive.
#
# If your host doesn't have a registered DNS name, enter its IP address here.
# You will have to access it by its address anyway, and this will make 
# redirections work in a sensible way.
#
#ServerName www.example.com:80
#
# UseCanonicalName: Determines how Apache constructs self-referencing 
# URLs and the SERVER_NAME and SERVER_PORT variables.
# When set "Off", Apache will use the Hostname and Port supplied
# by the client.  When set "On", Apache will use the value of the
# ServerName directive.
#
UseCanonicalName Off
#
# DocumentRoot: The directory out of which you will serve your
# documents. By default, all requests are taken from this directory, but
# symbolic links and aliases may be used to point to other locations.
#
DocumentRoot "/var/www/html"
#
# Each directory to which Apache has access can be configured with respect
# to which services and features are allowed and/or disabled in that
# directory (and its subdirectories). 
#
# First, we configure the "default" to be a very restrictive set of 
# features.  
#
<Directory />
    Options FollowSymLinks
    AllowOverride None
</Directory>
#
# Note that from this point forward you must specifically allow
# particular features to be enabled - so if something's not working as
# you might expect, make sure that you have specifically enabled it
# below.
#
#
# This should be changed to whatever you set DocumentRoot to.
#
<Directory "/var/www/html">
#
# Possible values for the Options directive are "None", "All",
# or any combination of:
#   Indexes Includes FollowSymLinks SymLinksifOwnerMatch ExecCGI MultiViews
#
# Note that "MultiViews" must be named *explicitly* --- "Options All"
# doesn't give it to you.
#
# The Options directive is both complicated and important.  Please see
# http://httpd.apache.org/docs/2.2/mod/core.html#options
# for more information.
#
    Options Indexes FollowSymLinks
#
# AllowOverride controls what directives may be placed in .htaccess files.
# It can be "All", "None", or any combination of the keywords:
#   Options FileInfo AuthConfig Limit
#
    AllowOverride None
#
# Controls who can get stuff from this server.
#
    Order allow,deny
    Allow from all
</Directory>
#
# UserDir: The name of the directory that is appended onto a user's home
# directory if a ~user request is received.
#
# The path to the end user account 'public_html' directory must be
# accessible to the webserver userid.  This usually means that ~userid
# must have permissions of 711, ~userid/public_html must have permissions
# of 755, and documents contained therein must be world-readable.
# Otherwise, the client will only receive a "403 Forbidden" message.
#
# See also: http://httpd.apache.org/docs/misc/FAQ.html#forbidden
#
<IfModule mod_userdir.c>
    #
    # UserDir is disabled by default since it can confirm the presence
    # of a username on the system (depending on home directory
    # permissions).
    #
    UserDir disabled
    #
    # To enable requests to /~user/ to serve the user's public_html
    # directory, remove the "UserDir disabled" line above, and uncomment
    # the following line instead:
    # 
    #UserDir public_html
</IfModule>
#
# Control access to UserDir directories.  The following is an example
# for a site where these directories are restricted to read-only.
#
#<Directory /home/*/public_html>
#    AllowOverride FileInfo AuthConfig Limit
#    Options MultiViews Indexes SymLinksIfOwnerMatch IncludesNoExec
#    <Limit GET POST OPTIONS>
#        Order allow,deny
#        Allow from all
#    </Limit>
#    <LimitExcept GET POST OPTIONS>
#        Order deny,allow
#        Deny from all
#    </LimitExcept>
#</Directory>
#
# DirectoryIndex: sets the file that Apache will serve if a directory
# is requested.
#
# The index.html.var file (a type-map) is used to deliver content-
# negotiated documents.  The MultiViews Option can be used for the 
# same purpose, but it is much slower.
#
DirectoryIndex index.html index.html.var
#
# AccessFileName: The name of the file to look for in each directory
# for additional configuration directives.  See also the AllowOverride
# directive.
#
AccessFileName .htaccess
#
# The following lines prevent .htaccess and .htpasswd files from being 
# viewed by Web clients. 
#
<Files ~ "^\.ht">
    Order allow,deny
    Deny from all
    Satisfy All
</Files>
#
# TypesConfig describes where the mime.types file (or equivalent) is
# to be found.
#
TypesConfig /etc/mime.types
#
# DefaultType is the default MIME type the server will use for a document
# if it cannot otherwise determine one, such as from filename extensions.
# If your server contains mostly text or HTML documents, "text/plain" is
# a good value.  If most of your content is binary, such as applications
# or images, you may want to use "application/octet-stream" instead to
# keep browsers from trying to display binary files as though they are
# text.
#
DefaultType text/plain
#
# The mod_mime_magic module allows the server to use various hints from the
# contents of the file itself to determine its type.  The MIMEMagicFile
# directive tells the module where the hint definitions are located.
#
<IfModule mod_mime_magic.c>
#   MIMEMagicFile /usr/share/magic.mime
    MIMEMagicFile conf/magic
</IfModule>
#
# HostnameLookups: Log the names of clients or just their IP addresses
# e.g., www.apache.org (on) or 204.62.129.132 (off).
# The default is off because it'd be overall better for the net if people
# had to knowingly turn this feature on, since enabling it means that
# each client request will result in AT LEAST one lookup request to the
# nameserver.
#
HostnameLookups Off
#
# EnableMMAP: Control whether memory-mapping is used to deliver
# files (assuming that the underlying OS supports it).
# The default is on; turn this off if you serve from NFS-mounted 
# filesystems.  On some systems, turning it off (regardless of
# filesystem) can improve performance; for details, please see
# http://httpd.apache.org/docs/2.2/mod/core.html#enablemmap
#
#EnableMMAP off
#
# EnableSendfile: Control whether the sendfile kernel support is 
# used to deliver files (assuming that the OS supports it). 
# The default is on; turn this off if you serve from NFS-mounted 
# filesystems.  Please see
# http://httpd.apache.org/docs/2.2/mod/core.html#enablesendfile
#
#EnableSendfile off
#
# ErrorLog: The location of the error log file.
# If you do not specify an ErrorLog directive within a <VirtualHost>
# container, error messages relating to that virtual host will be
# logged here.  If you *do* define an error logfile for a <VirtualHost>
# container, that host's errors will be logged there and not here.
#
ErrorLog logs/error_log
#
# LogLevel: Control the number of messages logged to the error_log.
# Possible values include: debug, info, notice, warn, error, crit,
# alert, emerg.
#
LogLevel warn
#
# The following directives define some format nicknames for use with
# a CustomLog directive (see below).
#
LogFormat "%h %l %u %t \"%r\" %>s %b \"%{Referer}i\" \"%{User-Agent}i\"" combined
LogFormat "%h %l %u %t \"%r\" %>s %b" common
LogFormat "%{Referer}i -> %U" referer
LogFormat "%{User-agent}i" agent
# "combinedio" includes actual counts of actual bytes received (%I) and sent (%O); this
# requires the mod_logio module to be loaded.
#LogFormat "%h %l %u %t \"%r\" %>s %b \"%{Referer}i\" \"%{User-Agent}i\" %I %O" combinedio
#
# The location and format of the access logfile (Common Logfile Format).
# If you do not define any access logfiles within a <VirtualHost>
# container, they will be logged here.  Contrariwise, if you *do*
# define per-<VirtualHost> access logfiles, transactions will be
# logged therein and *not* in this file.
#
#CustomLog logs/access_log common
#
# If you would like to have separate agent and referer logfiles, uncomment
# the following directives.
#
#CustomLog logs/referer_log referer
#CustomLog logs/agent_log agent
#
# For a single logfile with access, agent, and referer information
# (Combined Logfile Format), use the following directive:
#
CustomLog logs/access_log combined
#
# Optionally add a line containing the server version and virtual host
# name to server-generated pages (internal error documents, FTP directory
# listings, mod_status and mod_info output etc., but not CGI generated
# documents or custom error documents).
# Set to "EMail" to also include a mailto: link to the ServerAdmin.
# Set to one of:  On | Off | EMail
#
ServerSignature On
#
# Aliases: Add here as many aliases as you need (with no limit). The format is 
# Alias fakename realname
#
# Note that if you include a trailing / on fakename then the server will
# require it to be present in the URL.  So "/icons" isn't aliased in this
# example, only "/icons/".  If the fakename is slash-terminated, then the 
# realname must also be slash terminated, and if the fakename omits the 
# trailing slash, the realname must also omit it.
#
# We include the /icons/ alias for FancyIndexed directory listings.  If you
# do not use FancyIndexing, you may comment this out.
#
Alias /icons/ "/var/www/icons/"
<Directory "/var/www/icons">
    Options Indexes MultiViews FollowSymLinks
    AllowOverride None
    Order allow,deny
    Allow from all
</Directory>
#
# WebDAV module configuration section.
# 
<IfModule mod_dav_fs.c>
    # Location of the WebDAV lock database.
    DAVLockDB /var/lib/dav/lockdb
</IfModule>
#
# ScriptAlias: This controls which directories contain server scripts.
# ScriptAliases are essentially the same as Aliases, except that
# documents in the realname directory are treated as applications and
# run by the server when requested rather than as documents sent to the client.
# The same rules about trailing "/" apply to ScriptAlias directives as to
# Alias.
#
ScriptAlias /cgi-bin/ "/var/www/cgi-bin/"
#
# "/var/www/cgi-bin" should be changed to whatever your ScriptAliased
# CGI directory exists, if you have that configured.
#
<Directory "/var/www/cgi-bin">
    AllowOverride None
    Options None
    Order allow,deny
    Allow from all
</Directory>
#
# Redirect allows you to tell clients about documents which used to exist in
# your server's namespace, but do not anymore. This allows you to tell the
# clients where to look for the relocated document.
# Example:
# Redirect permanent /foo http://www.example.com/bar
#
# Directives controlling the display of server-generated directory listings.
#
#
# IndexOptions: Controls the appearance of server-generated directory
# listings.
#
IndexOptions FancyIndexing VersionSort NameWidth=* HTMLTable Charset=UTF-8
#
# AddIcon* directives tell the server which icon to show for different
# files or filename extensions.  These are only displayed for
# FancyIndexed directories.
#
AddIconByEncoding (CMP,/icons/compressed.gif) x-compress x-gzip
AddIconByType (TXT,/icons/text.gif) text/*
AddIconByType (IMG,/icons/image2.gif) image/*
AddIconByType (SND,/icons/sound2.gif) audio/*
AddIconByType (VID,/icons/movie.gif) video/*
AddIcon /icons/binary.gif .bin .exe
AddIcon /icons/binhex.gif .hqx
AddIcon /icons/tar.gif .tar
AddIcon /icons/world2.gif .wrl .wrl.gz .vrml .vrm .iv
AddIcon /icons/compressed.gif .Z .z .tgz .gz .zip
AddIcon /icons/a.gif .ps .ai .eps
AddIcon /icons/layout.gif .html .shtml .htm .pdf
AddIcon /icons/text.gif .txt
AddIcon /icons/c.gif .c
AddIcon /icons/p.gif .pl .py
AddIcon /icons/f.gif .for
AddIcon /icons/dvi.gif .dvi
AddIcon /icons/uuencoded.gif .uu
AddIcon /icons/script.gif .conf .sh .shar .csh .ksh .tcl
AddIcon /icons/tex.gif .tex
AddIcon /icons/bomb.gif core
AddIcon /icons/back.gif ..
AddIcon /icons/hand.right.gif README
AddIcon /icons/folder.gif ^^DIRECTORY^^
AddIcon /icons/blank.gif ^^BLANKICON^^
#
# DefaultIcon is which icon to show for files which do not have an icon
# explicitly set.
#
DefaultIcon /icons/unknown.gif
#
# AddDescription allows you to place a short description after a file in
# server-generated indexes.  These are only displayed for FancyIndexed
# directories.
# Format: AddDescription "description" filename
#
#AddDescription "GZIP compressed document" .gz
#AddDescription "tar archive" .tar
#AddDescription "GZIP compressed tar archive" .tgz
#
# ReadmeName is the name of the README file the server will look for by
# default, and append to directory listings.
#
# HeaderName is the name of a file which should be prepended to
# directory indexes. 
ReadmeName README.html
HeaderName HEADER.html
#
# IndexIgnore is a set of filenames which directory indexing should ignore
# and not include in the listing.  Shell-style wildcarding is permitted.
#
IndexIgnore .??* *~ *# HEADER* README* RCS CVS *,v *,t
#
# DefaultLanguage and AddLanguage allows you to specify the language of 
# a document. You can then use content negotiation to give a browser a 
# file in a language the user can understand.
#
# Specify a default language. This means that all data
# going out without a specific language tag (see below) will 
# be marked with this one. You probably do NOT want to set
# this unless you are sure it is correct for all cases.
#
# * It is generally better to not mark a page as 
# * being a certain language than marking it with the wrong
# * language!
#
# DefaultLanguage nl
#
# Note 1: The suffix does not have to be the same as the language
# keyword --- those with documents in Polish (whose net-standard
# language code is pl) may wish to use "AddLanguage pl .po" to
# avoid the ambiguity with the common suffix for perl scripts.
#
# Note 2: The example entries below illustrate that in some cases 
# the two character 'Language' abbreviation is not identical to 
# the two character 'Country' code for its country,
# E.g. 'Danmark/dk' versus 'Danish/da'.
#
# Note 3: In the case of 'ltz' we violate the RFC by using a three char
# specifier. There is 'work in progress' to fix this and get
# the reference data for rfc1766 cleaned up.
#
# Catalan (ca) - Croatian (hr) - Czech (cs) - Danish (da) - Dutch (nl)
# English (en) - Esperanto (eo) - Estonian (et) - French (fr) - German (de)
# Greek-Modern (el) - Hebrew (he) - Italian (it) - Japanese (ja)
# Korean (ko) - Luxembourgeois* (ltz) - Norwegian Nynorsk (nn)
# Norwegian (no) - Polish (pl) - Portugese (pt)
# Brazilian Portuguese (pt-BR) - Russian (ru) - Swedish (sv)
# Simplified Chinese (zh-CN) - Spanish (es) - Traditional Chinese (zh-TW)
#
AddLanguage ca .ca
AddLanguage cs .cz .cs
AddLanguage da .dk
AddLanguage de .de
AddLanguage el .el
AddLanguage en .en
AddLanguage eo .eo
AddLanguage es .es
AddLanguage et .et
AddLanguage fr .fr
AddLanguage he .he
AddLanguage hr .hr
AddLanguage it .it
AddLanguage ja .ja
AddLanguage ko .ko
AddLanguage ltz .ltz
AddLanguage nl .nl
AddLanguage nn .nn
AddLanguage no .no
AddLanguage pl .po
AddLanguage pt .pt
AddLanguage pt-BR .pt-br
AddLanguage ru .ru
AddLanguage sv .sv
AddLanguage zh-CN .zh-cn
AddLanguage zh-TW .zh-tw
#
# LanguagePriority allows you to give precedence to some languages
# in case of a tie during content negotiation.
#
# Just list the languages in decreasing order of preference. We have
# more or less alphabetized them here. You probably want to change this.
#
LanguagePriority en ca cs da de el eo es et fr he hr it ja ko ltz nl nn no pl pt pt-BR ru sv zh-CN zh-TW
#
# ForceLanguagePriority allows you to serve a result page rather than
# MULTIPLE CHOICES (Prefer) [in case of a tie] or NOT ACCEPTABLE (Fallback)
# [in case no accepted languages matched the available variants]
#
ForceLanguagePriority Prefer Fallback
#
# Specify a default charset for all content served; this enables
# interpretation of all content as UTF-8 by default.  To use the 
# default browser choice (ISO-8859-1), or to allow the META tags
# in HTML content to override this choice, comment out this
# directive:
#
AddDefaultCharset UTF-8
#
# AddType allows you to add to or override the MIME configuration
# file mime.types for specific file types.
#
#AddType application/x-tar .tgz
#
# AddEncoding allows you to have certain browsers uncompress
# information on the fly. Note: Not all browsers support this.
# Despite the name similarity, the following Add* directives have nothing
# to do with the FancyIndexing customization directives above.
#
#AddEncoding x-compress .Z
#AddEncoding x-gzip .gz .tgz
# If the AddEncoding directives above are commented-out, then you
# probably should define those extensions to indicate media types:
#
AddType application/x-compress .Z
AddType application/x-gzip .gz .tgz
#
#   MIME-types for downloading Certificates and CRLs
#
AddType application/x-x509-ca-cert .crt
AddType application/x-pkcs7-crl    .crl
#
# AddHandler allows you to map certain file extensions to "handlers":
# actions unrelated to filetype. These can be either built into the server
# or added with the Action directive (see below)
#
# To use CGI scripts outside of ScriptAliased directories:
# (You will also need to add "ExecCGI" to the "Options" directive.)
#
#AddHandler cgi-script .cgi
#
# For files that include their own HTTP headers:
#
#AddHandler send-as-is asis
#
# For type maps (negotiated resources):
# (This is enabled by default to allow the Apache "It Worked" page
#  to be distributed in multiple languages.)
#
AddHandler type-map var
#
# Filters allow you to process content before it is sent to the client.
#
# To parse .shtml files for server-side includes (SSI):
# (You will also need to add "Includes" to the "Options" directive.)
#
AddType text/html .shtml
AddOutputFilter INCLUDES .shtml
#
# Action lets you define media types that will execute a script whenever
# a matching file is called. This eliminates the need for repeated URL
# pathnames for oft-used CGI file processors.
# Format: Action media/type /cgi-script/location
# Format: Action handler-name /cgi-script/location
#
#
# Customizable error responses come in three flavors:
# 1) plain text 2) local redirects 3) external redirects
#
# Some examples:
#ErrorDocument 500 "The server made a boo boo."
#ErrorDocument 404 /missing.html
#ErrorDocument 404 "/cgi-bin/missing_handler.pl"
#ErrorDocument 402 http://www.example.com/subscription_info.html
#
#
# Putting this all together, we can internationalize error responses.
#
# We use Alias to redirect any /error/HTTP_<error>.html.var response to
# our collection of by-error message multi-language collections.  We use 
# includes to substitute the appropriate text.
#
# You can modify the messages' appearance without changing any of the
# default HTTP_<error>.html.var files by adding the line:
#
#   Alias /error/include/ "/your/include/path/"
#
# which allows you to create your own set of files by starting with the
# /var/www/error/include/ files and
# copying them to /your/include/path/, even on a per-VirtualHost basis.
#
Alias /error/ "/var/www/error/"
<IfModule mod_negotiation.c>
<IfModule mod_include.c>
    <Directory "/var/www/error">
        AllowOverride None
        Options IncludesNoExec
        AddOutputFilter Includes html
        AddHandler type-map var
        Order allow,deny
        Allow from all
        LanguagePriority en es de fr
        ForceLanguagePriority Prefer Fallback
    </Directory>
#    ErrorDocument 400 /error/HTTP_BAD_REQUEST.html.var
#    ErrorDocument 401 /error/HTTP_UNAUTHORIZED.html.var
#    ErrorDocument 403 /error/HTTP_FORBIDDEN.html.var
#    ErrorDocument 404 /error/HTTP_NOT_FOUND.html.var
#    ErrorDocument 405 /error/HTTP_METHOD_NOT_ALLOWED.html.var
#    ErrorDocument 408 /error/HTTP_REQUEST_TIME_OUT.html.var
#    ErrorDocument 410 /error/HTTP_GONE.html.var
#    ErrorDocument 411 /error/HTTP_LENGTH_REQUIRED.html.var
#    ErrorDocument 412 /error/HTTP_PRECONDITION_FAILED.html.var
#    ErrorDocument 413 /error/HTTP_REQUEST_ENTITY_TOO_LARGE.html.var
#    ErrorDocument 414 /error/HTTP_REQUEST_URI_TOO_LARGE.html.var
#    ErrorDocument 415 /error/HTTP_UNSUPPORTED_MEDIA_TYPE.html.var
#    ErrorDocument 500 /error/HTTP_INTERNAL_SERVER_ERROR.html.var
#    ErrorDocument 501 /error/HTTP_NOT_IMPLEMENTED.html.var
#    ErrorDocument 502 /error/HTTP_BAD_GATEWAY.html.var
#    ErrorDocument 503 /error/HTTP_SERVICE_UNAVAILABLE.html.var
#    ErrorDocument 506 /error/HTTP_VARIANT_ALSO_VARIES.html.var
</IfModule>
</IfModule>
#
# The following directives modify normal HTTP response behavior to
# handle known problems with browser implementations.
#
BrowserMatch "Mozilla/2" nokeepalive
BrowserMatch "MSIE 4\.0b2;" nokeepalive downgrade-1.0 force-response-1.0
BrowserMatch "RealPlayer 4\.0" force-response-1.0
BrowserMatch "Java/1\.0" force-response-1.0
BrowserMatch "JDK/1\.0" force-response-1.0
#
# The following directive disables redirects on non-GET requests for
# a directory that does not include the trailing slash.  This fixes a 
# problem with Microsoft WebFolders which does not appropriately handle 
# redirects for folders with DAV methods.
# Same deal with Apple's DAV filesystem and Gnome VFS support for DAV.
#
BrowserMatch "Microsoft Data Access Internet Publishing Provider" redirect-carefully
BrowserMatch "MS FrontPage" redirect-carefully
BrowserMatch "^WebDrive" redirect-carefully
BrowserMatch "^WebDAVFS/1.[0123]" redirect-carefully
BrowserMatch "^gnome-vfs/1.0" redirect-carefully
BrowserMatch "^XML Spy" redirect-carefully
BrowserMatch "^Dreamweaver-WebDAV-SCM1" redirect-carefully
#
# Allow server status reports generated by mod_status,
# with the URL of http://servername/server-status
# Change the ".example.com" to match your domain to enable.
#
#<Location /server-status>
#    SetHandler server-status
#    Order deny,allow
#    Deny from all
#    Allow from .example.com
#</Location>
#
# Allow remote server configuration reports, with the URL of
#  http://servername/server-info (requires that mod_info.c be loaded).
# Change the ".example.com" to match your domain to enable.
#
#<Location /server-info>
#    SetHandler server-info
#    Order deny,allow
#    Deny from all
#    Allow from .example.com
#</Location>
#
# Proxy Server directives. Uncomment the following lines to
# enable the proxy server:
#
#<IfModule mod_proxy.c>
#ProxyRequests On
#
#<Proxy *>
#    Order deny,allow
#    Deny from all
#    Allow from .example.com
#</Proxy>
#
# Enable/disable the handling of HTTP/1.1 "Via:" headers.
# ("Full" adds the server version; "Block" removes all outgoing Via: headers)
# Set to one of: Off | On | Full | Block
#
#ProxyVia On
#
# To enable a cache of proxied content, uncomment the following lines.
# See http://httpd.apache.org/docs/2.2/mod/mod_cache.html for more details.
#
#<IfModule mod_disk_cache.c>
#   CacheEnable disk /
#   CacheRoot "/var/cache/mod_proxy"
#</IfModule>
#
#</IfModule>
# End of proxy directives.
### Section 3: Virtual Hosts
#
# VirtualHost: If you want to maintain multiple domains/hostnames on your
# machine you can setup VirtualHost containers for them. Most configurations
# use only name-based virtual hosts so the server doesn't need to worry about
# IP addresses. This is indicated by the asterisks in the directives below.
#
# Please see the documentation at 
# <URL:http://httpd.apache.org/docs/2.2/vhosts/>
# for further details before you try to setup virtual hosts.
#
# You may use the command line option '-S' to verify your virtual host
# configuration.
#
# Use name-based virtual hosting.
#
#NameVirtualHost *:80
#
# NOTE: NameVirtualHost cannot be used without a port specifier 
# (e.g. :80) if mod_ssl is being used, due to the nature of the
# SSL protocol.
#
#
# VirtualHost example:
# Almost any Apache directive may go into a VirtualHost container.
# The first VirtualHost section is used for requests without a known
# server name.
#
#<VirtualHost *:80>
#    ServerAdmin webmaster@dummy-host.example.com
#    DocumentRoot /www/docs/dummy-host.example.com
#    ServerName dummy-host.example.com
#    ErrorLog logs/dummy-host.example.com-error_log
#    CustomLog logs/dummy-host.example.com-access_log common
#</VirtualHost>
#  PFILEP
## FILE: hera_preq/cmt_ohpc_inputs_dummy
# -*-sh-*-
# ------------------------------------------------------------------------------------------------
# ------------------------------------------------------------------------------------------------
# Template input file to define local variable settings for use with
# an installation recipe.
# ------------------------------------------------------------------------------------------------
# ---------------------------
# cluster fabric technology
# ---------------------------
# set to 1 for OPA, otherwise 0 for IB
opa_fabric="${opa_fabric:-0}"
# ---------------------------
# SMS (master) node settings
# ---------------------------
# Flag to recreate ssh keys on new install
recreate_keys="${recreate_keys:-0}"
# Hostname for master server (SMS)
sms_name="${sms_name:-sms030}"
                              
# Local (internal) IP address on SMS
sms_ip="${sms_ip:-192.168.1.30}"
# Internal ethernet interface on SMS
sms_eth_internal="${sms_eth_internal:-eth2}"
# Subnet netmask for internal cluster network
internal_netmask="${internal_netmask:-255.255.0.0}"
# Provisioning interface used by compute hosts
eth_provision="${eth_provision:-eth2}"
# Local ntp server for time synchronization
ntp_server="${ntp_server:-192.168.0.1}"
# BMC user credentials for use by IPMI
ipmi_username="${ipmi_username:-cod}"
IPMI_PASSWORD="${IPMI_PASSWORD:-unknown}"
# Additional time to wait for compute nodes to provision (seconds)
provision_wait="${provision_wait:-300}"
# Optional Stateful install device
stateful_dev="${stateful_dev:-}"
# Flags for optional installation/configuration
enable_clustershell="${enable_clustershell:-0}"
enable_ipmisol="${enable_ipmisol:-0}"
enable_ipoib="${enable_ipoib:-0}"
enable_ganglia="${enable_ganglia:-0}"
enable_kargs="${enable_kargs:-0}"
enable_lustre_client="${enable_lustre_client:-0}"
enable_mrsh="${enable_mrsh:-0}"
enable_nagios="${enable_nagios:-0}"
enable_powerman="${enable_powerman:-0}"
enable_stateful="${enable_stateful:-0}"
enable_ssf="${enable_ssf:-0}"
# -------------------------
# compute node settings
# -------------------------
# Set location of local BOS mirror for CN
BOS_MIRROR="${BOS_MIRROR:-http://linux-ftp.jf.intel.com/pub/mirrors/centos/7.3.1611/os/x86_64/}"
# Prefix for compute node hostnames
nodename_prefix="${nodename_prefix:-c}"
# compute node IP addresses
#c_ip[0]=172.16.1.13
#c_ip[1]=172.16.1.14
#c_ip[2]=172.16.1.15
#c_ip[3]=172.16.1.17
# compute node MAC addresses for provisioning interface
#c_mac[0]=00:1e:67:cb:e7:d4
#c_mac[1]=00:1e:67:cb:e8:56
#c_mac[2]=00:1e:67:cb:ee:5f
#c_mac[3]=00:1e:67:cb:f2:1a
# compute node BMC addresses
#c_bmc[0]=192.168.2.13
#c_bmc[1]=192.168.2.14
#c_bmc[2]=192.168.2.15
#c_bmc[3]=192.168.2.17
#-------------------
# Optional settings
#-------------------
# additional arguments to enable optional arguments for bootstrap kernel
kargs="${kargs:-acpi_pad.disable=1}"
# Lustre MGS mount name
mgs_fs_name="${mgs_fs_name:-192.168.1.90@tcp0:/hera1}"
# Subnet netmask for IPoIB network
ipoib_netmask="${ipoib_netmask:-255.255.0.0}"
# IPoIB address for SMS server
sms_ipoib="${sms_ipoib:-172.16.3.12}"
# IPoIB addresses for computes
#c_ipoib[0]=172.16.3.13
#c_ipoib[1]=172.16.3.14
#c_ipoib[2]=172.16.3.15
#c_ipoib[3]=172.16.3.17
#  PFILEP
## FILE: hera_preq/CentOS-Base.repo
# CentOS-Base.repo
#
# The mirror system uses the connecting IP address of the client and the
# update status of each mirror to pick mirrors that are updated to and
# geographically close to the client.  You should use this for CentOS updates
# unless you are manually picking other mirrors.
#
# If the mirrorlist= does not work for you, as a fall back you can try the 
# remarked out baseurl= line instead.
#
#
[base]
name=CentOS-7 - Base
#mirrorlist=http://mirrorlist.centos.org/?release=7&arch=$basearch&repo=os&infra=$infra
#baseurl=http://linux-ftp.jf.intel.com/pub/mirrors/centos/7.1.1503/os/x86_64/
#baseurl=http://mirror.centos.org/centos/7/os/$basearch/
baseurl=http://linux-ftp.jf.intel.com/pub/mirrors/centos/7.3.1611/os/x86_64/
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7
#released updates 
[updates]
name=CentOS-7 - Updates
#mirrorlist=http://mirrorlist.centos.org/?release=7&arch=$basearch&repo=updates&infra=$infra
#baseurl=http://mirror.centos.org/centos/7/updates/$basearch/
baseurl=http://mirror.centos.org/centos/7/updates/x86_64/
gpgcheck=1
enabled=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7
#additional packages that may be useful
[extras]
name=CentOS-7 - Extras
mirrorlist=http://mirrorlist.centos.org/?release=7&arch=$basearch&repo=extras&infra=$infra
#baseurl=http://mirror.centos.org/centos/7/extras/$basearch/
gpgcheck=1
enabled=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7
#additional packages that extend functionality of existing packages
[centosplus]
name=CentOS-7 - Plus
mirrorlist=http://mirrorlist.centos.org/?release=7&arch=$basearch&repo=centosplus&infra=$infra
#baseurl=http://mirror.centos.org/centos/7/centosplus/$basearch/
gpgcheck=1
enabled=0
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7
#  PFILEP
## FILE: hera_preq/setup.sh
#!/bin/bash
#Get the current allotted IP matching 192 series...
currip=$(ifconfig | grep inet | awk '{print $2}' | grep 192*)
currhost=$(hostname)
#Add proxies to /etc/environment
echo "export http_proxy=proxy.ra.intel.com:911" >> /etc/environment
echo "export https_proxy=proxy.ra.intel.com:911" >> /etc/environment
# lOCAL no proxies for Headnode, intel.com
echo "export no_proxy=.intel.com,$currip" >> /etc/environment
source /etc/environment
#update hosts file right ip addresses
perl -pi -e "s/^(.*)$currhost/$currip $currhost/g" /etc/hosts
# add root in /etc/security/access.conf
echo "+:root : $currip " >> /etc/security/access.conf
# setup ssh keys do that root can ssh locally on that system
ssh-keygen -f /root/.ssh/cluster -N ""
cat /root/.ssh/cluster.pub >> /root/.ssh/authorized_keys 
scp /home_nfs/guptarav/hpc_preq/CentOS* /etc/yum.repos.d/
yum clean all
yum repolist
yum update -y
#update ohpc-docs info here
rpm -ivh https://github.com/openhpc/ohpc/releases/download/v1.3.GA/ohpc-release-1.3-1.el7.x86_64.rpm
 
#update input.local
# update cloud inventory file
# update answers.txt
perl -pie "s/^CONFIG_CONTROLLER_HOST=(.+)/CONFIG_CONTROLLER_HOST=$currip/" /home_nfs/guptarav/hpc/packstack/recipe/answer.txt
perl -pie "s/^CONFIG_COMPUTE_HOSTS=(.+)/CONFIG_COMPUTE_HOSTS=$currip/" /home_nfs/guptarav/hpc/packstack/recipe/answer.txt
perl -pie "s/^CONFIG_NETWORK_HOSTS=(.+)/CONFIG_NETWORK_HOSTS=$currip/" /home_nfs/guptarav/hpc/packstack/recipe/answer.txt
perl -pie "s/^CONFIG_STORAGE_HOST=(.+)/CONFIG_STORAGE_HOST=$currip/" /home_nfs/guptarav/hpc/packstack/recipe/answer.txt
perl -pie "s/^CONFIG_SAHARA_HOST=(.+)/CONFIG_SAHARA_HOST=$currip/" /home_nfs/guptarav/hpc/packstack/recipe/answer.txt
perl -pie "s/^CONFIG_AMQP_HOST=(.+)/CONFIG_AMQP_HOST=$currip/" /home_nfs/guptarav/hpc/packstack/recipe/answer.txt
perl -pie "s/^CONFIG_MARIADB_HOST=(.+)/CONFIG_MARIADB_HOST=$currip/" /home_nfs/guptarav/hpc/packstack/recipe/answer.txt
perl -pie "s/^CONFIG_MONGODB_HOST=(.+)/CONFIG_MONGODB_HOST=$currip/" /home_nfs/guptarav/hpc/packstack/recipe/answer.txt
perl -pie "s/^CONFIG_REDIS_MASTER_HOST=(.+)/CONFIG_REDIS_MASTER_HOST=$currip/" /home_nfs/guptarav/hpc/packstack/recipe/answer.txt
perl -pie "s/^CONFIG_KEYSTONE_LDAP_URL=ldap\:\/\/(.+)/CONFIG_KEYSTONE_LDAP_URL=ldap\:\/\/$currip/" /home_nfs/guptarav/hpc/packstack/recipe/answer.txt
#update answer.txt with correct ips
cat /root/cmt_ohpc_inputs | grep "c_ip\[3\]" | awk '{split($0,numbers,"="); print numbers[2]}'
#  PFILEP
## FILE: hera_preq/environment
export http_proxy=proxy.ra.intel.com:911
export https_proxy=proxy.ra.intel.com:911
export no_proxy=.intel.com,172.16.1.12,127.0.0.1,172.16.0.0/32
#  PFILEP
## FILE: common_functions
#!/bin/bash
#validateIpInput () {
#}
function validateInputFile () {
    if [[ -z "${inputFile}" || ! -e "${inputFile}" ]];then
      echo "Error: Unable to access local input file -> \"${inputFile}\""
      exit 1
    else
      . ${inputFile} || { echo "Error sourcing ${inputFile}"; exit 1; }
    fi
    _BADCOUNT=0
    
    for((i=0; i<${#c_ip[@]}; i++)) ; do
      if ! [[ ${c_ip[i]} =~ ^(([0-9]|[1-9][0-9]|1[0-9][0-9]|2[0-4][0-9]|25[0-5])\.){3}([\
                                0-9]|[1-9][0-9]|1[0-9][0-9]|2[0-4][0-9]|25[0-5])$ ]]; then
        echo "ERROR: Invalid IP address #$i: ${c_ip[i]}"
        _BADCOUNT=$((_BADCOUNT+1))
      fi
      if ! [[ ${c_bmc[i]} =~ ^(([0-9]|[1-9][0-9]|1[0-9][0-9]|2[0-4][0-9]|25[0-5])\.){3}([\
                                 0-9]|[1-9][0-9]|1[0-9][0-9]|2[0-4][0-9]|25[0-5])$ ]]; then
        echo "ERROR: Invalid BMC IP address #$i: ${c_bmc[i]}"
        _BADCOUNT=$((_BADCOUNT+1))
      fi
      if ! [[ `echo ${c_mac[i]^^} | egrep "^([0-9A-F]{2}:){5}[0-9A-F]{2}$"` ]]; then
        echo "ERROR: Invalid MAC address #$i: ${c_mac[i]}"
        _BADCOUNT=$((_BADCOUNT+1))
      fi
    done
    
    [[ $_BADCOUNT -eq 0 ]] || exit 3
    #validateIpInput $nodename_prefix
}
function validateHpcInventory() {
    if [[ -z "${cloudHpcInventory}" || ! -e "${cloudHpcInventory}" ]];then
      echo "Error: Unable to access Cloud hpc inventory file -> \"${cloudHpcInventory}\""
      exit 1
    else
      . ${cloudHpcInventory} || { echo "Error sourcing ${cloudHpcInventory}"; exit 1; }
    fi
    #Verify Cloud IP. Move this to common function validateIpInput
    _BADCOUNT=0
    
    for((i=0; i<${#cc_ip[@]}; i++)) ; do
      if ! [[ ${cc_ip[i]} =~ ^(([0-9]|[1-9][0-9]|1[0-9][0-9]|2[0-4][0-9]|25[0-5])\.){3}([\
                                0-9]|[1-9][0-9]|1[0-9][0-9]|2[0-4][0-9]|25[0-5])$ ]]; then
        echo "ERROR: Invalid IP address #$i: ${cc_ip[i]}"
        _BADCOUNT=$((_BADCOUNT+1))
      fi
      if ! [[ ${cc_bmc[i]} =~ ^(([0-9]|[1-9][0-9]|1[0-9][0-9]|2[0-4][0-9]|25[0-5])\.){3}([\
                                 0-9]|[1-9][0-9]|1[0-9][0-9]|2[0-4][0-9]|25[0-5])$ ]]; then
        echo "ERROR: Invalid BMC IP address #$i: ${cc_bmc[i]}"
        _BADCOUNT=$((_BADCOUNT+1))
      fi
      if ! [[ `echo ${cc_mac[i]^^} | egrep "^([0-9A-F]{2}:){5}[0-9A-F]{2}$"` ]]; then
        echo "ERROR: Invalid MAC address #$i: ${cc_mac[i]}"
        _BADCOUNT=$((_BADCOUNT+1))
      fi
    done
    #validateIpInput $cnodename_prefix
}
function setup_hosts () {
    # first search if nodes are already configured. if yes then do not configure again, so that it can be re-run
    hpc_tag="#### Cloud-HPC nodes entry, entered by HPC Orchestrator ####"
    #if ! grep -Pq "^$hpc_tag" /etc/hosts; then
    if ! grep -Fq "$hpc_tag" /etc/hosts; then
        echo $hpc_tag >> /etc/hosts
    fi
    #check if sms_name is already configured, if not then add sms entry
    if ! grep -Fq ${sms_name} /etc/hosts; then
        echo -e "${sms_ip}\t${sms_name}" >> /etc/hosts
    fi
    for ((i=0; i<$num_ccomputes; i++)) ; do
       if ! grep -Fq "${cc_name[$i]}" /etc/hosts; then
           echo -e "${cc_ip[$i]}\t${cc_name[$i]}"
       fi
    done >> /etc/hosts
}
function setup_cname () {
    # Determine number of computes and their hostnames
    export num_computes=${num_computes:-${#c_ip[@]}}
    for((i=0; i<${num_computes}; i++)) ; do
       c_name[$i]=${nodename_prefix}$((i+1))
    done
    export c_name
}
function setup_ccname() {
    export num_ccomputes=${num_ccomputes:-${#cc_ip[@]}}
    for((i=0; i<${num_ccomputes}; i++)) ; do
       cc_name[$i]=${cnodename_prefix}$((i+1))
    done
    export cc_name
}
function setup_computename() {
    setup_cname
    setup_ccname
}
function cidr_to_netmask() {
    cidr=$1
    value=$(( 0xffffffff ^ ((1 << (32 - $cidr)) - 1) ))
    netmask="$(( (value >> 24) & 0xff )).$(( (value >> 16) & 0xff )).$(( (value >> 8) & 0xff )).$(( value & 0xff ))"
    echo $netmask
}
function netmask_to_cidr() {
    nmask=$1
    # To calculate cidr, we just need to calculate number bits in each octets and add them.
    cidr_bits=0
    # iterate through each octets
    # use dot as saperator
    IFS=.
    for octs in $nmask ; do
       # we can only have 8 combinations in cidr 11111111, 11111110, 11111100, 11110000, 11100000,
       case $octs in
          0);;
          128) cidr_bits=$(($cidr_bits + 1));;
          192) cidr_bits=$(($cidr_bits + 2));;
          224) cidr_bits=$(($cidr_bits + 3));;
          240) cidr_bits=$(($cidr_bits + 4));;
          248) cidr_bits=$(($cidr_bits + 5));;
          252) cidr_bits=$(($cidr_bits + 6));;
          254) cidr_bits=$(($cidr_bits + 7));;
          255) cidr_bits=$(($cidr_bits + 8));;
          *) echo "Error: wrong netmask octets $octs";
       esac
    done
    echo $cidr_bits
}
function get_ip_from_ipcidr()
{
   ipcidr=$1
   ipadd=$( echo $ipcidr|awk -F '[/]' '{print $1}')
   echo $ipadd
}
function get_netmask_from_ipcidr()
{
   ipcidr=$1
   cidr=$( echo $ipcidr|awk -F '[/]' '{print $2}')
   netmask="$( cidr_to_netmask $cidr )"
   echo $netmask
}
#  PFILEP
## FILE: c_init_workaround
#!/bin/bash
#set -x
#Possible requirement for this script: Set hostkey checking to no
sed --in-place "s|#\s*StrictHostKeyChecking\s*ask|StrictHostKeyChecking no|" /etc/ssh/ssh_config
#Possible requirement for this script: set ssh key permissions to 600
#chmod 600 /etc/ssh/ssh_host_ed*_key.pub
#chmod 600 /etc/ssh/ssh_host_ecdsa_key.pub
#Copy local cloud_hpc_init to all compute nodes. TODO: Call all nodes through a for loop
#scp -r /tmp/cloud_hpc_init/ ${cc_ip[0]}:/root
scp -r /tmp/cloud_hpc_init/ cc1:/root
#Execute cloud_hpc_init/chpc_init on nodes using pdsh. TODO: Call all nodes through a for loop
pdsh -w cc1 /root/cloud_hpc_init/chpcInit
#set +x
#  PFILEP
## FILE: setup_cloud_hpc.sh
#!/bin/bash
# -----------------------------------------------------------------------------------------
#  Example Installation Script Template
#  
#  This convenience script encapsulates command-line instructions highlighted in
#  the Install Guide that can be used as a starting point to perform a local
#  cluster install beginning with bare-metal. Necessary inputs that describe local
#  hardware characteristics, desired network settings, and other customizations
#  are controlled via a companion input file that is used to initialize variables 
#  within this script.
#   
#  Please see the Install Guide for more information regarding the
#  procedure. Note that the section numbering included in this script refers to
#  corresponding sections from the install guide.
# -----------------------------------------------------------------------------------------
if [[ $EUID -ne 0 ]]; then echo "ERROR: Please run $0 as root"; exit 1; fi
set -E # traps on ERR will now be inherited by shell functions,
       # command substitution or subshells - equivalent to set -o errtrace
#For Debugging 
#set -x
SCRIPTDIR="$( cd "$( dirname "$( readlink -f "${BASH_SOURCE[0]}" )" )" && pwd -P && echo x)"
SCRIPTDIR="${SCRIPTDIR%x}"
cd $SCRIPTDIR
SCRIPTDIR=$PWD
pwd
packstack_install=0
orchestrator_install=0
openhpc_install=0
USECASE=1
# enable common functions
source common_functions
usage () {
  echo "USAGE: $0 [-f] [-h] [-c] [-o] [-p] [-i=<input.local>] [-n=<cloud_node_inventory>] [-u=<use case id>]"
  echo " -u,--usecase       Select use case, 1, 2 or 3." 
  echo " -c,--openhpc       Install OpenHPC using the OpenHPC installation recipe"
  echo " -f,--forced        Forced run, run all sections with no prompt"
  echo " -h,--help          Print this message"
  echo " -i,--input         Location in local inputs"
  echo " -n,--inventory     Input to cloud HPC inventory file"
  echo " -o,--orchestrator  Install HPC Orchestrator using the HPC Orchestrator recipe"
  echo " -p,--packstack     Install OpenStack using the PackStack installation recipe"
}
for i in "$@"; do
  case $i in
    -c|--openhpc)
      openhpc_install=1
      shift # past argument with no value
    ;;
    -i=*|--input=*)
      if echo $i | grep '~'; then
        echo "ERROR: tilde(~) in pathname not supported."
        exit 3
      fi
      INPUT_LOCAL="${i#*=}"
      shift # past argument=value
    ;;
    -u=*|--usecase=*)
      export USECASE="${i#*=}"
      # Check if a valid use case
      if [[ $USECASE != "1" && $USECASE != "2" && $USECASE != "3" ]]; then
        echo "Unsupported usecase"
        exit 1
      fi
    ;;
    -f|--forced)
      FORCED=YES
      shift # past argument with no value
    ;;
    -n=*|--inventory=*)
      if echo $i | grep '~'; then
        echo "ERROR: tilde(~) in pathname not supported."
        exit 3
      fi
      CLOUD_HPC_INVENTORY="${i#*=}"
      shift # past argument with no value
    ;;
    -o|--orchestrator)
      orchestrator_install=1
      shift # past argument with no value
    ;;
    -p|--packstack)
      packstack_install=1
      shift # past argument with no value
    ;;
    -h|--help)
      usage
      exit 1
    ;;
    *)
      echo "ERROR: Unknown option \"$i\""
      usage
      exit 2
    ;;
  esac
done
# Check if a valid use case is selected
if [[ $USECASE != "1" && $USECASE != "2" && $USECASE != "3" ]]; then
  echo "Unsupported usecase, select a valid usecase [1,2,3]"
  exit 1
fi
inputFile=$(readlink -f ${INPUT_LOCAL})
cloudHpcInventory=$(readlink -f ${CLOUD_HPC_INVENTORY})
validateInputFile
validateHpcInventory
# -------------------------------- Begin Recipe -------------------------------------------
# Commands below are extracted from the install guide recipe and are intended for 
# execution on the master SMS host.
# -----------------------------------------------------------------------------------------
# Determine number of cloud computes and their hostnames
setup_computename
#Set the hostname of the machine
#hostnamectl set-hostname ${sms_name}
#Install hpc orchestrator OR openhpc
if [ "$USECASE" != "3" ]; then
    if [ "${orchestrator_install}" -eq "1" ]; then
        mkdir -p /mnt/hpc_orch_iso
        mount -o loop ${orch_iso_path} /mnt/hpc_orch_iso
        rpm -Uvh /mnt/hpc_orch_iso/x86_64/Intel_HPC_Orchestrator_release-16.01.002.beta-8.1.x86_64.rpm
        rpm --import /etc/pki/pgp/HPC-Orchestrator*.asc
        pushd hpc_cent7/intel
        time source recipe.sh -f
        popd
    fi
    if [ "${openhpc_install}" -eq "1" ]; then
        export OHPC_INPUT_LOCAL=$(realpath ${INPUT_LOCAL})
        pushd hpc_cent7/ohpc
        time source recipe.sh
        popd
    fi
fi
#Run packstack install.
if [ "${packstack_install}" -eq "1" ]; then
    pushd ../packstack/recipe
    time source packstack-install.sh -s=${controller_ip} -f=${cc_subnet_cidr} -e=${sms_eth_internal}
    popd
fi
#set up hosts at head node or sms node
setup_hosts
#set -x
case $USECASE in
  1)
    pushd 1_combined_controller
    time source set_os_hpc
    popd
  ;;
  2)
    pushd 2_cloud_extension
    time source set_os_hpc
    popd
  ;;
  3)
    pushd 3_hpc_as_service
    time source set_os_hpc
    popd
  ;;
  *)
    echo "ERROR: Unsupported usecase"
    exit 1
  ;;
esac
    
true
#Call sinfo and srun to verify slurm's connection to the compute nodes
#sinfo
#srun -N ${num_ccomputes} hostname -i
# End
#  PFILEP
## FILE: hpc_cent7/intel/recipe.sh
#!/bin/bash
# -----------------------------------------------------------------------------------------
#  Example Installation Script Template
#  
#  This convenience script encapsulates command-line instructions highlighted in
#  the Install Guide that can be used as a starting point to perform a local
#  cluster install beginning with bare-metal. Necessary inputs that describe local
#  hardware characteristics, desired network settings, and other customizations
#  are controlled via a companion input file that is used to initialize variables 
#  within this script.
#   
#  Please see the Install Guide for more information regarding the
#  procedure. Note that the section numbering included in this script refers to
#  corresponding sections from the install guide.
# -----------------------------------------------------------------------------------------
if [[ $EUID -ne 0 ]]; then echo "ERROR: Please run $0 as root"; exit 1; fi
set -E # traps on ERR will now be inherited by shell functions,
       # command substitution or subshells - equivalent to set -o errtrace
SCRIPTDIR="$( cd "$( dirname "$( readlink -f "${BASH_SOURCE[0]}" )" )" && pwd -P && echo x)"
SCRIPTDIR="${SCRIPTDIR%x}"
cd $SCRIPTDIR
pwd
SECTIONNUM=0
SECTIONNAME[$SECTIONNUM]="$0"
# this syntax needed to -1+1 doesn't have an RC of 1
CountErrorTrap() { SECTIONERR[$SECTIONNUM]=$((++SECTIONERR[$SECTIONNUM])); }
ReportExitTrap() {
  echo -e "\nError count by section:"
  for I in ${!SECTIONNAME[@]}; do
    printf "%-72s : %d\n" "${SECTIONNAME[$I]}" ${SECTIONERR[$I]}
  done
}
trap CountErrorTrap ERR
trap ReportExitTrap EXIT
askrun () {
  SECTIONNUM=${#SECTIONNAME[@]}
  SECTIONNAME[$SECTIONNUM]="$2"
  SECTIONERR[$SECTIONNUM]=0
  while [ -z "$FORCED" ]; do
    read -p "Run \"$2\", Yes/Abort? [(y),a]: " answer
    case ${answer:0:1} in
      ""|y|Y )
        break
      ;;
      a|A )
        echo -e "Aborting $0\n"
        exit 1
      ;;
    esac
  done
  time source $1
  if [ $? -ne 0 ]; then
    echo    "###############################"
    echo    "## Section Execution Failure ##"
    echo    "###############################\n"
    echo -e "Aborting $0\n"
    exit 1
  fi
  echo -ne "$3"
  # we're back to meta script errors
  SECTIONNUM=0
}
usage () {
  echo "USAGE: $0 [-f] [-h] [-i=<input.local>]"
  echo " -f,--forced     Forced run, run all sections with no prompt"
  echo " -i,--input      Location in local inputs"
  echo " -h,--help       Print this message"
}
for i in "$@"; do
  case $i in
    -i=*|--input=*)
      if echo $i | grep '~'; then
        echo "ERROR: tilde(~) in pathname not supported."
        exit 3
      fi
      INPUT_LOCAL="${i#*=}"
      shift # past argument=value
    ;;
    -f|--forced)
      FORCED=YES
      shift # past argument with no value
    ;;
    -h|--help)
      usage
      exit 1
    ;;
    *)
      echo "ERROR: Unknown option \"$i\""
      usage
      exit 2
    ;;
  esac
done
inputFile=${INPUT_LOCAL}
if [[ -z "${inputFile}" || ! -e "${inputFile}" ]];then
  echo "Error: Unable to access local input file -> \"${inputFile}\""
  exit 1
else
  . ${inputFile} || { echo "Error sourcing ${inputFile}"; exit 1; }
fi
_BADCOUNT=0
for((i=0; i<${#c_ip[@]}; i++)) ; do
  if ! [[ ${c_ip[i]} =~ ^(([0-9]|[1-9][0-9]|1[0-9][0-9]|2[0-4][0-9]|25[0-5])\.){3}([\
                            0-9]|[1-9][0-9]|1[0-9][0-9]|2[0-4][0-9]|25[0-5])$ ]]; then
    echo "ERROR: Invalid IP address #$i: ${c_ip[i]}"
    _BADCOUNT=$((_BADCOUNT+1))
  fi
  if ! [[ ${c_bmc[i]} =~ ^(([0-9]|[1-9][0-9]|1[0-9][0-9]|2[0-4][0-9]|25[0-5])\.){3}([\
                             0-9]|[1-9][0-9]|1[0-9][0-9]|2[0-4][0-9]|25[0-5])$ ]]; then
    echo "ERROR: Invalid BMC IP address #$i: ${c_bmc[i]}"
    _BADCOUNT=$((_BADCOUNT+1))
  fi
  if ! [[ `echo ${c_mac[i]^^} | egrep "^([0-9A-F]{2}:){5}[0-9A-F]{2}$"` ]]; then
    echo "ERROR: Invalid MAC address #$i: ${c_mac[i]}"
    _BADCOUNT=$((_BADCOUNT+1))
  fi
done
[[ $_BADCOUNT -eq 0 ]] || exit 3
# -------------------------------- Begin Recipe -------------------------------------------
# Commands below are extracted from the install guide recipe and are intended for 
# execution on the master SMS host.
# -----------------------------------------------------------------------------------------
# Determine number of computes and their hostnames
export num_computes=${num_computes:-${#c_ip[@]}}
for((i=0; i<${num_computes}; i++)) ; do
   c_name[$i]=${nodename_prefix}$((i+1))
done
export c_name
true
# Enable required repositories (Section 3.1)-(Section 3.2)
askrun "./sections/sec3.1-sec3.2:Enable_required_repositories.sh" \
  "Enable required repositories (Section 3.1)-(Section 3.2)" "\n\n"
cd $SCRIPTDIR
# Initial HeadNode configuration (Section 3.4)-(Section 3.7)
askrun "./sections/sec3.4-sec3.7:Initial_HeadNode_configuration.sh" \
  "Initial HeadNode configuration (Section 3.4)-(Section 3.7)" "\n\n"
cd $SCRIPTDIR
# Define compute image for provisioning (Section 3.8)-(Section 3.8.3)
askrun "./sections/sec3.8-sec3.8.3:Define_compute_image_for_provisioning.sh" \
  "Define compute image for provisioning (Section 3.8)-(Section 3.8.3)" "\n\n"
cd $SCRIPTDIR
# Additional Customizations (optional) (Section 3.8.4)-(Section 3.8.4.11)
askrun "./sections/sec3.8.4-sec3.8.4.11:Additional_Customizations_-optional-.sh" \
  "Additional Customizations (optional) (Section 3.8.4)-(Section 3.8.4.11)" "\n\n"
cd $SCRIPTDIR
# Finalize Provisioning (Section 3.9)-(Section 3.10)
askrun "./sections/sec3.9-sec3.10:Finalize_Provisioning.sh" \
  "Finalize Provisioning (Section 3.9)-(Section 3.10)" "\n\n"
cd $SCRIPTDIR
# Install Development Components (Section 4.1)-(Section 4.7)
askrun "./sections/sec4.1-sec4.7:Install_Development_Components.sh" \
  "Install Development Components (Section 4.1)-(Section 4.7)" "\n\n"
cd $SCRIPTDIR
# Resource Manager Startup (Section 5)-(Section 6)
askrun "./sections/sec5-sec6:Resource_Manager_Startup.sh" \
  "Resource Manager Startup (Section 5)-(Section 6)" "\n\n"
cd $SCRIPTDIR
# CLCK Supportability Extensions (Section 7)
askrun "./sections/sec7:CLCK_Supportability_Extensions.sh" \
  "CLCK Supportability Extensions (Section 7)" "\n\n"
cd $SCRIPTDIR
#Workaround for bad install orchestrator
echo "Workaround for section 4 installation bug of orchestrator install"
time source orch_bug_wr
# End
#  PFILEP
## FILE: hpc_cent7/intel/sections/sec5-sec6:Resource_Manager_Startup.sh
#!/bin/bash
echo ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>"
echo "> Resource Manager Startup (Section 5)-(Section 6)"
echo ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>"
# ------------------------------------------------
# Resource Manager Startup (Section 5)-(Section 6)
# ------------------------------------------------
systemctl enable munge
systemctl enable slurmctld
systemctl start munge
systemctl start slurmctld
# Here we need to run Postboot script on Cloud Compute nodes
#pdsh -w ${nodename_prefix}[1-${num_computes}] systemctl enable munge
#pdsh -w ${nodename_prefix}[1-${num_computes}] systemctl start munge
#pdsh -w ${nodename_prefix}[1-${num_computes}] systemctl enable slurmd
#pdsh -w ${nodename_prefix}[1-${num_computes}] systemctl start slurmd
# for Demo all the nodes are from Cloud so this needs to be called once nodes are added and slurm is started on nodes.
#scontrol update nodename=${nodename_prefix}[1-${num_computes}] state=idle
useradd -U -m test
chown -R test:test ~test
#wwsh file resync passwd shadow group
# Wait for WW to recalc checksums on synced files
sleep 5
#pdsh -w ${nodename_prefix}[1-${num_computes}] /warewulf/bin/wwgetfiles 
true
#  PFILEP
## FILE: hpc_cent7/intel/sections/sec3.8-sec3.8.3:Define_compute_image_for_provisioning.sh
#!/bin/bash
echo ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>"
echo "> Define compute image for provisioning (Section 3.8)-(Section 3.8.3)"
echo ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>"
# -------------------------------------------------------------------
# Define compute image for provisioning (Section 3.8)-(Section 3.8.3)
# -------------------------------------------------------------------
# For HPC Cloug POC, we are not using warewulf so comment this
#if [[ "http://BOS.mirror.requiredx" != "${BOS_MIRROR}x" ]]; then
#     perl -pi -e "s#^YUM_MIRROR=(\S*)#YUM_MIRROR=${BOS_MIRROR}#" \
#      /usr/libexec/warewulf/wwmkchroot/rhel-7.tmpl
#fi
#export CHROOT=/opt/intel/hpc-orchestrator/admin/images/rhel7.2
#wwmkchroot rhel-7 $CHROOT
#cp -p /etc/resolv.conf $CHROOT/etc/resolv.conf
# Add components to compute instance
#yum -y --installroot=$CHROOT groupinstall orch-slurm-client
#yum -y --installroot=$CHROOT install pdsh-orch
#yum -y --installroot=$CHROOT groupinstall "InfiniBand Support"
#yum -y --installroot=$CHROOT install infinipath-psm
#chroot $CHROOT systemctl enable rdma
#yum -y --installroot=$CHROOT install ntp
#yum -y --installroot=$CHROOT install kernel
#yum -y --installroot=$CHROOT install lmod-orch
### ssh keys need to be trasfered via cloud init
#wwinit ssh_keys
#cat ~/.ssh/cluster.pub >> $CHROOT/root/.ssh/authorized_keys
# This will be done post boot via CloudInit
# nfs mount is perfromed at post boot
#echo "${sms_ip}:/home /home nfs nfsvers=3,rsize=1024,wsize=1024,cto 0 0" >> $CHROOT/etc/fstab
#echo "${sms_ip}:/opt/intel/hpc-orchestrator/pub" \
# "/opt/intel/hpc-orchestrator/pub nfs nfsvers=3 0 0" \
# >> $CHROOT/etc/fstab
perl -pi -e "s/ControlMachine=\S+/ControlMachine=${sms_name}/" /etc/slurm/slurm.conf
echo "/home *(rw,no_subtree_check,fsid=10,no_root_squash)" >> /etc/exports
echo "/opt/intel/hpc-orchestrator/pub *(rw,no_subtree_check,fsid=11,no_root_squash)" >> /etc/exports
exportfs -a
systemctl restart nfs
systemctl enable nfs-server
#chroot $CHROOT systemctl enable ntpd
#echo "server ${sms_ip}" >> $CHROOT/etc/ntp.conf
# Update basic slurm configuration
   perl -pi -e "s/^NodeName=(\S+)/NodeName=${nodename_prefix}[1-${num_computes}]/" /etc/slurm/slurm.conf
   perl -pi -e "s/^PartitionName=normal Nodes=(\S+)/PartitionName=normal Nodes=${nodename_prefix}[1-${num_computes}]/" /etc/slurm/slurm.conf
   #perl -pi -e "s/^NodeName=(\S+)/NodeName=${nodename_prefix}[1-${num_computes}]/" $CHROOT/etc/slurm/slurm.conf
   #perl -pi -e "s/^PartitionName=normal Nodes=(\S+)/PartitionName=normal Nodes=${nodename_prefix}[1-${num_computes}]/" $CHROOT/etc/slurm/slurm.conf
true
#  PFILEP
## FILE: hpc_cent7/intel/sections/sec3.8.4-sec3.8.4.11:Additional_Customizations_-optional-.sh
#!/bin/bash
echo ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>"
echo "> Additional Customizations (optional) (Section 3.8.4)-(Section 3.8.4.11)"
echo ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>"
# -----------------------------------------------------------------------
# Additional Customizations (optional) (Section 3.8.4)-(Section 3.8.4.11)
# -----------------------------------------------------------------------
# For Cloud POC we are not using warewulf, and image will be created using diskimage-builder tool so comment all image creation here
#
perl -pi -e 's/# End of file/\* soft memlock unlimited\n$&/s' /etc/security/limits.conf
perl -pi -e 's/# End of file/\* hard memlock unlimited\n$&/s' /etc/security/limits.conf
##perl -pi -e 's/# End of file/\* soft memlock unlimited\n$&/s' $CHROOT/etc/security/limits.conf
##perl -pi -e 's/# End of file/\* hard memlock unlimited\n$&/s' $CHROOT/etc/security/limits.conf
if [[ ${enable_mrsh} -eq 1 ]];then
     # Install mrsh
     yum -y install mrsh-orch mrsh-rsh-compat-orch
     ##yum -y --installroot=$CHROOT install mrsh-orch mrsh-rsh-compat-orch mrsh-server-orch
     echo "mshell          21212/tcp                  # mrshd" >> /etc/services
     echo "mlogin            541/tcp                  # mrlogind" >> /etc/services
     ##chroot $CHROOT systemctl enable xinetd
fi
# Enable slurm pam module
##echo "account    required     pam_slurm.so" >> $CHROOT/etc/pam.d/sshd
##if [[ -f "$CHROOT/etc/pam.d/mrsh" && -f "$CHROOT/etc/pam.d/mrlogin" ]]; then
   ##echo "account    required     pam_slurm.so" >> $CHROOT/etc/pam.d/mrsh
   ##echo "account    required     pam_slurm.so" >> $CHROOT/etc/pam.d/mrlogin
#fi
# for now lets not use clck
#yum -y install intel-clck-orch
##yum -y --installroot=$CHROOT install intel-clck-orch
#yum -y install  intel-clck-supportability-orch
# Enable Optional packages
if [[ ${enable_lustre_client} -eq 1 ]];then
     # Install Lustre client on master
     yum -y install lustre-client-orch lustre-client-orch-modules
     # Enable lustre in WW compute image
     #yum -y --installroot=$CHROOT install lustre-client-orch lustre-client-orch-modules
     #mkdir $CHROOT/mnt/lustre
     #echo "${mgs_fs_name} /mnt/lustre lustre _netdev,lazystatfs,flock,nosuid,defaults 0 0" >> $CHROOT/etc/fstab
     # Enable o2ib for Lustre
     echo "options lnet networks=o2ib(ib0)" >> /etc/modprobe.d/lustre.conf
     #echo "options lnet networks=o2ib(ib0)" >> $CHROOT/etc/modprobe.d/lustre.conf
     # mount Lustre client on master
     mkdir /mnt/lustre
     mount -t lustre -o _netdev,lazystatfs,flock,nosuid,defaults ${mgs_fs_name} /mnt/lustre
fi
if [[ ${enable_nagios} -eq 1 ]];then
     # Install Nagios on master and vnfs image
     yum -y groupinstall orch-nagios
     #yum -y --installroot=$CHROOT groupinstall orch-nagios
     #chroot $CHROOT systemctl enable nrpe
     #perl -pi -e "s/^allowed_hosts=/# allowed_hosts=/" $CHROOT/etc/nagios/nrpe.cfg
     #echo "nrpe 5666/tcp # NRPE"         >> $CHROOT/etc/services
     #echo "nrpe : ${sms_ip}  : ALLOW"    >> $CHROOT/etc/hosts.allow
     #echo "nrpe : ALL : DENY"            >> $CHROOT/etc/hosts.allow
     #chroot $CHROOT getent group nrpe  || chroot $CHROOT /usr/sbin/groupadd -r nrpe
     #chroot $CHROOT getent passwd nrpe || chroot $CHROOT /usr/sbin/useradd -c \
      "NRPE user for the NRPE service" -d /var/run/nrpe -r -g nrpe -s /sbin/nologin nrpe
     mv /etc/nagios/conf.d/services.cfg.example /etc/nagios/conf.d/services.cfg
     mv /etc/nagios/conf.d/hosts.cfg.example /etc/nagios/conf.d/hosts.cfg
     for ((i=0; i<$num_computes; i++)) ; do
        perl -pi -e "s/HOSTNAME$(($i+1))/${c_name[$i]}/ || s/HOST$(($i+1))_IP/${c_ip[$i]}/" \
        /etc/nagios/conf.d/hosts.cfg
     done
     perl -pi -e "s/ \/bin\/mail/ \/usr\/bin\/mailx/g" /etc/nagios/objects/commands.cfg
     perl -pi -e "s/nagios\@localhost/root\@${sms_name}/" /etc/nagios/objects/contacts.cfg
     echo command[check_ssh]=/usr/lib64/nagios/plugins/check_ssh localhost \
      #>> $CHROOT/etc/nagios/nrpe.cfg
     chkconfig nagios on
     systemctl start nagios
     chmod u+s `which ping`
     mkdir /usr/share/warewulf/www
     touch /usr/share/warewulf/www/test.html
fi
if [[ ${enable_ganglia} -eq 1 ]];then
     # Install Ganglia on master
     yum -y groupinstall orch-ganglia
     #yum -y --installroot=$CHROOT install ganglia-gmond-orch
     cp /opt/intel/hpc-orchestrator/pub/examples/ganglia/gmond.conf /etc/ganglia/gmond.conf
     perl -pi -e "s/<sms>/${sms_name}/" /etc/ganglia/gmond.conf
     #cp /etc/ganglia/gmond.conf $CHROOT/etc/ganglia/gmond.conf
     echo "gridname MySite" >> /etc/ganglia/gmetad.conf
     systemctl enable gmond
     systemctl enable gmetad
     systemctl start gmond
     systemctl start gmetad
     #chroot $CHROOT systemctl enable gmond
     systemctl try-restart httpd
fi
if [[ ${enable_clustershell} -eq 1 ]];then
     # Install clustershell
     yum -y install clustershell-orch
     cd /etc/clustershell/groups.d
     mv local.cfg local.cfg.orig
     echo "adm: ${sms_name}" > local.cfg
     echo "compute: c[1-${num_computes}]" >> local.cfg
     echo "all: @adm,@compute" >> local.cfg
fi
if [[ ${enable_powerman} -eq 1 ]];then
     # Optionally, install powerman
     yum -y install powerman-orch
     cp /etc/powerman/powerman.conf{.example,}
     chown daemon:root /etc/powerman/powerman.conf
     chmod 0400 /etc/powerman/powerman.conf
     perl -pi -e 's/^\#(tcpwrappers yes)/$1/' /etc/powerman/powerman.conf
     perl -pi -e 's/^\#(listen "0.0.0.0:10101")/$1/' /etc/powerman/powerman.conf
     perl -pi -e 's/^\#(include "\/etc\/powerman\/ipmipower\.dev")/$1/' \
      /etc/powerman/powerman.conf
     for ((i=0; i<$num_computes; i++)) ; do
        perl -pi -e 'print "device \"ipmi'$i'\" \"ipmipower\" \"/usr/sbin/ipmipower -h ".
        "'${c_bmc[$i]}' -u '$bmc_username' -p ".
        "'${IPMI_PASSWORD:-undefined}'|&\"\n" if(/^\#device "ipmi1"/);' /etc/powerman/powerman.conf
     done
     for ((i=0; i<$num_computes; i++)) ; do
        perl -pi -e 'print "node \"'${c_name[$i]}'\" \"ipmi'$i'\" \"'${c_bmc[$i]}'\"\n"
        if(/^\#node "t1"/);' /etc/powerman/powerman.conf
     done
     systemctl start powerman
     pm -q
fi
# Optionally, enable conman and configure
if [[ ${enable_ipmisol} -eq 1 ]];then
     yum -y install conman-orch
     for ((i=0; i<$num_computes; i++)) ; do
        echo -n 'CONSOLE name="'${c_name[$i]}'" dev="ipmi:'${c_bmc[$i]}'" '
        echo 'ipmiopts="'U:${bmc_username},P:${IPMI_PASSWORD:-undefined},W:solpayloadsize'"'
     done >> /etc/conman.conf
     systemctl enable conman
     systemctl start conman
fi
#Update rsyslog
perl -pi -e "s/\\#\\\$ModLoad imudp/\\\$ModLoad imudp/" /etc/rsyslog.conf
perl -pi -e "s/\\#\\\$UDPServerRun 514/\\\$UDPServerRun 514/" /etc/rsyslog.conf
systemctl restart rsyslog
#echo "*.* @${sms_ip}:514" >> $CHROOT/etc/rsyslog.conf
#perl -pi -e "s/^\*\.info/\\#\*\.info/" $CHROOT/etc/rsyslog.conf
#perl -pi -e "s/^authpriv/\\#authpriv/" $CHROOT/etc/rsyslog.conf
#perl -pi -e "s/^mail/\\#mail/" $CHROOT/etc/rsyslog.conf
#perl -pi -e "s/^cron/\\#cron/" $CHROOT/etc/rsyslog.conf
#perl -pi -e "s/^uucp/\\#uucp/" $CHROOT/etc/rsyslog.conf
#No Warewulf for now. These needs to be synced either via Nova or post boot
#wwsh file import /etc/passwd
#wwsh file import /etc/group
#wwsh file import /etc/shadow 
#wwsh file import /etc/slurm/slurm.conf
#wwsh file import /etc/pam.d/slurm
#wwsh file import /etc/munge/munge.key
if [[ ${enable_ipoib} -eq 1 ]];then
     cp /opt/intel/hpc-orchestrator/pub/examples/network/rhel/ifcfg-ib0.ww /tmp/
     #wwsh file import /tmp/ifcfg-ib0.ww
     #wwsh -y file set ifcfg-ib0.ww --path=/etc/sysconfig/network-scripts/ifcfg-ib0
fi
true
#  PFILEP
## FILE: hpc_cent7/intel/sections/sec3.1-sec3.2:Enable_required_repositories.sh
#!/bin/bash
echo ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>"
echo "> Enable required repositories (Section 3.1)-(Section 3.2)"
echo ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>"
# --------------------------------------------------------
# Enable required repositories (Section 3.1)-(Section 3.2)
# --------------------------------------------------------
# Verify local repository has been enabled before proceeding
echo "This checks if the ISO file mounted at /mnt/hpc_orch_iso is enabled as a local repo" 
yum repolist | grep -q Orchestrator
if [ $? -ne 0 ];then
   echo "Error: Mounted local repository is not enabled"
   echo "Error: Check that /mnt/hpc_orch_iso or /etc/yum.repos.d/HPC_Orchestrator.repo are installed properly"
   exit 1
fi
true
#  PFILEP
## FILE: hpc_cent7/intel/sections/sec3.4-sec3.7:Initial_HeadNode_configuration.sh
#!/bin/bash
echo ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>"
echo "> Initial HeadNode configuration (Section 3.4)-(Section 3.7)"
echo ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>"
# ----------------------------------------------------------
# Initial HeadNode configuration (Section 3.4)-(Section 3.7)
# ----------------------------------------------------------
yum -y groupinstall orch-base
# No warewulf on Controller node, since we will install OpenStack
#yum -y groupinstall orch-warewulf
# Disable firewall 
rpm -q firewalld && systemctl disable firewalld
rpm -q firewalld && systemctl stop firewalld
# Enable NTP services on SMS host
systemctl enable ntpd.service
echo "server ${ntp_server}" >> /etc/ntp.conf
systemctl restart ntpd
yum -y groupinstall orch-slurm-server
getent passwd slurm || useradd slurm
perl -pi -e "s|^#UsePAM=|UsePAM=1|" /etc/slurm/slurm.conf
cat <<- HERE > /etc/pam.d/slurm
	account  required  pam_unix.so
	account  required  pam_slurm.so
	auth     required  pam_localuser.so
	session  required  pam_limits.so
	HERE
yum -y groupinstall "InfiniBand Support"
yum -y install infinipath-psm
systemctl enable rdma
systemctl start rdma
if [[ ${enable_ipoib} -eq 1 ]];then
     # Enable ib0
     cp /opt/intel/hpc-orchestrator/pub/examples/network/rhel/ifcfg-ib0 /etc/sysconfig/network-scripts
     perl -pi -e "s/master_ipoib/${sms_ipoib}/" /etc/sysconfig/network-scripts/ifcfg-ib0
     perl -pi -e "s/ipoib_netmask/${ipoib_netmask}/" /etc/sysconfig/network-scripts/ifcfg-ib0
     ifup ib0
fi
# No Warewulf in HPC POC, so remove this 
#perl -pi -e "s/device = eth1/device = ${sms_eth_internal}/" /etc/warewulf/provision.conf
#perl -pi -e "s/^\s+disable\s+= yes/ disable = no/" /etc/xinetd.d/tftp
#export MODFILE=/etc/httpd/conf.d/warewulf-httpd.conf
#perl -pi -e "s/cgi-bin>\$/cgi-bin>\n Require all granted/" $MODFILE
#perl -pi -e "s/Allow from all/Require all granted/" $MODFILE
#perl -ni -e "print unless /^\s+Order allow,deny/" $MODFILE
# Assign static IP for now comment this
#ifconfig ${sms_eth_internal} ${sms_ip} netmask ${internal_netmask} up
#these are needed for warewulf, so comment for now
#systemctl restart xinetd
#systemctl enable mariadb.service
#systemctl restart mariadb
#systemctl enable httpd.service
#systemctl restart httpd
# Install genders
yum -y install genders-orch
echo -e "${sms_name}\tsms,pdsh_all_skip" > /etc/genders
echo -e "${sms_name}\tinternal_eth=${sms_eth_internal}" > /etc/genders
for ((i=0; i<$num_computes; i++)) ; do
   echo -e "${c_name[$i]}\tcompute,bmc=${c_bmc[$i]}"
done >> /etc/genders
true
#  PFILEP
## FILE: hpc_cent7/intel/sections/sec7:CLCK_Supportability_Extensions.sh
#!/bin/bash
echo ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>"
echo "> CLCK Supportability Extensions (Section 7)"
echo ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>"
# ------------------------------------------
# CLCK Supportability Extensions (Section 7)
# ------------------------------------------
source /etc/profile.d/lmod.sh
#module load clck
#$CLCK_ROOT/bin/intel64/suppressions/clck_add_suppressions.sh
true
#  PFILEP
## FILE: hpc_cent7/intel/sections/sec3.9-sec3.10:Finalize_Provisioning.sh
#!/bin/bash
echo ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>"
echo "> Finalize Provisioning (Section 3.9)-(Section 3.10)"
echo ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>"
# --------------------------------------------------
# Finalize Provisioning (Section 3.9)-(Section 3.10)
# --------------------------------------------------
# for HPC Cloud no warewulf so commenting thoese parts
#export WW_CONF=/etc/warewulf/bootstrap.conf
#echo "drivers += updates/kernel/" >> $WW_CONF
#wwbootstrap `uname -r`
## if [[ ${enable_stateful} -ne 1 ]];then
#     # Assemble VNFS
#     wwvnfs -y --chroot $CHROOT
## fi
## Add hosts to cluster
# HPC Cloud these might be taken care  post boot, or openstack will take care of networking
# TBD Need to register node via Nova API
#echo "GATEWAYDEV=${eth_provision}" > /tmp/network.$$
#wwsh -y file import /tmp/network.$$ --name network
#wwsh -y file set network --path /etc/sysconfig/network --mode=0644 --uid=0
#for ((i=0; i<$num_computes; i++)) ; do
#   wwsh -y node new ${c_name[i]} --ipaddr=${c_ip[i]} --hwaddr=${c_mac[i]} -D ${eth_provision}
#done
# some of the files will be transfered via cloud init or post boot, files like slurm.conf,munge.key
## Add hosts to cluster (Cont.)
#wwsh -y provision set "${nodename_prefix}*" --vnfs=rhel7.2 --bootstrap=`uname -r` \
# --files=dynamic_hosts,passwd,group,shadow,slurm.conf,slurm,munge.key,network
# Optionally, add arguments to bootstrap kernel
#if [[ ${enable_kargs} ]]; then
#   wwsh provision set "${nodename_prefix}*" --kargs=${kargs}
#fi
# Restart ganglia services to pick up hostfile changes
if [[ ${enable_ganglia} -eq 1 ]];then
  systemctl restart gmond
  systemctl restart gmetad
fi
# Optionally, define IPoIB network settings (required if planning to mount Lustre over IB)
if [[ ${enable_ipoib} -eq 1 ]];then
     for ((i=0; i<$num_computes; i++)) ; do
        wwsh -y node set ${c_name[$i]} -D ib0 --ipaddr=${c_ipoib[$i]} --netmask=${ipoib_netmask}
     done
     wwsh -y provision set "${nodename_prefix}*" --fileadd=ifcfg-ib0.ww
fi
#No warewulf so comment these
#systemctl restart dhcpd
#wwsh pxe update || true
# Optionally, enable console redirection 
if [[ ${enable_ipmisol} -eq 1 ]];then
     wwsh -y provision set "${nodename_prefix}*" --kargs "${kargs} console=ttyS0,115200"
fi
# No warewulf
#if [[ ${enable_stateful} -eq 1 ]];then
#     # Add stateful provisioning support
#     yum -y --installroot=$CHROOT install grub2
#     wwvnfs -y --chroot $CHROOT
#fi
if [[ ${enable_stateful} -eq 1 ]];then
     # Add stateful node object parameters
     export sd1="mountpoint=/boot:dev=${stateful_dev}1:type=ext3:size=500"
     export sd2="dev=${stateful_dev}2:type=swap:size=32768"
     export sd3="mountpoint=/:dev=${stateful_dev}3:type=ext3:size=fill"
     for ((i=0; i<$num_computes; i++)); do
        wwsh -y object modify -s bootloader=${stateful_dev} -t node ${c_name[$i]};
        wwsh -y object modify -s diskpartition=${stateful_dev} -t node ${c_name[$i]};
        wwsh -y object modify -s \
        diskformat=${stateful_dev}1,${stateful_dev}2,${stateful_dev}3 -t node ${c_name[$i]};
        wwsh -y object modify -s filesystems="$sd1,$sd2,$sd3" -t node ${c_name[$i]};
     done
fi
# Here update the post provision script
# Create baremetal flavor with Nova
# register nodes with Nova
# register files with Nova
# restart the nodes with Nova or ironic
# then copy Postboot files to compute node
#for ((i=0; i<${num_computes}; i++)) ; do
#   ipmitool -E -I lanplus -H ${c_bmc[$i]} -U ${bmc_username} chassis power reset
#done
# wait for compute nodes to come up
sleep ${provision_wait}
# prevent re-imaging stateful nodes
#if [[ ${enable_stateful} -eq 1 ]];then
#  for ((i=0; i<$num_computes; i++)) ; do
#    wwsh -y object modify -s bootlocal=EXIT ${c_name[$i]};
#  done
#fi
true
#  PFILEP
## FILE: hpc_cent7/intel/sections/sec4.1-sec4.7:Install_Development_Components.sh
#!/bin/bash
echo ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>"
echo "> Install Development Components (Section 4.1)-(Section 4.7)"
echo ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>"
# ----------------------------------------------------------
# Install Development Components (Section 4.1)-(Section 4.7)
# ----------------------------------------------------------
yum -y groupinstall orch-autotools
#yum -y install valgrind-orch
yum -y install EasyBuild-orch
#yum -y install spack-orch
#yum -y install R_base-orch            
yum -y install gnu-compilers-orch intel-compilers-devel-orch
yum -y groupinstall orch-mpi
yum -y groupinstall orch-imb
#yum -y install papi-orch
#yum -y install intel-itac-orch
#yum -y install intel-vtune-orch
#yum -y install intel-advisor-orch
#yum -y install intel-inspector-orch
#yum -y groupinstall orch-mpiP
#yum -y groupinstall orch-tau
yum -y install lmod-defaults-intel-impi-orch
#yum -y groupinstall orch-adios        
#yum -y groupinstall orch-boost        
#yum -y groupinstall orch-fftw         
#yum -y groupinstall orch-gsl          
#yum -y groupinstall orch-hdf5         
#yum -y groupinstall orch-hypre        
#yum -y groupinstall orch-metis        
#yum -y groupinstall orch-mumps        
#yum -y groupinstall orch-netcdf       
#yum -y groupinstall orch-numpy        
#yum -y groupinstall orch-openblas     
#yum -y groupinstall orch-petsc        
#yum -y groupinstall orch-phdf5        
#yum -y groupinstall orch-scalapack    
#yum -y groupinstall orch-scipy        
#yum -y groupinstall orch-trilinos     
yum -y install testsuite-orch
echo "/opt/intel/hpc-orchestrator/pub/tests *(rw,no_subtree_check,fsid=12,no_root_squash)"
exportfs -a
#echo -n "${sms_ip}:/opt/intel/hpc-orchestrator/pub/tests " >> $CHROOT/etc/fstab
#echo "/opt/intel/hpc-orchestrator/pub/tests nfs nfsvers=3 0 0" >> $CHROOT/etc/fstab
#wwvnfs -y --chroot $CHROOT
#pdcp -g compute $CHROOT/etc/fstab /etc/fstab
# for Cloud this will be done in post cfg
#pdsh -g compute mount /opt/intel/hpc-orchestrator/pub/tests
true
#  PFILEP
## FILE: hpc_cent7/intel/input.local
# -*-sh-*-
# ------------------------------------------------------------------------------------------------
# ------------------------------------------------------------------------------------------------
# Template input file to define local variable settings for use with
# an installation recipe.
# ------------------------------------------------------------------------------------------------
# ---------------------------
# SMS (master) node settings
# ---------------------------
# Set location of local BOS mirror
BOS_MIRROR="${BOS_MIRROR:-http://BOS.mirror.required}"
# Hostname for master server (SMS)
sms_name="${sms_name:-sms}"
                              
# Local (internal) IP address on SMS
sms_ip="${sms_ip:-192.168.46.2}"
# Internal ethernet interface on SMS
sms_eth_internal="${sms_eth_internal:-eth1}"
# Subnet netmask for internal cluster network
internal_netmask="${internal_netmask:-255.255.0.0}"
# Provisioning interface used by compute hosts
eth_provision="${eth_provision:-eth1}"
# Local ntp server for time synchronization
ntp_server="${ntp_server:-0.centos.pool.ntp.org}"
# BMC user credentials for use by IPMI
bmc_username="${bmc_username:-unknown}"
bmc_password="${bmc_password:-unknown}"
# Additional time to wait for compute nodes to provision (seconds)
provision_wait="${provision_wait:-180}"
# Stateful install device
stateful_dev="${stateful_dev:-sda}"
# Flags for optional installation/configuration
enable_clustershell="${enable_clustershell:-0}"
enable_ipmisol="${enable_ipmisol:-0}"
enable_ipoib="${enable_ipoib:-0}"
enable_ganglia="${enable_ganglia:-0}"
enable_kargs="${enable_kargs:-0}"
enable_lustre_client="${enable_lustre_client:-0}"
enable_mrsh="${enable_mrsh:-0}"
enable_nagios="${enable_nagios:-0}"
enable_powerman="${enable_powerman:-0}"
enable_stateful="${enable_stateful:-0}"
# -------------------------
# compute node settings, are in independent files
# -------------------------
# Prefix for compute node hostnames
nodename_prefix="${nodename_prefix:-cc}"
#
#
## compute node IP addresses
c_ip[0]=192.168.46.5
#c_ip[1]=172.16.1.2
#c_ip[2]=172.16.1.3
#c_ip[3]=172.16.1.4
#
## compute node MAC addreses for provisioning interface
c_mac[0]=00:15:17:a3:7a:ed
#c_mac[1]=00:1a:2b:3c:4f:56
#c_mac[2]=00:1a:2b:3c:4f:56
#c_mac[3]=00:1a:2b:3c:4f:56
#
## compute node BMC addresses
c_bmc[0]=10.54.134.95
#c_bmc[1]=10.16.1.2
#c_bmc[2]=10.16.1.3
#c_bmc[3]=10.16.1.4
#
#-------------------
# Optional settings
#-------------------
# additional arguments to enable optional arguments for bootstrap kernel
kargs="${kargs:-acpi_pad.disable=1}"
# Lustre MGS mount name
mgs_fs_name="${mgs_fs_name:-192.168.100.254@o2ib:/lustre1}"
# Subnet netmask for IPoIB network
ipoib_netmask="${ipoib_netmask:-255.255.0.0}"
# IPoIB address for SMS server
sms_ipoib="${sms_ipoib:-192.168.0.1}"
# IPoIB addresses for computes
#c_ipoib[0]=192.168.1.1		            
#c_ipoib[1]=192.168.1.2
#c_ipoib[2]=192.168.1.3
#c_ipoib[3]=192.168.1.4
#  PFILEP
## FILE: hpc_cent7/intel/orch_bug_wr
pwd=$PWD
mkdir /tmp/
cd /tmp/
rpm2cpio /mnt/hpc_orch_iso/x86_64/testsuite-orch-1.0-122.1.x86_64.rpm | cpio -idmv
mv /tmp/opt/intel/hpc-orchestrator/pub/tests /opt/intel/hpc-orchestrator/pub/
mv /tmp/opt/intel/hpc-orchestrator/pub/modulefiles/testsuite /opt/intel/hpc-orchestrator/pub/modulefiles/
mv /tmp/opt/intel/hpc-orchestrator/pub/doc/contrib/testsuite-orch-1.0 /opt/intel/hpc-orchestrator/pub/doc/contrib/
cd $pwd
#  PFILEP
## FILE: hpc_cent7/intel/orch.conf
# -*-sh-*-
# ------------------------------------------------------------------------------------------------
# ------------------------------------------------------------------------------------------------
# Template input file to define local variable settings for use with
# an installation recipe.
# ------------------------------------------------------------------------------------------------
# ---------------------------
# SMS (master) node settings
# ---------------------------
# Set location of local BOS mirror
BOS_MIRROR="${BOS_MIRROR:-http://BOS.mirror.required}"
# Path to ISO file
orch_iso_path=/home/sunil/HPC-Orch/HPC-Orchestrator-rhel7.2u5-16.01.002.beta.iso
# Hostname for master server (SMS)
sms_name="${sms_name:-sun-hn1}"
                              
# Local (internal) IP address on SMS
sms_ip="${sms_ip:-192.168.46.2}"
# Internal ethernet interface on SMS
sms_eth_internal="${sms_eth_internal:-enp6s0f0}"
# Subnet netmask for internal cluster network
internal_netmask="${internal_netmask:-255.255.0.0}"
# Provisioning interface used by compute hosts
eth_provision="${eth_provision:-eth1}"
# Local ntp server for time synchronization
ntp_server="${ntp_server:-0.centos.pool.ntp.org}"
# BMC user credentials for use by IPMI
bmc_username="${bmc_username:root}"
bmc_password="${bmc_password:ppk123}"
# Additional time to wait for compute nodes to provision (seconds)
provision_wait="${provision_wait:-0}"
# Stateful install device
stateful_dev="${stateful_dev:-sda}"
# Flags for optional installation/configuration
enable_clustershell="${enable_clustershell:-0}"
enable_ipmisol="${enable_ipmisol:-0}"
enable_ipoib="${enable_ipoib:-0}"
enable_ganglia="${enable_ganglia:-0}"
enable_kargs="${enable_kargs:-0}"
enable_lustre_client="${enable_lustre_client:-0}"
enable_mrsh="${enable_mrsh:-1}"
enable_nagios="${enable_nagios:-0}"
enable_powerman="${enable_powerman:-0}"
enable_stateful="${enable_stateful:-0}"
# -------------------------
# compute node settings, are in independent files
# -------------------------
# Prefix for compute node hostnames
nodename_prefix="${nodename_prefix:-c}"
#
#
## compute node IP addresses
c_ip[0]=192.168.46.101
c_ip[1]=192.168.46.102
#c_ip[2]=192.168.46.103
#c_ip[3]=192.168.46.104
## compute node MAC addreses for provisioning interface
c_mac[0]=00:1a:2b:3c:4f:56
c_mac[1]=00:1a:2b:3c:4f:56
#c_mac[2]=00:1a:2b:3c:4f:56
#c_mac[3]=00:1a:2b:3c:4f:56
#
## compute node BMC addresses
c_bmc[0]=10.54.134.95
c_bmc[1]=10.54.134.95
#c_bmc[2]=10.16.1.3
#c_bmc[3]=10.16.1.4
#
#-------------------
# Optional settings
#-------------------
# additional arguments to enable optional arguments for bootstrap kernel
kargs="${kargs:-acpi_pad.disable=1}"
# Lustre MGS mount name
mgs_fs_name="${mgs_fs_name:-192.168.100.254@o2ib:/lustre1}"
# Subnet netmask for IPoIB network
ipoib_netmask="${ipoib_netmask:-255.255.0.0}"
# IPoIB address for SMS server
sms_ipoib="${sms_ipoib:-192.168.0.1}"
# IPoIB addresses for computes
c_ipoib[0]=192.168.1.1		            
c_ipoib[1]=192.168.1.2
c_ipoib[2]=192.168.1.3
c_ipoib[3]=192.168.1.4
#  PFILEP
## FILE: hpc_cent7/ohpc/recipe.sh
#!/bin/bash
# -----------------------------------------------------------------------------------------
#  Example Installation Script Template
#  
#  This convenience script encapsulates command-line instructions highlighted in
#  the OpenHPC Install Guide that can be used as a starting point to perform a local
#  cluster install beginning with bare-metal. Necessary inputs that describe local
#  hardware characteristics, desired network settings, and other customizations
#  are controlled via a companion input file that is used to initialize variables 
#  within this script.
#   
#  Please see the OpenHPC Install Guide for more information regarding the
#  procedure. Note that the section numbering included in this script refers to
#  corresponding sections from the install guide.
# -----------------------------------------------------------------------------------------
if [[ $EUID -ne 0 ]]; then echo "ERROR: Please run $0 as root"; exit 1; fi
#inputFile=${OHPC_INPUT_LOCAL:-/opt/ohpc/pub/doc/recipes/vanilla/input.local}
inputFile=${OHPC_INPUT_LOCAL}
if [ ! -e ${inputFile} ];then
   echo "Error: Unable to access local input file -> ${inputFile}"
   exit 1
else
   . ${inputFile} || { echo "Error sourcing ${inputFile}"; exit 1; }
fi
_BADCOUNT=0
for((i=0; i<${#c_ip[@]}; i++)) ; do
  if ! [[ ${c_ip[i]} =~ ^(([0-9]|[1-9][0-9]|1[0-9][0-9]|2[0-4][0-9]|25[0-5])\.){3}([\
                            0-9]|[1-9][0-9]|1[0-9][0-9]|2[0-4][0-9]|25[0-5])$ ]]; then
    echo "ERROR: Invalid IP address #$i: ${c_ip[i]}"
    _BADCOUNT=$((_BADCOUNT+1))
  fi
  if ! [[ ${c_bmc[i]} =~ ^(([0-9]|[1-9][0-9]|1[0-9][0-9]|2[0-4][0-9]|25[0-5])\.){3}([\
                             0-9]|[1-9][0-9]|1[0-9][0-9]|2[0-4][0-9]|25[0-5])$ ]]; then
    echo "ERROR: Invalid BMC IP address #$i: ${c_bmc[i]}"
    _BADCOUNT=$((_BADCOUNT+1))
  fi
  if ! [[ `echo ${c_mac[i]^^} | egrep "^([0-9A-F]{2}:){5}[0-9A-F]{2}$"` ]]; then
    echo "ERROR: Invalid MAC address #$i: ${c_mac[i]}"
    _BADCOUNT=$((_BADCOUNT+1))
  fi
done
[[ $_BADCOUNT -eq 0 ]] || exit 3
# Determine number of computes and their hostnames
export num_computes=${num_computes:-${#c_ip[@]}}
for((i=0; i<${num_computes}; i++)) ; do
   c_name[$i]=${nodename_prefix}$((i+1))
done
export c_name
# ---------------------------- Begin OpenHPC Recipe ---------------------------------------
# Commands below are extracted from an OpenHPC install guide recipe and are intended for 
# execution on the master SMS host.
# -----------------------------------------------------------------------------------------
# Install the OpenHPC rpm
yum -y install ${ohpc_pkg}
# Install docs-ohpc package
yum -y install docs-ohpc
# Verify OpenHPC repository has been enabled before proceeding
yum repolist | grep -q OpenHPC
if [ $? -ne 0 ];then
   echo "Error: OpenHPC repository must be enabled locally"
   exit 1
fi
# ------------------------------------------------------------
# Add baseline OpenHPC and provisioning services (Section 3.3)
# ------------------------------------------------------------
yum -y groupinstall ohpc-base
# Cloud HPC does not use Warewulf for provisioning
#yum -y groupinstall ohpc-warewulf
# Disabling of firwall is specific to warewulf usecase
# Disable firewall 
#systemctl disable firewalld
#systemctl stop firewalld
# Enable NTP services on SMS host
systemctl enable ntpd.service
echo "server ${ntp_server}" >> /etc/ntp.conf
systemctl restart ntpd
# -------------------------------------------------------------
# Add resource management services on master node (Section 3.4)
# -------------------------------------------------------------
yum -y groupinstall ohpc-slurm-server
useradd slurm
# ------------------------------------------------------------
# Add InfiniBand support services on master node (Section 3.5)
# ------------------------------------------------------------
yum -y groupinstall "InfiniBand Support"
yum -y install infinipath-psm
systemctl start rdma
if [[ ${enable_ipoib} -eq 1 ]];then
     # Enable ib0
     cp /opt/ohpc/pub/examples/network/centos/ifcfg-ib0 /etc/sysconfig/network-scripts
     perl -pi -e "s/master_ipoib/${sms_ipoib}/" /etc/sysconfig/network-scripts/ifcfg-ib0
     perl -pi -e "s/ipoib_netmask/${ipoib_netmask}/" /etc/sysconfig/network-scripts/ifcfg-ib0
     ifup ib0
fi
# -----------------------------------------------------------
# Complete basic Warewulf setup for master node (Section 3.6)
# -----------------------------------------------------------
# Cloud HPC does not use Warewulf for provisioning, so we skip warewulf specific steps
#perl -pi -e "s/device = eth1/device = ${sms_eth_internal}/" /etc/warewulf/provision.conf
#perl -pi -e "s/^\s+disable\s+= yes/ disable = no/" /etc/xinetd.d/tftp
#export MODFILE=/etc/httpd/conf.d/warewulf-httpd.conf
#perl -pi -e "s/cgi-bin>\$/cgi-bin>\n Require all granted/" $MODFILE
#perl -pi -e "s/Allow from all/Require all granted/" $MODFILE
#perl -ni -e "print unless /^\s+Order allow,deny/" $MODFILE
#ifconfig ${sms_eth_internal} ${sms_ip} netmask ${internal_netmask} up
#systemctl restart xinetd
#systemctl enable mariadb.service
#systemctl restart mariadb
#systemctl enable httpd.service
#systemctl restart httpd
#if [ ! -z ${BOS_MIRROR+x} ]; then
#     perl -pi -e "s#^YUM_MIRROR=(\S+)#YUM_MIRROR=${BOS_MIRROR}#" /usr/libexec/warewulf/wwmkchroot/centos-7.tmpl
#fi
# -------------------------------------------------
# Create compute image for Warewulf (Section 3.7.1)
# -------------------------------------------------
#export CHROOT=/opt/ohpc/admin/images/centos7.2
#wwmkchroot centos-7 $CHROOT
# -------------------------------------------------------
# Add OpenHPC components to compute image (Section 3.7.2)
# -------------------------------------------------------
#cp -p /etc/resolv.conf $CHROOT/etc/resolv.conf
# Add OpenHPC components to compute instance
#yum -y --installroot=$CHROOT groupinstall ohpc-slurm-client
#yum -y --installroot=$CHROOT groupinstall "InfiniBand Support"
#yum -y --installroot=$CHROOT install infinipath-psm
#chroot $CHROOT systemctl enable rdma
#yum -y --installroot=$CHROOT install ntp
#yum -y --installroot=$CHROOT install kernel
#yum -y --installroot=$CHROOT install lmod-ohpc
# ----------------------------------------------
# Customize system configuration (Section 3.7.3)
# ----------------------------------------------
#wwinit ssh_keys
#cat ~/.ssh/cluster.pub >> $CHROOT/root/.ssh/authorized_keys
#echo "${sms_ip}:/home /home nfs nfsvers=3,rsize=1024,wsize=1024,cto 0 0" >> $CHROOT/etc/fstab
#echo "${sms_ip}:/opt/ohpc/pub /opt/ohpc/pub nfs nfsvers=3 0 0" >> $CHROOT/etc/fstab
perl -pi -e "s/ControlMachine=\S+/ControlMachine=${sms_name}/" /etc/slurm/slurm.conf
echo "/home *(rw,no_subtree_check,fsid=10,no_root_squash)" >> /etc/exports
echo "/opt/ohpc/pub *(ro,no_subtree_check,fsid=11)" >> /etc/exports
exportfs -a
systemctl restart nfs
systemctl enable nfs-server
#chroot $CHROOT systemctl enable ntpd
#echo "server ${sms_ip}" >> $CHROOT/etc/ntp.conf
# Update basic slurm configuration if additional computes defined
if [ ${num_computes} -gt 4 ];then
   perl -pi -e "s/^NodeName=(\S+)/NodeName=${nodename_prefix}[1-${num_computes}]/" /etc/slurm/slurm.conf
   perl -pi -e "s/^PartitionName=normal Nodes=(\S+)/PartitionName=normal Nodes=${nodename_prefix}[1-${num_computes}]/" /etc/slurm/slurm.conf
   #perl -pi -e "s/^NodeName=(\S+)/NodeName=c[1-${num_computes}]/" $CHROOT/etc/slurm/slurm.conf
   #perl -pi -e "s/^PartitionName=normal Nodes=(\S+)/PartitionName=normal Nodes=c[1-${num_computes}]/" $CHROOT/etc/slurm/slurm.conf
fi
# -----------------------------------------
# Additional customizations (Section 3.7.4)
# -----------------------------------------
echo "* soft memlock unlimited" >> /etc/security/limits.conf
echo "* hard memlock unlimited" >> /etc/security/limits.conf
#echo "* soft memlock unlimited" >> $CHROOT/etc/security/limits.conf
#echo "* hard memlock unlimited" >> $CHROOT/etc/security/limits.conf
# Enable slurm pam module
#echo "account    required     pam_slurm.so" >> $CHROOT/etc/pam.d/sshd
# Enable Optional packages
if [[ ${enable_lustre_client} -eq 1 ]];then
     # Install Lustre client on master
     yum -y install lustre-client-ohpc lustre-client-ohpc-modules
     # Enable lustre in WW compute image
     #yum -y --installroot=$CHROOT install lustre-client-ohpc lustre-client-ohpc-modules
     #mkdir $CHROOT/mnt/lustre
     #echo "${mgs_fs_name} /mnt/lustre lustre defaults,_netdev,localflock 0 0" >> $CHROOT/etc/fstab
     # Enable o2ib for Lustre
     echo "options lnet networks=o2ib(ib0)" >> /etc/modprobe.d/lustre.conf
     #echo "options lnet networks=o2ib(ib0)" >> $CHROOT/etc/modprobe.d/lustre.conf
     # mount Lustre client on master
     mkdir /mnt/lustre
     mount -t lustre -o localflock ${mgs_fs_name} /mnt/lustre
fi
if [[ ${enable_nagios} -eq 1 ]];then
     # Install Nagios on master and vnfs image
     yum -y groupinstall ohpc-nagios
     #yum -y --installroot=$CHROOT groupinstall ohpc-nagios
     #chroot $CHROOT systemctl enable nrpe
     #perl -pi -e "s/^allowed_hosts=/# allowed_hosts=/" $CHROOT/etc/nagios/nrpe.cfg
     #echo "nrpe 5666/tcp # NRPE"         >> $CHROOT/etc/services
     #echo "nrpe : ${sms_ip}  : ALLOW"    >> $CHROOT/etc/hosts.allow
     #echo "nrpe : ALL : DENY"            >> $CHROOT/etc/hosts.allow
     #chroot $CHROOT /usr/sbin/useradd -c "NRPE user for the NRPE service" -d /var/run/nrpe -r -g nrpe -s /sbin/nologin nrpe
     #chroot $CHROOT /usr/sbin/groupadd -r nrpe
     mv /etc/nagios/conf.d/services.cfg.example /etc/nagios/conf.d/services.cfg
     mv /etc/nagios/conf.d/hosts.cfg.example /etc/nagios/conf.d/hosts.cfg
     for ((i=0; i<$num_computes; i++)) ; do
        perl -pi -e "s/HOSTNAME$(($i+1))/${c_name[$i]}/ || s/HOST$(($i+1))_IP/${c_ip[$i]}/" \
        /etc/nagios/conf.d/hosts.cfg
     done
     perl -pi -e "s/ \/bin\/mail/ \/usr\/bin\/mailx/g" /etc/nagios/objects/commands.cfg
     perl -pi -e "s/nagios\@localhost/root\@${sms_name}/" /etc/nagios/objects/contacts.cfg
     #echo command[check_ssh]=/usr/lib64/nagios/plugins/check_ssh localhost >> $CHROOT/etc/nagios/nrpe.cfg
     chkconfig nagios on
     systemctl start nagios
     chmod u+s `which ping`
fi
if [[ ${enable_ganglia} -eq 1 ]];then
     # Install Ganglia on master
     yum -y groupinstall ohpc-ganglia
     # Install Ganglia on compute node image
     #yum -y --installroot=$CHROOT install ganglia-gmond-ohpc
     cp /opt/ohpc/pub/examples/ganglia/gmond.conf /etc/ganglia/gmond.conf
     perl -pi -e "s/<sms>/${sms_name}/" /etc/ganglia/gmond.conf
     #cp /etc/ganglia/gmond.conf $CHROOT/etc/ganglia/gmond.conf
     echo "gridname MySite" >> /etc/ganglia/gmetad.conf
     systemctl enable gmond
     systemctl enable gmetad
     systemctl start gmond
     systemctl start gmetad
     #chroot $CHROOT systemctl enable gmond
     systemctl try-restart httpd
fi
if [[ ${enable_clustershell} -eq 1 ]];then
     # Install clustershell
     yum -y install clustershell-ohpc
     cd /etc/clustershell/groups.d
     mv local.cfg local.cfg.orig
     echo "adm: ${sms_name}" > local.cfg
     echo "compute: ${nodename_prefix}[1-${num_computes}]" >> local.cfg
     echo "all: @adm,@compute" >> local.cfg
fi
if [[ ${enable_mrsh} -eq 1 ]];then
     # Install mrsh
     yum -y install mrsh-ohpc mrsh-rsh-compat-ohpc
     #yum -y --installroot=$CHROOT install mrsh-ohpc mrsh-rsh-compat-ohpc mrsh-server-ohpc
     echo "mshell          21212/tcp                  # mrshd" >> /etc/services
     echo "mlogin            541/tcp                  # mrlogind" >> /etc/services
     #chroot $CHROOT systemctl enable xinetd
fi
if [[ ${enable_genders} -eq 1 ]];then
     # Install genders
     yum -y install genders-ohpc
     echo -e "${sms_name}\tsms" > /etc/genders
     for ((i=0; i<$num_computes; i++)) ; do
        echo -e "${c_name[$i]}\tcompute,bmc=${c_bmc[$i]}"
     done >> /etc/genders
fi
# Optionally, enable conman and configure
if [[ ${enable_ipmisol} -eq 1 ]];then
     yum -y install conman-ohpc
     for ((i=0; i<$num_computes; i++)) ; do
        echo -n 'CONSOLE name="'${c_name[$i]}'" dev="ipmi:'${c_bmc[$i]}'" '
        echo 'ipmiopts="'U:${bmc_username},P:${bmc_password},W:solpayloadsize'"'
     done >> /etc/conman.conf
     systemctl enable conman
     systemctl start conman
fi
# --------------------------------------------------------
# Configure rsyslog on SMS and computes (Section 3.7.4.10)
# --------------------------------------------------------
perl -pi -e "s/\\#\\\$ModLoad imudp/\\\$ModLoad imudp/" /etc/rsyslog.conf
perl -pi -e "s/\\#\\\$UDPServerRun 514/\\\$UDPServerRun 514/" /etc/rsyslog.conf
systemctl restart rsyslog
#echo "*.* @${sms_ip}:514" >> $CHROOT/etc/rsyslog.conf
#perl -pi -e "s/^\*\.info/\\#\*\.info/" $CHROOT/etc/rsyslog.conf
#perl -pi -e "s/^authpriv/\\#authpriv/" $CHROOT/etc/rsyslog.conf
#perl -pi -e "s/^mail/\\#mail/" $CHROOT/etc/rsyslog.conf
#perl -pi -e "s/^cron/\\#cron/" $CHROOT/etc/rsyslog.conf
#perl -pi -e "s/^uucp/\\#uucp/" $CHROOT/etc/rsyslog.conf
# ----------------------------
# Import files (Section 3.7.5)
# ----------------------------
#wwsh file import /etc/passwd
#wwsh file import /etc/group
#wwsh file import /etc/shadow 
#wwsh file import /etc/slurm/slurm.conf
#wwsh file import /etc/munge/munge.key
#if [[ ${enable_ipoib} -eq 1 ]];then
#     wwsh file import /opt/ohpc/pub/examples/network/centos/ifcfg-ib0.ww
#     wwsh -y file set ifcfg-ib0.ww --path=/etc/sysconfig/network-scripts/ifcfg-ib0
#fi
# --------------------------------------
# Assemble bootstrap image (Section 3.8)
# --------------------------------------
#export WW_CONF=/etc/warewulf/bootstrap.conf
#echo "drivers += updates/kernel/" >> $WW_CONF
#wwbootstrap `uname -r`
# Assemble VNFS
#wwvnfs -y --chroot $CHROOT
# Add hosts to cluster
#echo "GATEWAYDEV=${eth_provision}" > /tmp/network.$$
#wwsh -y file import /tmp/network.$$ --name network
#wwsh -y file set network --path /etc/sysconfig/network --mode=0644 --uid=0
#for ((i=0; i<$num_computes; i++)) ; do
#   wwsh -y node new ${c_name[i]} --ipaddr=${c_ip[i]} --hwaddr=${c_mac[i]} -D ${eth_provision}
#done
# Add hosts to cluster (Cont.)
#wwsh -y provision set "${compute_regex}" --vnfs=centos7.2 --bootstrap=`uname -r` --files=dynamic_hosts,passwd,group,shadow,slurm.conf,munge.key,network
# Optionally, add arguments to bootstrap kernel
#if [[ ${enable_kargs} ]]; then
#   wwsh provision set "${compute_regex}" --kargs=${kargs}
#fi
# Restart ganglia services to pick up hostfile changes
if [[ ${enable_ganglia} -eq 1 ]];then
  systemctl restart gmond
  systemctl restart gmetad
fi
# Optionally, define IPoIB network settings (required if planning to mount Lustre over IB)
#if [[ ${enable_ipoib} -eq 1 ]];then
#     for ((i=0; i<$num_computes; i++)) ; do
#        wwsh -y node set ${c_name[$i]} -D ib0 --ipaddr=${c_ipoib[$i]} --netmask=${ipoib_netmask}
#     done
#     wwsh -y provision set "${compute_regex}" --fileadd=ifcfg-ib0.ww
#fi
#systemctl restart dhcpd
#wwsh pxe update
# Optionally, enable console redirection 
#if [[ ${enable_ipmisol} -eq 1 ]];then
#     wwsh -y provision set "${compute_regex}" --kargs "${kargs} console=ttyS1,115200"
#fi
# --------------------------------
# Boot compute nodes (Section 3.9)
# --------------------------------
#for ((i=0; i<${num_computes}; i++)) ; do
#   ipmitool -E -I lanplus -H ${c_bmc[$i]} -U ${bmc_username} chassis power reset
#done
# ---------------------------------------
# Install Development Tools (Section 4.1)
# ---------------------------------------
yum -y groupinstall ohpc-autotools
yum -y install valgrind-ohpc
yum -y install EasyBuild-ohpc
yum -y install spack-ohpc
yum -y install R_base-ohpc            
# -------------------------------
# Install Compilers (Section 4.2)
# -------------------------------
yum -y install gnu-compilers-ohpc
# --------------------------------
# Install MPI Stacks (Section 4.3)
# --------------------------------
yum -y install openmpi-gnu-ohpc mvapich2-gnu-ohpc
# ---------------------------------------
# Install Performance Tools (Section 4.4)
# ---------------------------------------
yum -y groupinstall ohpc-perf-tools-gnu
yum -y install lmod-defaults-gnu-mvapich2-ohpc
# ---------------------------------------------------
# Install 3rd Party Libraries and Tools (Section 4.6)
# ---------------------------------------------------
yum -y groupinstall ohpc-serial-libs-gnu
yum -y groupinstall ohpc-parallel-libs-gnu
yum -y groupinstall ohpc-io-libs-gnu
yum -y groupinstall ohpc-python-libs-gnu
yum -y groupinstall ohpc-runtimes-gnu
# -----------------------------------------------------------------------------------
# Install Optional Development Tools for use with Intel Parallel Studio (Section 4.7)
# -----------------------------------------------------------------------------------
if [[ ${enable_intel_packages} -eq 1 ]];then
     yum -y install intel-compilers-devel-ohpc
     yum -y install intel-mpi-devel-ohpc
     yum -y groupinstall ohpc-serial-libs-intel
     yum -y groupinstall ohpc-parallel-libs-intel
     yum -y groupinstall ohpc-io-libs-intel
     yum -y groupinstall ohpc-perf-tools-intel
     yum -y groupinstall ohpc-python-libs-intel
     yum -y groupinstall ohpc-runtimes-intel
fi
# -------------------------------------------------------------
# Allow for optional sleep to wait for provisioning to complete
# -------------------------------------------------------------
#sleep ${provision_wait}
# ------------------------------------
# Resource Manager Startup (Section 5)
# ------------------------------------
systemctl enable munge
systemctl enable slurmctld
systemctl start munge
systemctl start slurmctld
echo "==========================================================="
echo "<< Finished installing OpenHPC on SMS node: ${sms_name} >>"
echo "==========================================================="
#pdsh -P normal systemctl start slurmd
#useradd -m test
#wwsh file resync passwd shadow group
#pdsh -P normal /warewulf/bin/wwgetfiles 
#  PFILEP
## FILE: hpc_cent7/ohpc/ohpc.conf
# -*-sh-*-
# ------------------------------------------------------------------------------------------------
# ------------------------------------------------------------------------------------------------
# Template input file to define local variable settings for use with
# an installation recipe.
# ------------------------------------------------------------------------------------------------
# ---------------------------
# SMS (master) node settings
# ---------------------------
# OpenHPC package to be installed
ohpc_pkg="https://github.com/openhpc/ohpc/releases/download/v1.1.GA/ohpc-release-centos7.2-1.1-1.x86_64.rpm"
# Hostname for master server (SMS)
sms_name="${sms_name:-head-4}"
                              
# Local (internal) IP address on SMS
sms_ip="${sms_ip:-192.168.14.1}"
# Internal ethernet interface on SMS
sms_eth_internal="${sms_eth_internal:-enp6s0f0}"
# Subnet netmask for internal cluster network
internal_netmask="${internal_netmask:-255.255.255.0}"
# Provisioning interface used by compute hosts
eth_provision="${eth_provision:-enp6s0f0}"
# Local ntp server for time synchronization
ntp_server="${ntp_server:-0.centos.pool.ntp.org}"
# BMC user credentials for use by IPMI
bmc_username="${bmc_username:root}"
bmc_password="${bmc_password:rootmenow12!}"
# Additional time to wait for compute nodes to provision (seconds)
provision_wait="${provision_wait:-180}"
# Stateful install device
stateful_dev="${stateful_dev:-sda}"
# Flags for optional installation/configuration
enable_clustershell="${enable_clustershell:-1}"
enable_ipmisol="${enable_ipmisol:-1}"
enable_ipoib="${enable_ipoib:-0}"
enable_ganglia="${enable_ganglia:-1}"
enable_kargs="${enable_kargs:-1}"
enable_lustre_client="${enable_lustre_client:-1}"
enable_mrsh="${enable_mrsh:-1}"
enable_nagios="${enable_nagios:-1}"
enable_powerman="${enable_powerman:-1}"
enable_stateful="${enable_stateful:-0}"
# -------------------------
# compute node settings, are in independent files
# -------------------------
# Prefix for compute node hostnames
nodename_prefix="${nodename_prefix:-c}"
## compute node IP addresses
c_ip[0]=192.168.14.3
#c_ip[1]=
#c_ip[2]=
#c_ip[3]=
#
## compute node MAC addreses for provisioning interface
c_mac[0]=00:1e:67:fe:93:d7
#c_mac[1]=
#c_mac[2]=
#c_mac[3]=
#
## compute node BMC addresses
c_bmc[0]=192.168.1.105
#c_bmc[1]=
#c_bmc[2]=
#c_bmc[3]=
#
#-------------------
# Optional settings
#-------------------
# additional arguments to enable optional arguments for bootstrap kernel
kargs="${kargs:-acpi_pad.disable=1}"
# Lustre MGS mount name
mgs_fs_name="${mgs_fs_name:-192.168.1.4@o2ib:/lustre1}"
# Subnet netmask for IPoIB network
ipoib_netmask="${ipoib_netmask:-255.255.255.0}"
# IPoIB address for SMS server
sms_ipoib="${sms_ipoib:-192.168.1.4}"
# IPoIB addresses for computes
#c_ipoib[0]=
#c_ipoib[1]=
#c_ipoib[2]=
#c_ipoib[3]=
#  PFILEP
## FILE: hpc_cent7/ohpc/input.local
# -*-sh-*-
# ------------------------------------------------------------------------------------------------
# ------------------------------------------------------------------------------------------------
# Template input file to define local variable settings for use with
# an OpenHPC installation recipe.
# ------------------------------------------------------------------------------------------------
# ---------------------------
# SMS (master) node settings
# ---------------------------
# Hostname for master server (SMS)
sms_name="${sms_name:-sms}"
                              
# Local (internal) IP address on SMS
sms_ip="${sms_ip:-172.16.0.1}"
# Internal ethernet interface on SMS
sms_eth_internal="${sms_eth_internal:-eth1}"
# Subnet netmask for internal cluster network
internal_netmask="${internal_netmask:-255.255.0.0}"
# Provisioning interface used by compute hosts
eth_provision="${eth_provision:-eth0}"
# Local ntp server for time synchronization
ntp_server="${ntp_server:-0.centos.pool.ntp.org}"
# BMC user credentials for use by IPMI
bmc_username="${bmc_username:-unknown}"
bmc_password="${bmc_password:-unknown}"
# Additional time to wait for compute nodes to provision (seconds)
provision_wait="${provision_wait:-180}"
# Flags for optional installation/configuration
enable_clustershell="${enable_clustershell:-0}"
enable_ipmisol="${enable_ipmisol:-0}"
enable_ipoib="${enable_ipoib:-0}"
enable_ganglia="${enable_ganglia:-0}"
enable_genders="${enable_genders:-0}"
enable_kargs="${enable_kargs:-0}"
enable_lustre_client="${enable_lustre_client:-0}"
enable_mrsh="${enable_mrsh:-0}"
enable_nagios="${enable_nagios:-0}"
enable_powerman="${enable_powerman:-0}"
enable_intel_packages="${enable_intel_packages:-0}"
# -------------------------
# compute node settings
# -------------------------
# total number of computes
num_computes="${num_computes:-4}"
# regex that matches defined compute hostnames
compute_regex="${compute_regex:-c*}"
# compute hostnames
c_name[0]=c1
c_name[1]=c2
c_name[2]=c3
c_name[3]=c4
# compute node IP addresses
c_ip[0]=172.16.1.1
c_ip[1]=172.16.1.2
c_ip[2]=172.16.1.3
c_ip[3]=172.16.1.4
# compute node MAC addreses for provisioning interface
c_mac[0]=00:1a:2b:3c:4f:56
c_mac[1]=00:1a:2b:3c:4f:56
c_mac[2]=00:1a:2b:3c:4f:56
c_mac[3]=00:1a:2b:3c:4f:56
# compute node BMC addresses
c_bmc[0]=10.16.1.1
c_bmc[1]=10.16.1.2
c_bmc[2]=10.16.1.3
c_bmc[3]=10.16.1.4
#-------------------
# Optional settings
#-------------------
# additional arguments to enable optional arguments for bootstrap kernel
kargs="${kargs:-acpi_pad.disable=1}"
# Lustre MGS mount name
mgs_fs_name="${mgs_fs_name:-192.168.100.254@o2ib:/lustre1}"
# Subnet netmask for IPoIB network
ipoib_netmask="${ipoib_netmask:-255.255.0.0}"
# IPoIB address for SMS server
sms_ipoib="${sms_ipoib:-192.168.0.1}"
# IPoIB addresses for computes
c_ipoib[0]=192.168.1.1		            
c_ipoib[1]=192.168.1.2
c_ipoib[2]=192.168.1.3
c_ipoib[3]=192.168.1.4
#  PFILEP
## FILE: hpc_cent7/ohpc/ohpc_sun_hn2.conf
# -*-sh-*-
# ------------------------------------------------------------------------------------------------
# ------------------------------------------------------------------------------------------------
# Template input file to define local variable settings for use with
# an installation recipe.
# ------------------------------------------------------------------------------------------------
# ---------------------------
# SMS (master) node settings
# ---------------------------
# OpenHPC package to be installed
ohpc_pkg="https://github.com/openhpc/ohpc/releases/download/v1.1.GA/ohpc-release-centos7.2-1.1-1.x86_64.rpm"
# Hostname for master server (SMS)
sms_name="${sms_name:-sun-hn2}"
                              
# Local (internal) IP address on SMS
sms_ip="${sms_ip:-192.168.46.12}"
# Internal ethernet interface on SMS
sms_eth_internal="${sms_eth_internal:-enp6s0f1}"
# Subnet netmask for internal cluster network
internal_netmask="${internal_netmask:-255.255.255.0}"
# Provisioning interface used by compute hosts
eth_provision="${eth_provision:-enp6s0f1}"
# Local ntp server for time synchronization
ntp_server="${ntp_server:-0.centos.pool.ntp.org}"
# BMC user credentials for use by IPMI
bmc_username="${bmc_username:root}"
bmc_password="${bmc_password:ppk123}"
# Additional time to wait for compute nodes to provision (seconds)
provision_wait="${provision_wait:-180}"
# Stateful install device
stateful_dev="${stateful_dev:-sda}"
# Flags for optional installation/configuration
enable_clustershell="${enable_clustershell:-0}"
enable_ipmisol="${enable_ipmisol:-0}"
enable_ipoib="${enable_ipoib:-0}"
enable_ganglia="${enable_ganglia:-0}"
enable_kargs="${enable_kargs:-0}"
enable_lustre_client="${enable_lustre_client:-0}"
enable_mrsh="${enable_mrsh:-0}"
enable_nagios="${enable_nagios:-0}"
enable_powerman="${enable_powerman:-0}"
enable_stateful="${enable_stateful:-0}"
# -------------------------
# compute node settings, are in independent files
# -------------------------
# Prefix for compute node hostnames
nodename_prefix="${nodename_prefix:-c}"
## compute node IP addresses
c_ip[0]=192.168.46.121
#c_ip[1]=192.168.46.132
#c_ip[2]=192.168.46.133
#c_ip[3]=192.168.46.134
#
## compute node MAC addreses for provisioning interface
c_mac[0]=00:1e:67:43:89:48
#c_mac[1]=a4:bf:01:0d:0f:af
#c_mac[2]=a4:bf:01:0c:dd:64
#c_mac[3]=a4:bf:01:0c:e1:42
#
## compute node BMC addresses
c_bmc[0]=192.168.46.54
#c_bmc[1]=192.168.46.56
#c_bmc[2]=192.168.46.57
#c_bmc[3]=192.168.46.58
#
#-------------------
# Optional settings
#-------------------
# additional arguments to enable optional arguments for bootstrap kernel
kargs="${kargs:-}"
# Lustre MGS mount name
mgs_fs_name="${mgs_fs_name:-192.168.46.12@o2ib:/lustre1}"
# Subnet netmask for IPoIB network
ipoib_netmask="${ipoib_netmask:-255.255.255.0}"
# IPoIB address for SMS server
sms_ipoib="${sms_ipoib:-192.168.46.12}"
# IPoIB addresses for computes
#c_ipoib[0]=
#c_ipoib[1]=
#c_ipoib[2]=
#c_ipoib[3]=
#  PFILEP
## FILE: hpc_cent7/ohpc/ohpc_sun_hn3.conf
# -*-sh-*-
# ------------------------------------------------------------------------------------------------
# ------------------------------------------------------------------------------------------------
# Template input file to define local variable settings for use with
# an installation recipe.
# ------------------------------------------------------------------------------------------------
# ---------------------------
# SMS (master) node settings
# ---------------------------
# OpenHPC package to be installed
ohpc_pkg="https://github.com/openhpc/ohpc/releases/download/v1.1.GA/ohpc-release-centos7.2-1.1-1.x86_64.rpm"
# Hostname for master server (SMS)
sms_name="${sms_name:-sun-hn3}"
                              
# Local (internal) IP address on SMS
sms_ip="${sms_ip:-192.168.46.13}"
# Internal ethernet interface on SMS
sms_eth_internal="${sms_eth_internal:-enp4s0f3}"
# Subnet netmask for internal cluster network
internal_netmask="${internal_netmask:-255.255.255.0}"
# Provisioning interface used by compute hosts
eth_provision="${eth_provision:-enp4s0f3}"
# Local ntp server for time synchronization
ntp_server="${ntp_server:-0.centos.pool.ntp.org}"
# BMC user credentials for use by IPMI
bmc_username="${bmc_username:root}"
bmc_password="${bmc_password:root}"
# Additional time to wait for compute nodes to provision (seconds)
provision_wait="${provision_wait:-180}"
# Stateful install device
stateful_dev="${stateful_dev:-sda}"
# Flags for optional installation/configuration
enable_clustershell="${enable_clustershell:-0}"
enable_ipmisol="${enable_ipmisol:-0}"
enable_ipoib="${enable_ipoib:-0}"
enable_ganglia="${enable_ganglia:-0}"
enable_kargs="${enable_kargs:-0}"
enable_lustre_client="${enable_lustre_client:-0}"
enable_mrsh="${enable_mrsh:-0}"
enable_nagios="${enable_nagios:-0}"
enable_powerman="${enable_powerman:-0}"
enable_stateful="${enable_stateful:-0}"
# -------------------------
# compute node settings, are in independent files
# -------------------------
# Prefix for compute node hostnames
nodename_prefix="${nodename_prefix:-c}"
## compute node IP addresses
c_ip[0]=192.168.46.131
c_ip[1]=192.168.46.132
c_ip[2]=192.168.46.133
c_ip[3]=192.168.46.134
#
## compute node MAC addreses for provisioning interface
c_mac[0]=a4:bf:01:0d:11:94
c_mac[1]=a4:bf:01:0d:0f:af
c_mac[2]=a4:bf:01:0c:dd:64
c_mac[3]=a4:bf:01:0c:e1:42
#
## compute node BMC addresses
c_bmc[0]=192.168.46.55
c_bmc[1]=192.168.46.56
c_bmc[2]=192.168.46.57
c_bmc[3]=192.168.46.58
#
#-------------------
# Optional settings
#-------------------
# additional arguments to enable optional arguments for bootstrap kernel
kargs="${kargs:-acpi_pad.disable=1}"
# Lustre MGS mount name
mgs_fs_name="${mgs_fs_name:-192.168.46.13@o2ib:/lustre1}"
# Subnet netmask for IPoIB network
ipoib_netmask="${ipoib_netmask:-255.255.255.0}"
# IPoIB address for SMS server
sms_ipoib="${sms_ipoib:-192.168.46.13}"
# IPoIB addresses for computes
#c_ipoib[0]=
#c_ipoib[1]=
#c_ipoib[2]=
#c_ipoib[3]=
#  PFILEP
## FILE: teardown_cloud_nodes.sh
#!/bin/bash
#This script is designed to be used on our internal sun-hn1 node to clean up all of our configured OpenStack
#configurations for a clean OpenStack configuration without completely uninstalling and reinstalling the
#entire OpenStack software stack.
#Source the keystone file so we have secure access to the OpenStack commands
source ${HOME}/keystonerc_admin
#First stop the compute nodes. This is done to cleanly delete the nodes. If you skip the stop step, the
#nova delete command will sometimes result in an error. This is found to be the safest way to delete
#nova nodes.
nova stop cc1
nova stop cc2
nova stop cc3
#Wait for the nova nodes to stop bing in status ACTIVE (i.e. they are now in SHUTOFF)
nova list | awk {'print $6'} | grep -v 'Status' | grep ACTIVE > /dev/null
nova_stopped=$?
until [ "${nova_stopped}" -eq "1" ]; do
    sleep 5
    nova list | awk {'print $6'} | grep -v 'Status' | grep ACTIVE > /dev/null
    nova_stopped=$?
done
#Once all the nodes are shutdown, they can safely be deleted from nova.
nova delete cc1
nova delete cc2
nova delete cc3
#Now that there are no booted nodes and association of a compute node with the ironic nodes,
#the ironic nodes can safely be deleted.
ironic node-delete cc1
ironic node-delete cc2
ironic node-delete cc3
#Once the ironic nodes are deleted, we can delete the associated neutron port that was associated with each
#of the nodes.
neutron port-delete cc1
neutron port-delete cc2
neutron port-delete cc3
#Now we can delete the shared network that was configured with neutron
neutron net-delete sharednet1
#Remove the nova flavor 'baremetal-flavor' association we created with the machine's hardware
nova flavor-delete baremetal-flavor
#Remove every image locally saved in glance
for x in `glance image-list | awk {'print $2'} | grep -v ID`; do
    glance image-delete $x
done
#Finally, remove the keypair association we have in nova. This will leave the system clean and ready for another run
nova keypair-delete ostack_key
#  PFILEP
## FILE: get_cn_mac
#!/bin/bash
# check if ipmitool is installed, if not then quit
#check if BMC_IP, BMC_uname and BMC Password is provided, if not then exi
function get_bmc_mac {
    bmc_ip=$1
    bmc_user=$2
    bmc_pass=$3
    if [[ -z $bmc_pass ]]; then
       ipmi_mac=`ipmitool -E -I lanplus -H $bmc_ip -U $bmc_user lan print 1 |grep "MAC Address"|awk '{print $4}'`
    else
       ipmi_mac=`ipmitool -E -I lanplus -H $bmc_ip -U $bmc_user -P $bmc_pass lan print 1 |grep "MAC Address"|awk '{print $4}'`
    fi
}
function get_ipmi_mac_parts {
    ipmi_mac=$1
    ipmi_mac_constant_part=${ipmi_mac%:*}
    ipmi_mac_last_octet=${ipmi_mac##*:}
}
function get_comput_mac_octets {
    ipmi_mac_l=$1
    hex_ipmi_mac="0x$ipmi_mac_l"
    mac1_last_octet=$(($hex_ipmi_mac - 2))
    mac2_last_octet=$(($hex_ipmi_mac - 1))
    mac1_last_octet=`echo "obase=16; $mac1_last_octet"|bc`
    mac2_last_octet=`echo "obase=16; $mac2_last_octet"|bc`
}
function get_compute_mac {
    mac1="$ipmi_mac_constant_part:$mac1_last_octet"
    mac2="$ipmi_mac_constant_part:$mac2_last_octet"
}
usage () {
  echo "USAGE: $0 <bmc_ip> <bmc_user> [bmc_password]"
}
### Main ##
# check of help is requested
for i in "$@"; do
  case $i in
    -h|--help)
      usage
      exit 1
    ;;
  esac
done
# check if we have at least 3 arguments
if [[ $# -lt 2 ]]; then
    echo "Error: Insufficient Arguments"
    usage
    exit
fi
bmc_ip=$1
bmc_user=$2
bmc_pass=$3
get_bmc_mac $bmc_ip $bmc_user $bmc_pass 
# check if we got the virtual MAC
if [[ -z $ipmi_mac ]]; then
    echo "Error: BMC Communication Error"
    exit
fi
get_ipmi_mac_parts $ipmi_mac
get_comput_mac_octets $ipmi_mac_last_octet
get_compute_mac
echo "Compute MAC1: $mac1"
echo "Compute MAC2: $mac2"
#  PFILEP
## FILE: cloud_hpc_init/orch/chpc_init
#!/bin/bash
#
#Ensure the executing shell is in the same directory as the script.
SCRIPTDIR="$( cd "$( dirname "$( readlink -f "${BASH_SOURCE[0]}" )" )" && pwd -P && echo x)"
SCRIPTDIR="${SCRIPTDIR%x}"
cd $SCRIPTDIR
chpcInitPath=/opt/intel/hpc-orchestrator/admin/cloud_hpc_init
logger "chpcInit: Updating Compute Node with HPC configuration"
# Update rsyslog
cat /etc/rsyslog.conf | grep "<sms_ip>:514"
rsyslog_set=$?
if [ "${rsyslog_set}" -ne "0" ]; then
    echo "*.* @<sms_ip>:514" >> /etc/rsyslog.conf
fi
systemctl restart rsyslog
logger "chpcInit: rsyslog configuration complete, updating remaining HPC configuration"
# nfs mount directory from SMS head node to Compute Node
cat /etc/fstab | grep "<sms_ip>:/home"
home_exists=$?
if [ "${home_exists}" -ne "0" ]; then
    echo "<sms_ip>:/home /home nfs nfsvers=3,rsize=1024,wsize=1024,cto 0 0" >> /etc/fstab
fi
cat /etc/fstab | grep "<sms_ip>:/opt/intel/hpc-orchestrator/pub"
orchestrator_pub_exists=$?
if [ "${orchestrator_pub_exists}" -ne "0" ]; then
    echo "<sms_ip>:/opt/intel/hpc-orchestrator/pub /opt/intel/hpc-orchestrator/pub nfs nfsvers=3 0 0" >> /etc/fstab
fi
mount /home
mount /opt/intel/hpc-orchestrator/pub
# enable test suite
cat /etc/fstab | grep "<sms_ip>:/opt/intel/hpc-orchestrator/pub/tests"
orchestrator_tests_exist=$?
if [ "${orchestrator_tests_exist}" -ne "0" ]; then
    echo -n "<sms_ip>:/opt/intel/hpc-orchestrator/pub/tests " >> /etc/fstab
    echo "/opt/intel/hpc-orchestrator/pub/tests nfs nfsvers=3 0 0" >> /etc/fstab
fi
#mount 
mkdir -p /opt/intel/hpc-orchestrator/pub/tests
mount /opt/intel/hpc-orchestrator/pub/tests
# mount cloud_hpc_init
cat /etc/fstab | grep "<sms_ip>:$chpcInitPath"
CloudHPCInit_exist=$?
if [ "${CloudHPCInit_exist}" -ne "0" ]; then
    echo "<sms_ip>:$chpcInitPath $chpcInitPath nfs nfsvers=3 0 0" >> /etc/fstab
fi 
mkdir -p $chpcInitPath
mount $chpcInitPath
# Restart nfs
systemctl restart nfs
# Restart ntp at CN
systemctl enable ntpd
# Update ntp server
cat /etc/ntp.conf | grep "server <sms_ip>"
ntp_server_exists=$?
if [ "${ntp_server_exists}" -ne "0" ]; then
    echo "server <sms_ip>" >> /etc/ntp.conf
fi
systemctl restart ntpd
# time sync
ntpstat
# Sync following files to compute node
# Assuming nfs is setup properly
if [ -d $chpcInitPath ]; then
    # Update the slurm file
    cp -f -L $chpcInitPath/slurm.conf /etc/slurm/slurm.conf
    # Sync head node configuration with Compute Node
    cp -f -L $chpcInitPath/passwd /etc/passwd
    cp -f -L $chpcInitPath/group /etc/group
    cp -f -L $chpcInitPath/shadow /etc/shadow 
    cp -f -L $chpcInitPath/slurm.conf /etc/slurm/slurm.conf
    cp -f -L $chpcInitPath/slurm /etc/pam.d/slurm
    cp -f -L $chpcInitPath/munge.key /etc/munge/munge.key
    # For hostname resolution
    cp -f -L $chpcInitPath/hosts /etc/hosts
    # make sure that hostname mentioned into /etc/hosts matches machine hostname. TBD
    # Start slurm and munge 
    systemctl enable munge
    systemctl restart munge
    systemctl enable slurmd
    systemctl restart slurmd
else
    logger "chpcInit:ERROR: cannot stat nfs shared /opt directory, cannot copy HPC system files"
fi
# Setup hostname as per the head node
#Find the hostname of this machine from the copied over /etc/hosts file
cc_ipaddrs=(`hostname -I`)
for cc_ipaddr in ${cc_ipaddrs[@]}; do
    cat /etc/hosts | grep ${cc_ipaddr} > /dev/null
    result=$?
    if [ "$result" -eq "0" ]; then
        cc_hostname=`cat /etc/hosts | grep ${cc_ipaddr} | cut -d$'\t' -f2`
        break
    fi
done
if [ -z "${cc_hostname}" ]; then
    logger "chpcInit:ERROR: No resolved hostname found for any IP address in /etc/hosts"
    exit 1
fi
#set the hostname
if [ $(hostname) != ${cc_hostname} ]; then
    hostnamectl set-hostname ${cc_hostname}
fi
# Start slurm and munge 
systemctl enable munge
systemctl restart munge
systemctl enable slurmd
systemctl restart slurmd
#Change file permissions in /etc/ssh to fix ssh into compute node
chmod 0600 /etc/ssh/ssh_host_*_key
#  PFILEP
## FILE: cloud_hpc_init/ohpc/chpc_init
#!/bin/bash
#
#Ensure the executing shell is in the same directory as the script.
SCRIPTDIR="$( cd "$( dirname "$( readlink -f "${BASH_SOURCE[0]}" )" )" && pwd -P && echo x)"
SCRIPTDIR="${SCRIPTDIR%x}"
cd $SCRIPTDIR
chpcInitPath=/opt/ohpc/admin/cloud_hpc_init
logger "chpcInit: Updating Compute Node with HPC configuration"
# Update rsyslog
cat /etc/rsyslog.conf | grep "<sms_ip>:514"
rsyslog_set=$?
if [ "${rsyslog_set}" -ne "0" ]; then
    echo "*.* @<sms_ip>:514" >> /etc/rsyslog.conf
fi
systemctl restart rsyslog
logger "chpcInit: rsyslog configuration complete, updating remaining HPC configuration"
# nfs mount directory from SMS head node to Compute Node
cat /etc/fstab | grep "<sms_ip>:/home"
home_exists=$?
if [ "${home_exists}" -ne "0" ]; then
    echo "<sms_ip>:/home /home nfs nfsvers=3,rsize=1024,wsize=1024,cto 0 0" >> /etc/fstab
fi
cat /etc/fstab | grep "<sms_ip>:/opt/ohpc/pub"
ohpc_pub_exists=$?
if [ "${ohpc_pub_exists}" -ne "0" ]; then
    echo "<sms_ip>:/opt/ohpc/pub /opt/ohpc/pub nfs nfsvers=3 0 0" >> /etc/fstab
    # Make sure we have directory to mount
    # Clean up if required
    if [ -e /opt/ohpc/pub ]; then
        echo "chpcInit: [WARNING] /opt/ohpc/pub already exists!!"
    fi
fi
mkdir -p /opt/ohpc/pub
mount /home
mount /opt/ohpc/pub
# mount cloud_hpc_init
cat /etc/fstab | grep "sms_ip:$chpcInitPath"
CloudHPCInit_exist=$?
if [ "${CloudHPCInit_exist}" -ne "0" ]; then
    echo "<sms_ip>:$chpcInitPath $chpcInitPath nfs nfsvers=3 0 0" >> /etc/fstab
fi
mkdir -p $chpcInitPath
mount $chpcInitPath
# Restart nfs
systemctl restart nfs
# Restart ntp at CN
systemctl enable ntpd
# Update ntp server
cat /etc/ntp.conf | grep "server <sms_ip>"
ntp_server_exists=$?
if [ "${ntp_server_exists}" -ne "0" ]; then
    echo "server <sms_ip>" >> /etc/ntp.conf
fi
systemctl restart ntpd
# time sync
ntpstat
# Sync following files to compute node
# Assuming nfs is setup properly
if [ -d $chpcInitPath ]; then
    # Update the slurm file
    cp -f -L $chpcInitPath/slurm.conf /etc/slurm/slurm.conf
    # Sync head node configuration with Compute Node
    #cp -f -L $chpcInitPath/passwd /etc/passwd
    #cp -f -L $chpcInitPath/group /etc/group
    #cp -f -L $chpcInitPath/shadow /etc/shadow 
    # Copy public keys
    cp -f -L $chpcInitPath/authorized_keys /root/.ssh/
    cp -f -L $chpcInitPath/slurm.conf /etc/slurm/slurm.conf
    cp -f -L $chpcInitPath/slurm /etc/pam.d/slurm
    cp -f -L $chpcInitPath/munge.key /etc/munge/munge.key
    # For hostname resolution
    cp -f -L $chpcInitPath/hosts /etc/hosts
    # make sure that hostname mentioned into /etc/hosts matches machine hostname. TBD
    # Start slurm and munge 
    systemctl enable munge
    systemctl restart munge
    systemctl enable slurmd
    systemctl restart slurmd
else
    logger "chpcInit:ERROR: cannot stat nfs shared /opt directory, cannot copy HPC system files"
fi
# Setup hostname as per the head node
#Find the hostname of this machine from the copied over /etc/hosts file
cc_ipaddrs=(`hostname -I`)
for cc_ipaddr in ${cc_ipaddrs[@]}; do
    cat /etc/hosts | grep ${cc_ipaddr} > /dev/null
    result=$?
    if [ "$result" -eq "0" ]; then
        cc_hostname=`cat /etc/hosts | grep ${cc_ipaddr} | cut -d$'\t' -f2`
        break
    fi
done
if [ -z "${cc_hostname}" ]; then
    logger "chpcInit:ERROR: No resolved hostname found for any IP address in /etc/hosts"
    exit 1
fi
#set the hostname
if [ $(hostname) != ${cc_hostname} ]; then
    hostnamectl set-hostname ${cc_hostname}
fi
# Start slurm and munge 
systemctl enable munge
systemctl restart munge
systemctl enable slurmd
systemctl restart slurmd
#Change file permissions in /etc/ssh to fix ssh into compute node
chmod 0600 /etc/ssh/ssh_host_*_key
#  PFILEP
## FILE: cloud_hpc_init/ohpc/chpc_sms_init
#!/bin/bash
#
logger "chpcInit: Entered chpcInit"
#Ensure the executing shell is in the same directory as the script.
SCRIPTDIR="$( cd "$( dirname "$( readlink -f "${BASH_SOURCE[0]}" )" )" && pwd -P && echo x)"
SCRIPTDIR="${SCRIPTDIR%x}"
cd $SCRIPTDIR
# Get the Compute node prefix and number of compute nodes
cnodename_prefix=<update_cnodename_prefix>
num_ccomputes=<update_num_ccomputes>
ntp_server=<update_ntp_server>
sms_name=<update_sms_name>
# setup cloudinit directory
chpcInitPath=/opt/ohpc/admin/cloud_hpc_init
# create directory of not exists
mkdir -p $chpcInitPath
chmod 700 $chpcInitPath
# Copy other files needed for Cloud Init
#sudo cp -fpr /etc/passwd $chpcInitPath
#sudo cp -fpr /etc/shadow $chpcInitPath
#sudo cp -fpr /etc/group $chpcInitPath
#TBD: This is a workaround for now, what we want is nodes to communicate to other nodes and sms node. so need to update cn entries here. might want to generate a script which is executed on compute node, and that updates entries into /etc/hosts of compute node. This workaround will break other functionalities in Cloudburst scenario
#sudo cp -fpr /etc/hosts $chpcInitPath
# Copy public ssh key to shared drive
_ssh_path=/root/.ssh
if [ ! -e "$_ssh_path/hpcasservice" ]; then
    if [ ! -d "$_ssh_path" ]; then
        install -d -m 700 $_ssh_path
    fi
    ssh-keygen -t dsa -f $_ssh_path/hpcasservice -N '' -C "HPC Cluster key" > /dev/null 2>&1
    cat $_ssh_path/hpcasservice.pub >> $_ssh_path/authorized_keys
    chmod 0600 $_ssh_path/authorized_keys
fi
#update config
if [ ! -e "$_ssh_path/config" ]; then
    echo "Host *" > $_ssh_path/config
    echo "    IdentityFile ~/.ssh/hpcasservice" >> $_ssh_path/config
    echo "    StrictHostKeyChecking=no" >> $_ssh_path/config
fi
cp -fpr $_ssh_path/authorized_keys $chpcInitPath
# export CloudInit Path to nfs share
cat /etc/exports | grep "$chpcInitPath"
chpcInitPath_exported=$?
if [ "${chpcInitPath_exported}" -ne "0" ]; then
    echo "$chpcInitPath *(rw,no_subtree_check,no_root_squash)" >> /etc/exports
fi
# share /home from HN
if ! grep "^/home" /etc/exports; then
    echo "/home *(rw,no_subtree_check,fsid=10,no_root_squash)" >> /etc/exports
fi
# share /opt/ from HN
if ! grep "^/opt/ohpc/pub" /etc/exports; then
    echo "/opt/ohpc/pub *(ro,no_subtree_check,fsid=11)" >> /etc/exports
fi
exportfs -a
# Restart nfs
systemctl restart nfs
systemctl enable nfs-server
logger "chpcInit: nfs configuration complete, updating remaining HPC configuration"
#cat /etc/rsyslog.conf | grep "<sms_ip>:514"
#fi
#systemctl restart rsyslog
#logger "chpcInit: rsyslog configuration complete, updating remaining HPC configuration"
# configure NTP
systemctl enable ntpd
if [[ ! -z "$ntp_server" ]]; then
   echo "server ${ntp_server}" >> /etc/ntp.conf
fi
systemctl restart ntpd
systemctl enable ntpd.service
# time sync
ntpstat
logger "chpcInit:ntp configuration done"
### Update Resource manager configuration ###
# Update basic slurm configuration at sms node
perl -pi -e "s/ControlMachine=\S+/ControlMachine=${sms_name}/" /etc/slurm/slurm.conf
perl -pi -e "s/^NodeName=(\S+)/NodeName=${cnodename_prefix}[1-${num_ccomputes}]/" /etc/slurm/slurm.conf
perl -pi -e "s/^PartitionName=normal Nodes=(\S+)/PartitionName=normal Nodes=${cnodename_prefix}[1-${num_ccomputes}]/" /etc/slurm/slurm.conf
# copy slurm file from sms node to Cloud Comute Nodes
cp -fpr -L /etc/slurm/slurm.conf $chpcInitPath
cp -fpr -L /etc/pam.d/slurm $chpcInitPath
cp -fpr -L /etc/munge/munge.key $chpcInitPath
# Start slurm and munge 
systemctl enable munge
systemctl restart munge
systemctl enable slurmctld
systemctl restart slurmctld
#systemctl enable slurmd
#systemctl restart slurmd
logger "chpcInit:slurm configuration done"
#Change file permissions in /etc/ssh to fix ssh into compute node
chmod 0600 /etc/ssh/ssh_host_*_key
# work-around for bug https://bugs.launchpad.net/neutron/+bug/1531426
# create /etc/hosts file with sms and compute node entry
#  # # QFILEQ
#  # FILE: README
# This directory "hpc/recipe/3_hpc_as_service" maintains the recipes for use
# case 3. In this case OpenStack provides baremetal nodes and recipe
# creates one of the bare metal node as HPC head node and create other 
# remaining bare metals as HPC compute nodes.
# it provides recipe to create HPC head node images as well as hpc
# compute node images.
# for hpc configuration it creates cloud-init script (post boot script)
# for each type of nodes (SMS as well CN)
# This use case can be invoked as below:
# ../setup_cloud_hpc.sh -i=inventory/3_hpc_as_service/hn3_has_input.local -u=3 % % ohpc_validation_comment -n=inventory/3_hpc_as_service/hn3_has_inventory
# Below are the scripts called to create the environment for use case 1:
# file: set_os_hpc
# ================
# This is the script to setup HPC in OpenStack Cloud. This script is called by 
# setup_cloud_hpc.sh if user provides a "-u=3" input to it. To perform the job,
# it executes other scripts.
# file: prepare_cloud_init
# ======================
# This sripts generates cloudinit script (chpcInit) for both sms node as well as
# compute nodes, which is supplied to Nova to boot the nodes.
# file: prepare_chpc_image
# ========================
# This generates HPC images for provisioning bare metal cloud nodes. It generates 
# 3 images
# Two deploy images for ironic to use 
# 1. icloud-hpc-deploy-c7.kernel
# 2. icloud-hpc-deply-c7.initramfs
# One user image for compute node
# 3. icloud-hpc-cent7.qcow2
# One user image for hpc head node (sms)
# 4. icloud-hpc-cent7-sms.qcow2
# file: prepare_chpc_openstack
# ============================
# This script prepares nova, ironic for baremetal provisioning, installs and 
# configure pxe boot, pxe-impmi driver for ironic, and enable cloud init for 
# baremetal nodes.
# It also configure neutron for internal dns service
# File: deploy_chpc_openstack
# ===========================
# This script deploy baremetal nodes in an openstack using nova, ironic, neutron 
# and glance by using cloudinit recipe (prepare under prepare_cloudInit) and HPC
# images (prepared by prepare_chpc_image).
# it first deploys sms node and then deploy compute nodes.
# file: update_cnodes_to_sms
# ==========================
# This script perform the post boot configuration including updating SLURM 
# resource manager at sms node for hpc compute nodes
#  # # QFILEQ
#!/bin/bash 
#  # FILE: set_os_hpc
#This script expects compute node ID as an input
set -x
# find and setup working directory
CHPC_SCRIPTDIR="$( cd "$( dirname "$( readlink -f "${BASH_SOURCE[0]}" )" )" && pwd -P )"
CHPC_SCRIPTDIR="${CHPC_SCRIPTDIR%x}"
cd $CHPC_SCRIPTDIR
echo "..$CHPC_SCRIPTDIR .."
#function cont_next() {
#   echo "continue to ? "
#   read uinput
#   if [ "$uinput" == "y" ]; then
#       echo "Continueing ..."
#    else
#      exit 0
#   fi
#}
# Create Post boot file, can be used for cloudInit
# Check for NTP server and configuration on compute nodes
#
#
# =====================================
# Preparation for CloudInit Script and files
# =====================================
# This assumes that HPC head node recipe is installed and SMS_node functionality is already configured. It will get some data from there to prepare cloudInit
time source prepare_cloud_init
# ========================
# Prepare CloudHPC Image :
# ========================
# Check if User selected to prepare cloud HPC images
#cont_next
time source prepare_chpc_image
# =============================================
# Prepare OpenStack for HPC baremetal instances
# =============================================
#time source prepare_chpc_openstack
#cont_next
time source deploy_chpc_openstack
#Wait for CN(s) to come up. TODO: Poll and wait rather than just a set 10 minutes.
sleep 600
#Call cloudInit workaround script
#time source c_init_workaround
#
# ========================================================
# Prepare SMS/Service Node. 
# Add Cloud baremetal nodes to HPC Orchestrator
# ========================================================
#time source update_cnodes_to_sms
#  # # QFILEQ
#!/bin/bash 
#  # FILE: update_cnodes_to_sms
#  # Update chpc_sms_init or prepare_cloud_init  with similar informoation from recipe for use case 2
#on Head node, start slurm
sleep 5
ssh $sms_ip scontrol update nodename=cc[1-${num_ccomputes}] state=idle
ssh $sms_ip sinfo
#  # # QFILEQ
#  # # HFILEH
# # FILE:heat-sms.yaml
heat_template_version: 2013-05-23
description: >
  This is a heat template to create HPC SMS node
  http://docs.openstack.org/developer/heat/template_guide/index.html
  http://cloudinit.readthedocs.io/en/latest/
  create openstack with minimum parameter
  openstack stack create --template heat-sms.yaml --parameter "ssh_key_name:ostack_key;server_name:sms" stack-name1
  openstack stack delete stack-name1
  #Created by: Sunil Mahawar
parameters:
  ssh_key_name:
    type: string
    label: SSH Keypair
    description: Name of a SSH keypair.
    hidden: false
    default: ostack_key
  server_name:
    type: string
    label: Instance Name
    description: Name of the baremetal instance.
    hidden: false
    default: sms1
  instance_flavor:
    type: string
    label: Instance Flavor
    description: The flavor type to use for baremetal server.
    default: baremetal-flavor
    hidden: false
  image_id:
    type: string
    label: Image ID
    description: The image to use for baremetal server.
    default: sms-image
    hidden: false
  network_id:
    type: string
    label: Network ID
    description: The flat network to be used for baremetal server
    default: sharednet1
  port_id:
    type: string
    label: Network port ID
    description: mac address to IP address mapping for static IP reservation
    default: sms1
resources:
  boot_config:
    type: OS::Heat::CloudConfig
    properties:
      cloud_config:
        write_files:
        - path: /opt/ohpc/admin/cloud_hpc_init/chpc_sms_init.sh
          content: {get_file: /opt/ohpc/admin/cloud_hpc_init/chpc_sms_init}
  boot_script:
    type: OS::Heat::SoftwareConfig
    properties:
      group: ungrouped
      config: |
        #!/bin/bash
        echo "Running sms_init script"
        sh /opt/ohpc/admin/cloud_hpc_init/chpc_sms_init.sh
        echo "Done"
  server_init:
    type: OS::Heat::MultipartMime
    properties:
      parts:
      - config: {get_resource: boot_config}
      - config: {get_resource: boot_script}
  baremetal_instance:
    type: OS::Nova::Server
    properties:
      name: { get_param: server_name }
      image: { get_param: image_id }
      flavor: { get_param: instance_flavor }
      key_name: { get_param: ssh_key_name }
      networks:
        - port: {get_param: port_id}
      user_data_format: SOFTWARE_CONFIG
      user_data: {get_resource: server_init}
outputs:
  ip_address:
    description: IP address of the baremetal instance
    value: { get_attr: [baremetal_instance, first_address] }
#  # # HFILEH
#  #FILE: heat-cn.yaml
heat_template_version: 2013-05-23
description: >
  This is a heat template to create HPC compute nodes
  http://docs.openstack.org/developer/heat/template_guide/index.html
  http://cloudinit.readthedocs.io/en/latest/
  create openstack with minimum parameter
  openstack stack create --template heat-cn.yaml -P "cn_count=2" stack-name1
  openstack stack delete stack-name1
  #Created by: Sunil Mahawar
parameters:
  ssh_key_name:
    type: string
    label: SSH Keypair
    description: Name of a SSH keypair.
    hidden: false
    default: ostack_key
  server_name:
    type: string
    label: Instance Name
    description: Name of the baremetal instance.
    hidden: false
    default: cc%index%
  instance_flavor:
    type: string
    label: Instance Flavor
    description: The flavor type to use for baremetal server.
    default: baremetal-flavor
    hidden: false
  image_id:
    type: string
    label: Image ID
    description: The image to use for baremetal server.
    default: user-image
    hidden: false
  network_id:
    type: string
    label: Network ID
    description: The flat network to be used for baremetal server
    default: sharednet1
  port_id:
    type: string
    label: Network port ID
    description: mac address to IP address mapping for static IP reservation
    default: cc%index%
  cn_count:
    type: number
    label: baremetal instances
    description: number of baremetal instances
    default: 1
resources:
  boot_config:
    type: OS::Heat::CloudConfig
    properties:
      cloud_config:
        write_files:
        - path: /opt/ohpc/admin/cloud_hpc_init/chpc_init.sh
          content: {get_file: /opt/ohpc/admin/cloud_hpc_init/chpc_init}
  boot_script:
    type: OS::Heat::SoftwareConfig
    properties:
      group: ungrouped
      config: |
        #!/bin/bash
        echo "Running compute node boot script"
        sh /opt/ohpc/admin/cloud_hpc_init/chpc_init.sh
        echo "Done"
  server_init:
    type: OS::Heat::MultipartMime
    properties:
      parts:
      - config: {get_resource: boot_config}
      - config: {get_resource: boot_script}
  multi-server:
   type: OS::Heat::ResourceGroup
   properties:
      count: { get_param: cn_count }
      resource_def:
       type: OS::Nova::Server
       properties:
         name: { get_param: server_name }
         image: { get_param: image_id }
         flavor: { get_param: instance_flavor }
         key_name: { get_param: ssh_key_name }
         networks:
           - network: { get_param: network_id }
         user_data_format: SOFTWARE_CONFIG
         user_data: {get_resource: server_init}
#outputs:
#  ip_address:
#    description: IP address of the baremetal instance
#    value: { get_attr: [baremetal_instance, first_address] }
#  # # HFILEH

