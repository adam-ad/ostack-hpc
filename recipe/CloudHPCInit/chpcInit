#!/bin/bash
#


# nfs mount directoruy from SMS head node to Compute Node
echo "<sms_ip>:/home /home nfs nfsvers=3,rsize=1024,wsize=1024,cto 0 0" >> /etc/fstab
echo "<sms_ip>:/opt/intel/hpc-orchestrator/pub /opt/intel/hpc-orchestrator/pub nfs nfsvers=3 0 0" >> /etc/fstab
mount /home
mount /opt/intel/hpc-orchestrator/pub
# enable test suite
echo -n "<sms_ip>:/opt/intel/hpc-orchestrator/pub/tests " >> /etc/fstab
echo "/opt/intel/hpc-orchestrator/pub/tests nfs nfsvers=3 0 0" >> /etc/fstab
#mount 
mount /opt/intel/hpc-orchestrator/pub/tests
#update ntp server
echo "server <sms_ip>" >> /etc/ntp.conf
# Restart nfs
systemctl restart nfs
# Restart ntp at CN
systemctl enable ntpd
# Update ntp server
echo "server <sms_ip>" >> /etc/ntp.conf
systemctl restart ntpd
#
# Copy slurm file: 
# TBD: this should happen in parent script at HN
#cp -f /etc/slurm/slurm.conf $chpcInitPath/

#TBD Extend nodes here
# Update basic slurm configuration, Ideally this should happen in SMS node, we should just copy files 
perl -pi -e "s/^NodeName=(\S+)/NodeName=${cnodename_prefix}[1-${num_ccomputes}]/" slurm.conf
perl -pi -e "s/^PartitionName=normal Nodes=(\S+)/PartitionName=normal Nodes=${cnodename_prefix}[1-${num_ccomputes}]/" /etc/slurm/slurm.conf
# Now update the slurm file
cp -f slurm.conf /etc/slurm/slurm.conf
# Update rsyslog
$Update  rsyslog
echo "*.* @<sms_ip>:514" >> /etc/rsyslog.conf
systemctl restart rsyslog
#Sync following files to compute node
cp -f -L passwd /etc/passwd
cp -f -L group /etc/group
cp -f -L shadow /etc/shadow 
cp -f -L slurm.conf /etc/slurm/slurm.conf
cp -f -L slurm /etc/pam.d/slurm
cp -f -L munge.key /etc/munge/munge.key
# For hostname resolution
#TBD: This is a workaround for now, what we want is nodes to communicate to other nodes and sms node. so need to update cn entries here. might want to generate a script which is executed on compute node, and that updates entries into /etc/hosts of compute node. This workaround will break other functionalities in Cloudburst scenario
cp -f -L hosts /etc/hosts
