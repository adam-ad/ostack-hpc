#!/bin/bash
#

#Ensure the executing shell is in the same directory as the script.
SCRIPTDIR="$( cd "$( dirname "$( readlink -f "${BASH_SOURCE[0]}" )" )" && pwd -P && echo x)"
SCRIPTDIR="${SCRIPTDIR%x}"
cd $SCRIPTDIR

#set the hostname
hostnamectl set-hostname <cnode_name>

# nfs mount directory from SMS head node to Compute Node
cat /etc/fstab | grep "<sms_ip>:/home"
home_exists=$?
if [ "${home_exists}" -ne "0" ]; then
    echo "<sms_ip>:/home /home nfs nfsvers=3,rsize=1024,wsize=1024,cto 0 0" >> /etc/fstab
fi
cat /etc/fstab | grep "<sms_ip>:/opt/intel/hpc-orchestrator/pub"
orchestrator_pub_exists=$?
if [ "${orchestrator_pub_exists}" -ne "0" ]; then
    echo "<sms_ip>:/opt/intel/hpc-orchestrator/pub /opt/intel/hpc-orchestrator/pub nfs nfsvers=3 0 0" >> /etc/fstab
fi
mount /home
mount /opt/intel/hpc-orchestrator/pub
# enable test suite
cat /etc/fstab | grep "<sms_ip>:/opt/intel/hpc-orchestrator/pub/tests"
orchestrator_tests_exist=$?
if [ "${orchestrator_tests_exist}" -ne "0" ]; then
    echo -n "<sms_ip>:/opt/intel/hpc-orchestrator/pub/tests " >> /etc/fstab
fi
cat /etc/fstab | grep "opt/intel/hpc-orchestrator/pub/tests"
orchestrator_tests_exist=$?
if [ "${orchestrator_tests_exist}" -ne "0" ]; then
	echo "/opt/intel/hpc-orchestrator/pub/tests nfs nfsvers=3 0 0" >> /etc/fstab
fi
#mount 
mount /opt/intel/hpc-orchestrator/pub/tests
# Restart nfs
systemctl restart nfs
# Restart ntp at CN
systemctl enable ntpd
# Update ntp server
cat /etc/ntp.conf | grep "server <sms_ip>"
ntp_server_exists=$?
if [ "${ntp_server_exists}" -ne "0" ]; then
    echo "server <sms_ip>" >> /etc/ntp.conf
fi
systemctl restart ntpd
# time sync
ntpstat
#
# Copy slurm file: 
# TBD: this should happen in parent script at HN
#cp -f /etc/slurm/slurm.conf $chpcInitPath/

#TBD Extend nodes here
# Update basic slurm configuration, Ideally this should happen in SMS node, we should just copy files 
# Now update the slurm file
cp -f slurm.conf /etc/slurm/slurm.conf
# Update rsyslog
cat /etc/rsyslog.conf | grep "<sms_ip>:514"
rsyslog_set=$?
if [ "${rsyslog_set}" -ne "0" ]; then
    echo "*.* @<sms_ip>:514" >> /etc/rsyslog.conf
fi

systemctl restart rsyslog

#Sync following files to compute node
cp -f -L passwd /etc/passwd
cp -f -L group /etc/group
cp -f -L shadow /etc/shadow 
cp -f -L slurm.conf /etc/slurm/slurm.conf
cp -f -L slurm /etc/pam.d/slurm
cp -f -L munge.key /etc/munge/munge.key
# For hostname resolution
#TBD: This is a workaround for now, what we want is nodes to communicate to other nodes and sms node. so need to update cn entries here. might want to generate a script which is executed on compute node, and that updates entries into /etc/hosts of compute node. This workaround will break other functionalities in Cloudburst scenario
cp -f -L hosts /etc/hosts
# make sure that hostname mentioned into /etc/hosts matches machine hostname. TBD
# Start slurm and munge ?? 
systemctl enable munge
systemctl restart munge
systemctl enable slurmd
systemctl restart slurmd

#Change file permissions in /etc/ssh to fix ssh into compute node
chmod 0600 /etc/ssh/ssh_host_*_key

