#!/bin/bash
#Update the genders at sms with compute node information
# first check if genders is install TBD
for ((i=0; i<$num_ccomputes; i++)) ; do
   cat /etc/genders | grep ${cc_name[$i]}
   gender_exists=$?
   if [ "${gender_exists}" -ne "0" ]; then 
       echo -e "${cc_name[$i]}\tcompute,bmc=${cc_bmc[$i+1]}"
   fi
done >> /etc/genders

# configure mrsh at sms
if [[ ${enable_mrsh} -eq 1 ]];then
   # check if it is already configured grep mshell /etc/services will return non-zero, else configure"
   cat /etc/services | grep mshell
   mshell_exists=$?
   if [ "${mshell_exists}" -ne "0" ]; then
       echo "mshell          21212/tcp                  # mrshd" >> /etc/services
   fi
   cat /etc/services | grep mlogin
   mlogin_exists=$?
   if [ "${mlogin_exists}" -ne "0" ]; then
       echo "mlogin            541/tcp                  # mrlogind" >> /etc/services
   fi
fi

#Update cluster shell
if [[ ${enable_clustershell} -eq 1 ]];then
     # TBD check if clustershell is installed
     #cd /etc/clustershell/groups.d
     #echo "compute: cc[1-${num_ccomputes}]" >> local.cfg
     #echo "all: @adm,@compute" >> local.cfg
     #Update configuration with new nodes
     sed -i -- 's/all: @adm,@compute/compute: cc[1-${num_ccomputes}]\n&/' /etc/clustershell/group.d/local.cfg
fi


#on Head node
perl -pi -e "s/ControlMachine=\S+/ControlMachine=${sms_name}/" /etc/slurm/slurm.conf
#echo "/home *(rw,no_subtree_check,fsid=10,no_root_squash)" >> /etc/exports
#echo "/opt/intel/hpc-orchestrator/pub *(rw,no_subtree_check,fsid=11,no_root_squash)" >> /etc/export
# exportfs -a
# systemctl restart nfs
# Update slurm configuration on Head node
if [ ${num_ccomputes} -gt 4 ];then
    perl -pi -e "s/^NodeName=(\S+)/NodeName=${cnodename_prefix}[1-${num_ccomputes}]/" /etc/slurm/slurm.conf
    perl -pi -e "s/^PartitionName=normal Nodes=(\S+)/PartitionName=normal Nodes=${cnodename_prefix}[1-${num_ccomputes}]/" /etc/slurm/slurm.conf
#    perl -pi -e "s/^NodeName=(\S+)/NodeName=${cnodename_prefix}[1-${num_ccomputes}]/" $CHROOT/etc/slurm/slurm.conf
#    perl -pi -e "s/^PartitionName=normal Nodes=(\S+)/PartitionName=normal Nodes=${cnodename_prefix}[1-${num_ccomputes}]/" $CHROOT/etc/slurm/slurm.conf
fi
#Restart the slurm after above configuration
systemctl restart slurmctld
#
# Power ON the CNs using Nova or Ironic,
#
#
# Restart munge and slurmd after they are restarted at HN
pdsh -w cc[1-4] systemctl start munge
pdsh -w cc[1-4] systemctl start slurmd
scontrol update cnodename=cc[1-4] state=idle

